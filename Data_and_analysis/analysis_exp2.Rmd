---
title: "WordRec Experiment 2"
author: "A. Fourtassi & M. C. Frank"
date: "January 13, 2017"
output:
  html_document:
    number_sections: yes
    toc: yes
---

Libraries.

```{r}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(langcog)
library(boot)
theme_set(theme_bw())
```

Data and exclusion
```{r}

#Skip the pre task trials 
#Filter subjects who had no technical problem
#Select subjects who got more than 50% correct answers on the pre task trials

d <- read_delim("data_exp2_anonym.txt", delim = " ") %>%
  filter(type == "Task", problem=="No") %>%
  filter(score > 0.5)
```

Break out by conditions. 
```{r}
sounds <- d %>%
  filter(condition == "sound") %>%
  group_by(sound_dist) %>%
  multi_boot_standard(col = "answer")

concepts <- d %>%
  filter(condition == "concept") %>%
  group_by(concept_dist) %>%
  multi_boot_standard(col = "answer")

joint <- d %>%
  filter(condition == "joint") %>%
  group_by(concept_dist, sound_dist) %>%
  multi_boot_standard(col = "answer")
```

Initial plot
```{r}

ggplot(joint, 
       aes(x = sound_dist, y = mean, col = factor(concept_dist))) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(width = .1), size=0.3) + 
  geom_line(lty = 2) + 
  geom_line(data = sounds, aes(col = 1), col = "black") + 
  geom_pointrange(data = sounds, aes(ymin = ci_lower, ymax = ci_upper, 
                                     col = 1), col = "black", size=0.4)+
  xlab("Auditory distance") +ylab("% different")+
  scale_colour_discrete(name="Visual Dist")+
theme(legend.title = element_text(size=8))
  

```


Fit the models.
```{r}

sound_data <- d %>%
    filter(condition == "sound")

concept_data <- d %>%
    filter(condition == "concept")

joint_data <- d %>%
    filter(condition == "joint")


#Fit the non-linear function
#

sound_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=sound_data, start = list(e=0, vrA=2))

concept_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=concept_data, start = list(e=0, vrV=2))

joint_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=joint_data, start = list(e=0, vrA_j=2, vrV_j=2))


##extract parameters

#The sound-ony bias term:
eA <- coef(sound_nl)["e"]
#The sound-only variance
vrA <- coef(sound_nl)["vrA"]

#The visual-ony bias term:
eV <- coef(concept_nl)["e"]
#The visual-ony variance:
vrV <- coef(concept_nl)["vrV"]

#The bimodal bias term:
eJ <- coef(joint_nl)["e"]

#The bimodal auditory variance:
vrJ_A <- coef(joint_nl)["vrA_j"]
#The bimodal visual variance:
vrJ_V <- coef(joint_nl)["vrV_j"]
```


Bootstrap sampling on the model's parameters
```{r}
lmfit <- function(data, indices) {
  
  myd = data[indices, ]
  
  s_data <- myd %>%
    filter(condition == "sound")

  v_data <- myd %>%
    filter(condition == "concept") 

  j_data <- myd %>%
    filter(condition == "joint")
    
  s_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=s_data, start = list(e=0, vrA=2), nls.control(warnOnly = TRUE))

 c_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=v_data, start = list(e=0, vrV=2), nls.control(warnOnly = TRUE))

 j_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=j_data, start = list(e=0, vrA_j=2, vrV_j=2), nls.control(warnOnly = TRUE))

  
  s_va <- coef(s_nl)["vrA"]
  v_va <- coef(c_nl)["vrV"]
  
  model_baseline=v_va/s_va

  j_va_s=coef(j_nl)["vrA_j"]
  j_va_v=coef(j_nl)["vrV_j"]
  
  model_data=j_va_v/j_va_s
  
  preference=c(model_baseline, model_data)
  
  return(preference)

  }

results <- boot(data= d, statistic = lmfit, R = 10000)

results
myCI1=boot.ci(results, index = 1, conf = 0.95)
myCI2=boot.ci(results, index = 2, conf = 0.95)
print(myCI1)
print(myCI2)


```

Analysis of the unimodal cases.
```{r}

#Recogniton functions from the unimodal fit 
x <- seq(0, 4, 0.01)

y_sound_nl <- predict(sound_nl, list(sound_dist = x), type="response")
y_concept_nl <- predict(concept_nl, list(concept_dist = x), type="response")

uniS_nl <- data.frame(xS_nl=x,yS_nl=y_sound_nl)
uniV_nl <- data.frame(xV_nl=x,yV_nl=y_concept_nl)


##The auditory case
ggplot(sounds, 
       aes(x = sound_dist, y = mean)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(width = .1)) + 
  #geom_line(data=uniS,aes(x=xS, y=yS))+
  geom_line(data=uniS_nl,aes(x=xS_nl, y=yS_nl), col='black')+
  xlab("Auditory distance") +ylab("% different")+
  scale_y_continuous(limits = c(0, 1))+theme(aspect.ratio = 0.7)
  #stat_function(fun = Logistic_a, colour="red")

##The visual case
ggplot(concepts, 
       aes(x = concept_dist, y = mean)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(width = .1)) + 
  #geom_line(data=uniV,aes(x=xV, y=yV))+
  geom_line(data=uniV_nl,aes(x=xV_nl, y=yV_nl), col='black')+
  xlab("Visual distance") +ylab("% different")+
  scale_y_continuous(limits = c(0, 1))+theme(aspect.ratio = 0.7)
  #stat_function(fun = Logistic_v, colour="red")
 

```

The bimodal case

```{r}

#The bimodal fit
model_fit <- function (x,y) {
   1/(1 + (1-eJ)*exp((-4/vrJ_A)*x+(-4/vrJ_V)*y+(8/vrJ_A)+(8/vrJ_V)))
}

#The ideal observer model
model_base <- function (x,y) {
    1/(1 + (1-eJ)*exp((-4/vrA)*x+(-4/vrV)*y+(8/vrA)+(8/vrV)))
}


#Add the fit and the baeline to data
ms_all <- joint %>% 
  rename(joint = mean) %>%
  left_join(select(concepts, concept_dist, mean) %>% 
              rename(concepts = mean)) %>%
  left_join(select(sounds, sound_dist, mean) %>% 
              rename(sounds = mean)) %>%
  mutate(fit = model_fit(sound_dist, concept_dist)) %>%
  mutate(base = model_base(sound_dist, concept_dist)) %>%
  gather(model, pred, concepts, sounds, base, fit)

#Here plot human data
ggplot(joint, 
       aes(x = sound_dist, y = mean, col = factor(concept_dist))) + 
  #geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                 # position = position_dodge(width = .1), size=0.3) + 
  geom_point(size=2.5) +
  geom_line(size=0.6) + 
  geom_line(data = sounds, aes(col = 1), col = "black",size=0.8, lty=2) + 
  #geom_pointrange(data = sounds, aes(ymin = ci_lower, ymax = ci_upper, 
                             #        col = 1), col = "black", size=0.4)+
  geom_hline(data = concepts, aes(yintercept=mean, col = factor(concept_dist)), lty = 2)+
  xlab("Auditory distance") +ylab("% different")+
  scale_colour_discrete(name="Visual Dist")+
theme(legend.title = element_text(size=8))+
  scale_y_continuous(limits = c(0, 1))+
  theme(aspect.ratio = 0.7)


#Plot predictions of the ideal observer model (or the baseline)
mybase <- ms_all %>%
  filter(model=="base")

ggplot(mybase, 
       aes(x = sound_dist, y = pred, col = factor(concept_dist))) + 
  geom_point(size=2.5) + 
  geom_line(size = 0.6) + 
  geom_line(data = sounds, aes(y=mean, col = 1), col = "black",size=0.8, lty=2) + 
  geom_hline(data = concepts, aes(yintercept=mean, col = factor(concept_dist)), lty = 2)+
  #scale_colour_solarized() +
  xlab("Auditory distance") +ylab("% different")+
  scale_colour_discrete(name="Visual Dist")+
theme(legend.title = element_text(size=8))+
  scale_y_continuous(limits = c(0, 1))+
  theme(aspect.ratio = 0.7)


#Plot the predictions of the bimodal fit
myfit <- ms_all %>%
  filter(model=="fit")

ggplot(myfit, 
       aes(x = sound_dist, y = pred, col = factor(concept_dist))) + 
  geom_point(size=2.5) + 
  geom_line(size= 0.6) + 
  geom_line(data = sounds, aes(y=mean, col = 1), col = "black" ,size=0.8, lty=2) +
  geom_hline(data = concepts, aes(yintercept=mean, col = factor(concept_dist)), lty = 2)+
  #scale_colour_solarized() +
  xlab("Auditory distance") +ylab("% different")+
  scale_colour_discrete(name="Visual Dist")+
theme(legend.title = element_text(size=8))+
  scale_y_continuous(limits = c(0, 1))+
theme(aspect.ratio = 0.7)

```


The correlations models vs. human data
```{r}

#Plot the correlation figure
myfilter <- ms_all %>%
  filter(model=="fit" | model=="base")

#Correlation models (baseline and fit) to bimodal data 
ggplot(myfilter, 
       aes(x = pred, y = joint, col = factor(concept_dist), 
           shape = factor(sound_dist))) +
 geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(width = .1), size=0.2) + 
  geom_abline(slope = 1, lty = 2) +
  facet_grid(.~model)+
theme(aspect.ratio = 0.7)

#The correlation values
base_pred <- subset(myfilter, model=='base')
fit_pred <- subset(myfilter, model=='fit')
r_base <- cor(base_pred$joint,base_pred$pred)
r_base
r_base*r_base
r_fit <- cor(fit_pred$joint,fit_pred$pred)
r_fit
r_fit*r_fit
```

Modality preference
```{r}

#Modality dominance coeficent
a_fit=vrJ_V/vrJ_A
a_base=vrV/vrA
a_Vdom=2*a_base
a_Sdom=0.5*a_base

#Classification thresholds (prob=0.5) for:

##The basline (combination of the unimodal cases)
classif_base = function (x, A=a_base) {
    2*A+2-A*x
}

##Bimodal fit 
classif_fit = function (x, A=a_fit) {
    2*A+2-A*x
}

##A baseline for visual dominance
classif_Vdom = function (x, A=a_Vdom) {
    2*A+2-A*x
}

##A baseline for auditory domiance
classif_Sdom = function (x, A=a_Sdom) {
    2*A+2-A*x
}

Gaus_fit <- data.frame(x=c(rnorm(1000,0,2),rnorm(1000,4,2)),
          y=c(rnorm(1000,0,2),rnorm(1000,4,2)))

#Modality dominance?
ggplot(Gaus_fit, aes(x,y)) +
  stat_function(fun = classif_base, colour = "red", lty=2) +
  stat_function(fun = classif_Vdom, colour = "blue", lty=2 )+
  stat_function(fun = classif_Sdom, colour = "blue", lty=2)+
  stat_function(fun = classif_fit, colour = "black")+
  xlab("Auditory") +ylab("Visual") + scale_x_continuous(limits = c(-6, 10))+
  scale_y_continuous(limits = c(-6, 10))+theme(aspect.ratio = 1)


```








