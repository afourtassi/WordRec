---
title             : "Word-Referent Identification Under Multimodal Uncertainty"
shorttitle        : "Word Identification Under Multimodal Uncertainty"

author: 
  - name          : "Abdellah Fourtassi"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "afourtas@stanford.edu"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"

author_note: |

  Abdellah Fourtassi
  
  Department of Psychology
  
  Stanford University
  
  50 Serra Mall
  
  Jordan Hall, Building 420
  
  Stanford, CA 94301

abstract: |

 Identifying a spoken word in a referential context requires both the ability to process and integrate multimodal input and the ability to reason under uncertainty. How do these tasks interact with one another? We introduce a task that allows us to examine how adults identify words under joint uncertainty in the auditory and visual modalities. We propose an ideal observer model of the task which provides an account of how auditory and visual cues are combined optimally. Model predictions are tested in three experiments where word recognition is made under two kinds of uncertainty: category ambiguity and/or distorting noise. In all cases, the optimal model explains much of the variance in human mean judgments. In particular, when the signal is not distorted with noise, participants weight the auditory and visual cues optimally, that is, according to the relative reliability of each modality. But when one modality has noise added to it, human perceivers systematically prefer the unperturbed modality to a greater extent than the optimal model does. The study provides a formal framework which helps us understand precisely how word form and word meaning interact in word recognition under uncertainty. Moreover it offers a first step towards a model that accounts for form-meaning synergy in early word learning.

  
keywords          : "Language understanding; audio-visual processing; word learning; speech perception; computational modeling."

wordcount         : "X"

header-includes:
   #- \usepackage{bibentry}
   - \usepackage[sortcites=false,sorting=none]{biblatex}
   
bibliography      : ["references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf 

citation_package: biblatex

---

```{r load_packages, include = FALSE}
library(papaja)
library(readr)
library(tidyr)
library(ggplot2)
library(cowplot)
library(knitr)
library(boot)
library(dplyr)
library(langcog)
theme_set(theme_bw())
library(broom)
#library("knitcitations")
#cleanbib()
#options("citation_format" = "pandoc")
```

```{r}

#Data from the 3 experiments 
#############################
exp1 <- read_delim("../Data_and_analysis/data_exp1_anonym.txt", delim = " ") %>%
  filter(type == "Task") %>%
  mutate(experiment='Experiment1')

exp2 <- read_delim("../Data_and_analysis/data_exp2_anonym.txt", delim = " ") %>%
  filter(type == "Task") %>%
  mutate(experiment='Experiment2')

exp3 <- read_delim("../Data_and_analysis/data_exp3_anonym.txt", delim = " ") %>%
  filter(type == "Task") %>%
  mutate(experiment='Experiment3')
```


```{r}
summarise <- dplyr::summarise
rename <- dplyr::rename
```

```{r}
#First Exlusion criteria:
#############################

#All data
N_all_1 <- exp1 %>%
  distinct(ID) %>%
  nrow()

N_all_2 <- exp2 %>%
  distinct(ID) %>%
  nrow()

N_all_3 <- exp3 %>%
  distinct(ID) %>%
  nrow()

#Participants who did not encounter a technical problem with the online experiment
 
#Exp1
exp1_noProb <- exp1 %>%
  filter(problem=="No") 

N_noProb_1 <- exp1_noProb %>%
  distinct(ID) %>%
  nrow()

#Exp2
exp2_noProb <- exp2 %>%
  filter(problem=="No") 

N_noProb_2 <- exp2_noProb %>%
  distinct(ID) %>%
  nrow()

#Exp3
exp3_noProb <- exp3 %>%
  filter(problem=="No") 

N_noProb_3 <- exp3_noProb %>%
  distinct(ID) %>%
  nrow()

#Participants who who did not encounter a problem AND were above 50% accuracy on obvious trials

#Exp1
exp1_good <- exp1_noProb %>%
  filter(score > 0.5)

N_good_1 <- exp1_good %>%
  distinct(ID) %>%
  nrow()

#Exp2
exp2_good <- exp2_noProb %>%
  filter(score > 0.5)

N_good_2 <- exp2_good %>%
  distinct(ID) %>%
  nrow()

#Exp3
exp3_good <- exp3_noProb %>%
  filter(score > 0.5)

N_good_3 <- exp3_good %>%
  distinct(ID) %>%
  nrow()

```


```{r}


#Data subseting

#Experiment 1
##############

sound_all_exp1 <- exp1_good %>%
    filter(condition == "sound")

concept_all_exp1 <- exp1_good %>%
    filter(condition == "concept")

joint_all_exp1 <- exp1_good %>%
    filter(condition == "joint")

#Summary
sounds_exp1 <- exp1_good %>%
  filter(condition == "sound") %>%
  group_by(sound_dist) %>%
  dplyr::summarise(mean = mean(answer),
                   sd = sd(answer),
                   n = n()) %>%
  mutate(se = sd / sqrt(n),
         lower = mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%
  select(-sd, -n, -se,) %>%
  mutate(Experiment="Experiment 1", 
         Condition="Auditory") %>%
  rename(distance = sound_dist)

concepts_exp1 <- exp1_good %>%
  filter(condition == "concept") %>%
  group_by(concept_dist) %>%
  dplyr::summarise(mean = mean(answer),
                   sd = sd(answer),
                   n = n()) %>%
  mutate(se = sd / sqrt(n),
         lower = mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%
  select(-sd, -n, -se,) %>%
  mutate(Experiment="Experiment 1", 
         Condition="Visual") %>%
  rename(distance = concept_dist)

joint_exp1 <- exp1_good %>%
  filter(condition == "joint") %>%
  group_by(concept_dist, sound_dist) %>%
  summarise(mean = mean(answer)) %>%
  mutate(Experiment="Experiment 1", 
         Condition="Joint") 

#Experiment2
############

sound_all_exp2 <- exp2_good %>%
    filter(condition == "sound")

concept_all_exp2 <- exp2_good %>%
    filter(condition == "concept")

joint_all_exp2 <- exp2_good %>%
    filter(condition == "joint")

#Summary
sounds_exp2 <- exp2_good %>%
  filter(condition == "sound") %>%
  group_by(sound_dist) %>%
  dplyr::summarise(mean = mean(answer),
                   sd = sd(answer),
                   n = n()) %>%
  mutate(se = sd / sqrt(n),
         lower = mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%
  select(-sd, -n, -se,) %>%
  mutate(Experiment="Experiment 2", 
         Condition="Auditory") %>%
  rename(distance = sound_dist)

concepts_exp2 <- exp2_good %>%
  filter(condition == "concept") %>%
  group_by(concept_dist) %>%
  dplyr::summarise(mean = mean(answer),
                   sd = sd(answer),
                   n = n()) %>%
  mutate(se = sd / sqrt(n),
         lower = mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%
  select(-sd, -n, -se,) %>%
  mutate(Experiment="Experiment 2", 
         Condition="Visual") %>%
  rename(distance = concept_dist)

joint_exp2 <- exp2_good %>%
  filter(condition == "joint") %>%
  group_by(concept_dist, sound_dist) %>%
  summarise(mean = mean(answer)) %>%
  mutate(Experiment="Experiment 2", 
         Condition="Joint") 

#Experiment3
############

sound_all_exp3 <- exp3_good %>%
    filter(condition == "sound")

concept_all_exp3 <- exp3_good %>%
    filter(condition == "concept")

joint_all_exp3 <- exp3_good %>%
    filter(condition == "joint")

#Summary
sounds_exp3 <- exp3_good %>%
  filter(condition == "sound") %>%
  group_by(sound_dist) %>%
  dplyr::summarise(mean = mean(answer),
                   sd = sd(answer),
                   n = n()) %>%
  mutate(se = sd / sqrt(n),
         lower = mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%
  select(-sd, -n, -se,) %>%
  mutate(Experiment="Experiment 3", 
         Condition="Auditory") %>%
  rename(distance = sound_dist)

concepts_exp3 <- exp3_good %>%
  filter(condition == "concept") %>%
  group_by(concept_dist) %>%
  dplyr::summarise(mean = mean(answer),
                   sd = sd(answer),
                   n = n()) %>%
  mutate(se = sd / sqrt(n),
         lower = mean - qt(1 - (0.05 / 2), n - 1) * se,
         upper = mean + qt(1 - (0.05 / 2), n - 1) * se) %>%
  select(-sd, -n, -se,) %>%
  mutate(Experiment="Experiment 3", 
         Condition="Visual") %>%
  rename(distance = concept_dist)

joint_exp3 <- exp3_good %>%
  filter(condition == "joint") %>%
  group_by(concept_dist, sound_dist) %>%
  summarise(mean = mean(answer)) %>%
  mutate(Experiment="Experiment 3", 
         Condition="Joint") 

exp_uni_data <- bind_rows(sounds_exp1, concepts_exp1,
                     sounds_exp2, concepts_exp2,
                     sounds_exp3, concepts_exp3)


```

Language uses symbols expressed in one modality (e.g., the auditory modality, in the case of speech) to communicate about the world, which we perceive through many different sensory modalities. Consider hearing someone yell "bee!" at a picnic, as a honey bee buzzes around the food. Identifying a word involves processing the auditory information as well as other perceptual signals (e.g., the visual image of the bee, the sound of its wings, the sensation of the bee flying by your arm). A word is successfully identified when information from these modalities provide convergent evidence. However, word identification takes place in a noisy world, and the cues received through each modality may not provide a definitive answer. On the auditory side, individual acoustic word tokens are almost always ambiguous with respect to the particular sequence of phonemes they represent, which is due to the inherent variability of how a phonetic category is realized acoustically [@hillenbrand1995]. And some tokens may be distorted additionally by mispronunciation or ambient noise. Perhaps the speaker was yelling "pea" and not "bee". Similarly, a sensory impression may not be enough to make a definitive identification of a visual category.\footnote{In the general case, language can of course be visual as well as auditory, and object identification can be done through many modalities. For simplicity, we focus on audio-visual matching here.} Perhaps the insect was a beetle or a fly instead.

How does the mind deal with uncertainty to identify the speaker's intended word? To address this question, many researchers in recent years have adopted the rational approach to cognition [@anderson90], which corresponds to Marr's computational level of analysis. This approach has had a tremendous impact not only on speech related research [@Norris08; @clayard08; @feldman2009; @kleinschmidt2015], but also on many other disciplines in the cognitive sciences [for reviews, see @chater06; @Knill04; @tenenbaum11]. Within this framework, researchers typically consider an "ideal observer" model which provides a characterization of the task/goal and shows what the optimal performance should be under this characterization. When there is uncertainty in the input, the ideal observer performs an optimal probabilistic inference. For example, in order to recognize an ambiguous linguistic input, the model uses all available probabilistic knowledge in order to maximize the accuracy of this recognition. Thus, when the task is well specified, the ideal observer model can be seen as a theoretical upper limit on performance. It is often used, not so much as a realistic model of human performance, as much as a baseline against which human performance can be compared [@Geisler2003]. When there is a deviation from the ideal, this can reveal extra constraints on human cognition, such as limitations on working memory or attentional resources.

In this line of research, @clayard08 simulated auditory uncertainty by manipulating the probability distribution of a cue (VOT) that differentiated similar words (e.g., "beach" and "peach"). They found that humans were sensitive to these probabilistic cues and their judgments closely reflected the optimal predictions. In another work, @feldman2009 studied the perceptual magnet effect, which is related to how categories influence the way the stimuli are perceived. They showed that this effect can be explained as the consequence of optimally solving the problem of perception under uncertainty. Both studies explored optimal performance under uncertainty from one modality, i.e., the auditory modality. There is however extensive evidence that information from the visual modality, in particular the speaker's facial features,  influences the perception of auditory information [see @Campbell2008 for a review]. @bejjanki2011 offered a mathematical characterization of how cues from speech and lip movements can be optimally combined and showed that human performance during audio-visual phonemic labeling was consistent (at least at the qualitative level) with the behavior of the ideal observer.

The speaker's facial features are not the only visual information that can be used to disambiguate the auditory input, however. For instance, in a referential context, information about the identity of the semantic referent can be integrated with linguistic information to resolve lexical and syntactic ambiguities in speech [e.g., @Eberhard1995; @Tanenhaus1995; @spivey2002]. To our knowledge, however, no study offered an ideal observer analysis of word identification in such context, that is, when the mind has to combine cues from the sound and the referent. Imagine, for example, that someone is uncertain whether they heard "pea" or "bee", does this uncertainty make them rely more on the referent (e.g., the object being pointed at)? Vice versa, if they are not sure if they saw a bee or a fly, does it make them rely more on the sound? More importantly, when input in both modalities is uncertain to varying degrees, do they weight each modality according to its relative reliability (which is the optimal strategy), or do they over-rely on a particular modality (which is a sub-optimal strategy)?

The visual information provided by the referent is arguably different from the visual information provided by the speaker's facial features (such as lip movements). There seems to be at least two fundamental differences between these two cases, and both can influence the cue combination strategy:

<!-- Outside the realm of language per se, research on multisensory integration suggests that the brain integrates information from different modalities in a statistically optimal way, that is, the integration results in a higher overall precision, and modalities are weighed according to their relative reliability [@ernst02]. However, it is not a priori obvious whether this finding can be extended to word-referent recognition under multimodal uncertainty.  Indeed, there seems to be at least two fundamental differences between these two cases, and both can influence the cue combination strategy. -->

First, in the case of audio-visual speech, modalities are understood to differ only in terms of their reliability. In a referential context, however, modalities also differ in terms of their roles in the referential process: the auditory input represents the symbol (i.e., the word) whereas the visual input represents the referent. It has been suggested that because of its referential property, speech is a privileged signal for humans, starting in infancy [see @vouloumanos2014 for a review].\footnote{There is a debate over whether speech is privileged for infant and adults for the same reasons. Whereas Waxman and colleagues suggest that speech is privileged for both infants and adults because of its ability to refer, Sloutsky and colleagues suggest that speech might not have a referential status from the start. Rather, according to Sloutsky and colleagues, speech seems to be prefered by infants only because of a low level auditory "overshadowing".} Thus, in a referential context, it is possible that we do not treat the auditory and visual modalities as equivalent sources of information. Instead, there could be a bias for the auditory modality beyond what is expected from informational reliability alone. 

<!--An example of such integration is when we determine the size of an object using the visual and haptic modalities [@ernst02], or when we determine the spacial location of a stimulus using the visual and the auditory modalities [@alais04] -->
Second, in the case of audio-visual speech, the associations are expected to be perceptually correlated. The expectation for this correlation is such that when there is a mismatch between auditory and visual input, people still integrate them into a unified (but illusory) percept [@mcgurk1976]. In the case of referential language, however, the multimodal association is by nature *arbitrary* [@saussure1916; @greenberg1957].  For instance, there is no logical/perceptual connection between the sound "bee" and the corresponding insect. Moreover, variation in the way the sound "bee" is pronounced is generally not expected to correlate perceptually with variation in the shape (or any other visual property) in the category of bees. Thus, one could imagine that cue combination in the case of arbitrary audio-visual associations (word-referent) is less automatic, more effortful, and therefore less conducive to optimal integration than it is in the case of perceptually correlated associations (as in audio-visual speech perception). 

In the current study, we investigate how people combine cues from the auditory and the visual modality to recognize words in a referential context. In particular, we study how this combination is performed under various degrees of uncertainty in both the auditory and the visual modality.  We perform a rational analysis of the task. First we propose an ideal observer model that performs the combination in an optimal fashion. Second we compare the predictions of the optimal model to human responses. Humans can deviate from the ideal for several reasons. For instance, as mentioned above, a sub-optimality can be induced by the suggested privileged status of speech or by the arbitrariness of the referential association.  In order to study possible patterns of sub-optimality, we compare the optimal model (which provides a normative benchmark) to a descriptive model which is identical to the optimal model, except that the parameters in the former are fitted to human responses. <!--We do not perform the ideal observer analysis at the individual level, but at the population level. There might be some individual variation, but this study addresses the overall performance in the population as a whole.-->

We tested the ideal observer model's predictions in three behavioral experiments where we varied the source of uncertainty. In Experiment 1, audio-visual tokens were ambiguous with respect to their category membership only. In Experiment 2, we intervened by adding background noise to the auditory modality, and in Experiment 3, we intervened by adding background noise to the visual modality. In all experiments, participants were quantitatively near-optimal, though overall response precision was slightly lower than expected. Moreover, in Experiment 1 where neither of the modalities was perturbed with background noise, participants weighed auditory and visual cues according to the relative reliability predicted by the optimal model. In other words, we found no evidence for a modality bias towards either the auditory or the visual modality. However, in Experiment 2 and 3, participants over-relied on one modality when the other modality was perturbed with additional noise. <!-- In the discussion, we talk about the broad implications of these results and especially in relation to some developmental findings in early word learning [e.g., @stager1997]. -->


<!--The nature of the multi-modal associations (redundant vs. arbitrary) influences the quality of multi-modal processing in both children and adults: the processing and integration is easier in the case of redundant input, than it in the case of arbitrary input [see @robinson2010 for a review]. Thus, there is a priori no obvious reason multimodal cue combination in the case of arbitrary assoiations (such as the case of word-referent) should be optimal in the same way cue combination of redundant associations is. -->
<!--

A few studies have explored some aspects of audio-visual processing in a probabilistic framework  [e.g., @bejjanki2011; @kleinschmidt2015]. In these studies, the researchers focused on the specific case of audio-visual speech  where information is *redundant* across modalities. In the present work, we study the case of word reference where the audio-visual associations are usually *arbitrary* (Cite Saussure). For instance, whereas in the case of audio-visual speech, the sound "bee" and the corresponding lip movements are supposed to merge into a unified percept, there is nothing in the sound "bee" or in the way it can be pronounced, that correlates perceptually with the shape (or some other visual property) of the insect.

More generally, we know that the nature of the multi-modal associations (redundant vs. arbitrary) influences the quality of the processing in both children and adults [see @robinson2010 for a review]. In the case of redundant multimodal information (e.g. determining the size of an object through the visual and haptic modality), the sensory integration is often statistically optimal (Ernst and Banks, 2002). Optimality in this context means two things: first, the precision of the bimodal estimate increases compared to the unimodal estimates, and second, the cues from different modalities are combined according to their relative reliability. In the case of arbitrary associations, empirical studies suggest, rather, sub-optimality. In fact, processing of the bimodal input does not increase the precision, it tends to hinder performance instead (Sloutsky, Palmer, Stager, ... I should dig into the literature on the arbitrary associations). Moreover, modalities are not optimally weighted according to their reliability, instead, one modality dominates the other depending on the task, e.g., Sloutsky et al. reported auditory dominance (for kids) and then prefernce for adults (..), and Colovita et al. reported visual dominance for adults. 

-->

# Models and predictions

In this section we describe the multimodal cue combination model. First, we briefly introduce the experimental paradigm, and second we explain how behavior in this paradigm can be characterized optimally with an ideal observer model.

## The Audio-Visual Word Recognition Task

We introduce a new task that tests word recognition in a referential context.  We use two visual categories (cat and dog) and two auditory categories (/b/ and /d/ embedded in the minimal pair /aba/-/ada/). For each participant, an arbitrary pairing is set between the auditory and the visual categories, leading to two audio-visual word categories (e.g., dog-/aba/, cat-/ada/). In each trial, participants are presented with an audio-visual target (the prototype of the target category), immediately followed by an audio-visual test stimulus (Figure\ \@ref(fig:task)). The test stimulus may differ from the target in both the auditory and the visual components.  After these two presentations, participants press "same" or "different".

```{r task, fig.cap = "Overview of the task. In the audio-visual condition, participants are first presented with an audio-visual target (the prototype of the target category), immediately followed by an audio-visual test. The test may differ from the target in both the auditory and the visual components. After these two presentations, participants press `same' (i.e., the same category as the target) or `different' (not the same category). The auditory-only and visual-only conditions are similar to the audio-visual condition, except that only the sounds are heard, or only the pictures are shown, respectively.", fig.align = "center", out.width = "400px"}
knitr::include_graphics("pictures/task.png", dpi = 108)
```

This task is similar to the task introduced by @sloutsky2003, and used in subsequent studies in both children and adults [see @robinson2010 for a review]. However, in this original task, participants are asked whether or not the two audio-visual presentations are *identical*. Such task would allow us to probe audio-visual encoding, but not necessarily language understanding. The latter requires---in addition to the perceptual encoding---a categorical judgement, i.e, determining whether or not two similar tokens are members of the same phonological/semantic category. Thus, our task is category-based: Participants are asked to press "same" if they think the second item (the test) belonged to the same category as the first (target) (e.g.,  dog-/aba/), even if there is a slight difference in the sound, in the referent, or in both. They are instructed to press "different" only if they think that the second stimulus was an instance of the other word category (cat-/ada/). The task also includes trials where pictures are hidden (audio-only) or where sounds are muted (visual-only). These unimodal trials provide us with the participants' evaluation of the probabilistic information present in the auditory and visual categories. This unimodal distributions are used as inputs to the optimal combination model, described below.

<!--We assume, for the sake of simplicity, that category tokens vary along a single dimension. Phonetic tokens are sampled from an acoustic continuum linking /aba/ to /aba/, and obtained by continuously varying the value of the second formant [@vroomen2004]. Visual tokens are sampled from a morph continuum linking the picture of a dog and the picture of a cat [@freedman2001]. -->


## Optimal Model


<!--First we describe how probabilistic representations in each modality are derived, and second we explain how they are combined to yield optimal predictions. -->

<!--
### Unimodal condition

In each modality, we have two categories: /ada/ ($A=1$) and /aba/ ($A=2$) in the auditory dimension, and *cat* ($V=1$) and *dog* ($V=2$) in the visual dimension.

We assume a token's probability of membership in each category to be normally distributed:

$$ p(a | A) \sim  N(\mu_A, \sigma^2_A) $$
$$ p(v | V) \sim  N(\mu_V, \sigma^2_V) $$
The parameters of these distributions are fitted to the participants' judgement in the unimodal case, i.e., when they only hear the sounds, and when they only see the objects. For an ideal recognizer, the probability of choosing category 2 (that is, to answer "different") when presented with, say, an audio instance $a$, is the posterior probability of this category $p(A_2|a)$. If we assume that both categories have equal variances, the posterior probability reduces to:

$$p(A_2 | a)=\frac{1}{1+(1+\epsilon_A)\exp(\beta_{a0}+\beta_aa)}$$

with $\beta_a=\frac{\mu_{A_1}-\mu_{A_2}}{\sigma^2_{A}}$, $\beta_{a0}=\frac{\mu^2_{A_2}-\mu^2_{A_1}}{2\sigma^2_{A}}$ and $1+\epsilon_A=\frac{p(A_1)}{p(A_2)}$ is the proportion of the prior probabilities. If the identity of the auditory categories is randomized, and if $A_1$ is the target, then $\epsilon$ measures a response bias to "same" if $\epsilon > 0$, and a response bias to "different" if $\epsilon < 0$.

We modeled the visual representation in exactly the same way.

-->

The ideal observer model combines probabilistic information from the auditory and visual modalities, and makes a categorical decision. How can this task be characterized in a formal way, and what would be the optimal behavior? If the stimuli were continuous (e.g., size, location), cues derived from each modality would be characterized with their sensory noise variance only, and (assuming conditional independence) their combined information would be characterized with a simple linear combination rule where the combined estimate is an average of the individual estimates, each weighted according to its relative reliability [@ernst02; @alais04]. However, in our case decision is made over categorical variables (phonemes and concepts), the optimal model should take into account, not only the noise variability around an individual perceptual estimate, but also its categorical variability (i.e., the probability with which the estimate belongs to a given category). 

One way to account for the categorical variability is to model the decision step which maps sensory input onto task-dependent categories [@Bankieris17; @bejjanki2011]. We represent the decision step as follows (see Figure\ \@ref(fig:model) for illustration). If we assume that both the visual and the auditory cues vary along a single dimension, an audio-visual signal $(a,v)$ can be represented as a point in this two-dimensional space, and a word category can be characterized with a bivariate probability distribution over audio-visual tokens.\footnote{For the sake of clarity, we assume here that the sensory noise around this point is small/negligible compared to the categorical variability. But see below how the noise can be integrated when its effect is relatively large} Each category is peaked around the most typical audio-visual token, and the probability of membership in the category (the likelihood probability) decreases gradually as the auditory token deviates from the typical sound instance (depending on the variance of the auditory category) and/or as the visual token deviates from the typical visual instance (depending on the variance of the visual category). <!--To illustrate how categorical decision is made within this formal framework, suppose we receive an ambiguous image which can be either of a cat or of a dog, associated with a slightly noisy auditory input that sounds more like "cat". If the judgment is based on the picture only, this would lead to a random choice between cat and dog. However, if the judgement is based on both the picture and the sound (which is less ambiguous), the combined information should lead to a preference for the word cat  we illustrate the cateorical decision making. Audio-visual points along this line lead to random responses, meaning that both the auditory and the visual components are maximally ambiguous. Audio-visual points that lay on either side of the decision threshold lead to decisions in favor of one word or the other. This means that at least one component of this audio-visual token is not totally ambiguous, which leads to a diambiguated combined information. -->

A crucial feature of the ideal observer model is that it makes use of the entire probability distribution of a given category in order to make optimal decisions [e.g., @clayard08]. This means that the probability of picking a word (the posterior probability) should mirror closely the probability of the combined information (the likelihood probability). For instance, the model becomes less ambiguous and more certain as the tokens get closer to the typical instance of the category. In what follows, we formalize this sense of optimality using a Bayesian framework.
<!--characterized with a likelihood function, e.g., $P(S_V | S)$ and $P(S_A | S)$ for a visual and an auditory cue, respectively. If these cues are conditionally independent, their combined information can be written as $P(S_V, S_A | S)=P(S_A | S)P(S_V | S)$. The maximum likelihood estimate of this combined information $\hat{S}$ is an average of the individual maximum liklihoods $\hat{S}=w_V\hat{S_V}+w_A\hat{S_A}$, each wighted according to its relative reliability:
$w_V=\frac{1/\sigma_V^2}{1/\sigma_V^2 + 1/\sigma_A^2}$, and $w_A=\frac{1/\sigma_A^2}{1/\sigma_V^2 + 1/\sigma_A^2}$. 


If, however, the target is a categorical dimension, the optimal model should take into account, not only the noise variability arround an individual perceptual estimate, but also its categorical variability (i.e., the probability with which this estimate belongs to a given category). For example, upon hearing an audio-visual signal (the sound "cat" associated with the image of a cat), the ideal obsever has both to identify the sensory input and to map it onto the task-dependent categories.  

The optimal model that takes into account categorical variability is somewhat different, since -->



<!-- more adequate to the case of redundant information, where, for instance, cues are in the same units, the same coordinates and about the same aspect of the environmental property. It is less adequate to describe the case of arbitrary associations where cues are in different units, coordinate systems, or about complementary aspects of the same environmental property [see @ernst04 for further detail]. -->

An auditory category $A$ and a visual category $V$ are defined as distributions in the auditory and the visual dimensions, respectively. A word category $W$ is defined as a bivariate distribution in the audio-visual. More specifically, we define it as the joint distribution of the auditory and the visual categories.  For simplicity, we assume both auditory and visual categories to be normally distributed. That is, if $w=(a,v)$ is an audio-visual token, then:
$$ p(a | A) \sim  N(\mu_A, \sigma^2_A) $$
where $\mu_A$ and $\sigma^2_A$ are respectively the mean and the variance of the auditory category,
$$ p(v | V) \sim  N(\mu_V, \sigma^2_V) $$
where $\mu_V$ and $\sigma^2_V$ are  the mean and the variance of the visual category, and 

$$ p(w | W) \sim  N(M_W, \Sigma_W) $$
where $M_W=(\mu_A, \mu_V)$ and $\Sigma_W$ are mean and the covariance matrix of the word category. We assume that auditory and visual variables are independent (i.e., uncorrelated), so the covariance matrix is simply:
 \[
   \Sigma_W=
  \left[ {\begin{array}{cc}
   \sigma^2_A & 0 \\
   0 & \sigma^2_V \\
  \end{array} } \right]
\]

```{r model, fig.cap = "Illustration of the model using simulated data. A word category is defined as the joint bivariate distribution of an auditory category (horizontal, bottom panel) and a visual semantic category (vertical, left panel). Upon the presentation of a word token $w$, participants guess whether it is sampled from the word type $W_1$ or from the word type $W_2$. Decision threshold is where the guessing probability is 0.5.", fig.align = "center", out.width = "400px"}
knitr::include_graphics("pictures/model.png", dpi = 108)
```

This assumption simply says that, given a word-object mapping, e.g., W=("cat"-CAT), variation in the way "cat" is pronounced does not correlate with changes in any visual property of the object CAT, which is a valid assumption.\footnote{Note that this assumptions is more adequate in the case of arbitrary associations such as ours, and less so in the case of redundant association such as audio-visual speech. In the latter, variation in the pronunciation is expected to correlate, at least to some extent, with lip movements.}

Now we turn to the crucial question of modeling how optimal decision should proceed based on the probabilistic information in the auditory and the visual modalities. In our task, we have two word categories: dog-/aba/ ($W_1$) and cat-/ada/ ($W_2$).\footnote{This mapping is randomized in the experiments.} Participants can be understood as choosing one of these two word categories (Figure\ \@ref(fig:model)). For an ideal observer, the probability of choosing category 2 when presented with an audio-visual instance $w=(a,v)$ is the posterior probability of this category:
$$
p(W_2 | w)=\frac{p(w|W_2)p(W_2)}{p(w|W_2)p(W_2)+p(w|W_1)p(W_1)}
$$
Using our assumption that the cues are uncorrelated, we have
$$p(w | W) = p(a,v| W) = p(a| A)p(v| V)$$
Under this assumption, the posterior probability reduces to:\footnote{See the appendix for the details of the derivation}
\begin{equation}
 p(W_2 | w)=\frac{1}{1+(1+\epsilon)\exp(\beta_0+\beta_aa+\beta_vv)}
\end{equation}
where 
$$1+\epsilon=\frac{p(W_1)}{p(W_2)}$$
$$\beta_0=\frac{\mu^2_{A2}-\mu^2_{A1}}{2\sigma^2_{A}}+\frac{\mu^2_{V2}-\mu^2_{V1}}{2\sigma^2_{V}}$$

$$\beta_a=\frac{\mu_{A1}-\mu_{A2}}{\sigma^2_{A}}$$
$$\beta_v=\frac{\mu_{V1}-\mu_{V2}}{\sigma^2_{V}}$$ 

The parameter $\epsilon$ represents the differential between the categories' prior probabilities. However, since the identity of word categories is randomized across participants, $\epsilon$ measures, rather, a response bias to "same" if $\epsilon > 0$, and a response bias to "different" if $\epsilon < 0$. We expect a general bias towards answering "different" because of the categorical nature of our same-different task: When two items are ambiguous but perceptually different, participants might have a slight preference for "different" over "same".

<!--
We fix the values of the means as follows. Since the endpoints of the auditory and the visual continua are the most typical tokens, we assume that the categories are peaked around these endpoints, so we fix the values of the mean to be these endponts.  to be the endpoints of the corresponding continuum, meaning that <!--For example, if both the auditory and visual continua are made of 5 steps going from 0 to 4, then $\mu_{A1}=0$ and $\mu_{A2}=4$ (and similarly $\mu_{V1}=0$, and $\mu_{V2}=4$). --> The values of the means are fixed: They correspond to the value of the most typical tokens in our stimuli. Thus, observations from each modality ($a$ and $v$) are weighted according to their reliability in the posterior: $$\beta_a \propto \frac{1}{\sigma^2_{A}}$$ $$\beta_v \propto \frac{1}{\sigma^2_{V}}$$.

Note that the weights account for categorical variability only. For instance, if the speaker generates a target production $a_t$ from an auditory category
$p(a_t | A) \sim N(\mu_{A}, \sigma^2_{A})$, the ideal model assumes that it has direct access to this production token (i.e., $a=a_t$), and that all uncertainty is about the category membership of this token. However, we might also want to account for noise in the brain and/or in the environment.  For example, the observer might not have access to the exact produced target, but only to the target perturbed by noise. If we assume this noise to be normally distributed, that is,  $p(a | a_t) \sim N(a_t, \sigma^2_{N})$, then integrating over $a_t$ lead to the following simple expression:
$$ p(a | A) \sim N(\mu_{A}, \sigma^2_{A}+\sigma^2_{N})$$
Using an identical reasoning for the case of the visual modality, we end up with the following multimodal weighting scheme in the optimal combination model:

 $$\beta_a \propto \frac{1}{\sigma^2_{A}+\sigma^2_{N_A}}$$ $$\beta_v \propto \frac{1}{\sigma^2_{V} +\sigma^2_{N_V}}$$
To summarize, the posterior provides the optimal model's predictions for how probabilities that characterize uncertainty in each modality can be combined to make categorical decision about the bimodal input. The model will be constructed based on data from the unimodal trials, but the predictions of the optimal model will be compared to human responses in the bimodal trials.

## Auditory and Visual models

Besides the optimal model, we also test the predictions of two baselines models: a visual model, which assumes that participants rely only on visual information, and an auditory model, which assumes that participants rely only on auditory information. If participants rely on both the auditory and the visual modalities, the optimal model would explain more variance in human responses than the visual model alone or the auditory model alone.

## Descriptive model

The visual, auditory, and optimal models are *normative* models. Their predictions are made about human data in the bimodal condition, but their crucial parameters (i.e., variances associated with the visual and auditory modalities) are derived from data in the unimodal conditions.
In addition to these normative models, we consider a *descriptive* model. The parameters of this model are fit to human data in the bimodal condition. If the referential task induces sub-optimality (due, for instance, to the arbitrary nature of the sound-object association), then we predict that the descriptive model should explain more variance than the optimal model does. 

A systematic comparison of the optimal and the descriptive models would allow us, not only to quantify how much people deviate from optimality, but also to understand precisely how they deviate from this optimality. Let $\sigma^2_{A}$ and $\sigma^2_{V}$ be the values of the variances used in the optimal model (derived from the unimodal conditions), and $\sigma^2_{Ab}$ and $\sigma^2_{Vb}$ be the values observed through the descriptive model in the bimodal condition. Deviation from optimality is measured in two ways. First, we measure the change in the values of the variances specific to each modality, that is, how $\sigma^2_{A}$ compares to $\sigma^2_{Ab}$, and how $\sigma^2_{V}$ compares to $\sigma^2_{Vb}$. Second, we measure changes in the proportion of the visual and auditory variances, i.e., how $\frac{\sigma^2_{A}}{\sigma^2_{V}}$ compares to $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}}$. The first measure allows us to test if precision changes for each modality when we move from the unimodal to the bimodal conditions. The second allows us to test whether or not the weighting scheme follows the prediction of the optimal model.  The reason we used the proportion of the variances as a measure of cross-modal weighing is because it corresponds to the slope\footnote{Or more precisely the absolute value of the slope} of the decision threshold in the audio-visual space (Figure\ \@ref(fig:model)). The decision threshold is defined as the set of values in this audio-visual space along which the posterior is equal to 0.5. Formally speaking, the decision threshold takes the following form:

$$v=-\frac{\sigma^2_V}{\sigma^2_A}a+v_0$$

If the absolute value of the slope derived from the descriptive model is greater than that of the optimal model, the corresponding shift in the decision threshold indicates that participants have a preference for the auditory modality in the bimodal case. Similarly, a smaller absolute value of the slope would lead to a preference for the visual modality. The limit cases are when there is exclusive reliance on the auditory cue (a vertical line), and where there is exclusive reliance on the visual (a horizontal line). 

It follows that there are three possible scenarios describing how humans can deviate from optimality. These scenarios are illustrated in Figure\ \@ref(fig:subOptim) and are as follows: 

1) Both variances may increase, but their proportion remains the same. That is, $\sigma^2_{Ab} \geqslant \sigma^2_{A}$ and $\sigma^2_{Vb} \geqslant \sigma^2_{V}$, but  $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}} \approx \frac{\sigma^2_{A}}{\sigma^2_{V}}$. In this case, sub-optimality would be due to increased randomness in human responses in the bimodal condition. However, this randomness would not affect the relative weighting of both modalities, i.e., participants would still weigh modalities according to the relative reliability predicted by the optimal model.

2) The auditory variance increases at a higher rate.  That is, $\sigma^2_{Ab} \gg \sigma^2_{A}$ and $\sigma^2_{Vb} \geqslant \sigma^2_{V}$, leading to $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}} > \frac{\sigma^2_{A}}{\sigma^2_{V}}$. In this case, sub-optimally would consist not only in participants being more random in the bimodal condition, but also in having a systematic preference for the visual modality, even after accounting for informational reliability. 

3) The visual variance increases at a higher rate. That is, $\sigma^2_{Vb} \gg \sigma^2_{V}$, and  $\sigma^2_{Ab} \geqslant \sigma^2_{A}$, leading to $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}} > \frac{\sigma^2_{A}}{\sigma^2_{V}}$. This case is the reverse of case 2, i.e., in addition to increased randomness in the bimodal condition, there is a systematic preference for the auditory modality, even after accounting for informational reliability. 

```{r subOptim, fig.cap = "Illustration using simulated data showing the example of a prediction made by the optimal model (top), and the three possible ways human participants can deviate from this prediction (bottom). These cases are the following: 1) The variance increases equally for both modalities, but the weighting scheme (characterized by the decision threshold) is optimal, 2) The auditory variance increases at a higher rate, leading to a preference for the auditory modality, and 3) The visual variance increases at a higher rate, leading to a preference for the visual modality", fig.align = "center", out.width = "400px"}
knitr::include_graphics("pictures/sub-optimal", dpi = 108)
```

We tested the predictions of the optimal model in three experiments. In Experiment 1, we studied the case where bimodal uncertainty was due to ambiguity in terms of category membership, without any additional background noise. In Experiment 2 and 3 we added background noise on top of ambiguity in category membership. 

# Experiment 1

In this Experiment, we start with testing the predictions in the case where uncertainty is due to ambiguity in terms of category membership only, that is, according to the following weighting scheme:  $$\beta_a \propto \frac{1}{\sigma^2_{A}}$$ $$\beta_v \propto \frac{1}{\sigma^2_{V}}$$ We do not introduce any additional noise in the background, and we assume that sensory noise is negligible compared to categorical variability.  

## Methods

### Participants:

We recruited a planned sample of `r N_all_1` participants from Amazon Mechanical Turk. Only participants with US IP addresses and a task approval rate above 85\% were allowed to participate. They were paid at an hourly rate of \$6/hour. Participants were excluded if they reported having experienced a technical problem of any sort during the online experiment (N=`r N_all_1 - N_noProb_1`), or if they had less than 50\% accurate responses on the unambiguous training trials (N=`r N_noProb_1 - N_good_1`). The final sample consisted of (N = `r N_good_1`) participants.\footnote{The sample size and exclusion criteria were specified in the pre-registration at https://osf.io/h7mzp/.}

### Stimuli:
For auditory stimuli, we used the continuum introduced in @vroomen2004, a 9-point /aba/--/ada/ speech continuum created by varying the frequency of the second (F2) formant in equal steps. We selected 5 equally spaced points from the original continuum by keeping the endpoints (prototypes) 1 and 9, as well as points 3, 5, and 7 along the continuum. For visual stimuli, we used a morph continuum introduced in @freedman2001. From the original 14 points, we selected 5 points as follows: we kept the item that seemed most ambiguous (point 8), the 2 preceding points (i.e., 7 and 6) and the 2 following points (i.e., 9 and 10). The 6 and 10 points along the morph were quite distinguishable, and we took them to be our prototypes. 

### Design and Procedure
We told participants that an alien was naming two objects: a dog, called "aba" in the alien language, and a cat, called "ada". In each trial, we presented the first object (the target) on the left side of the screen simultaneously with the corresponding sound. For each participant, the target was always the same (e.g., dog-/aba/). The second sound-object pair (the test) followed on the other side of the screen after 500ms and varied in its category membership. For both the target and the test, visual stimuli were present for the duration of the sound clip ($\sim$ 800ms). We instructed participants to press "S" for same if they thought the alien was naming another dog-/aba/, and "D" for different if they thought the alien was naming a cat-/ada/. We randomized the sound-object mapping (e.g., dog-/aba/, cat-/ada/) as well as the identity of the target (dog or cat) across participants.

The first part of the experiment trained participants using only the prototype pictures and the prototype sounds (12 trials, 4 each from the bimodal, audio-only, and visual-only conditions). After completing training, we instructed participants on the structure of the task and encouraged them to base their answers on both the sounds and the pictures (in the bimodal condition). There were a total of 25 possible combinations in the bimodal condition, and 5 in each of the unimodal conditions. Each participant saw each possible trial twice, for a total of 70 trials/participant. Trials were blocked by condition and blocks were presented in random order.


```{r echo=FALSE}

#Bootstrap sample parameters

lmfit <- function(data, indices) {
  
  myd = data[indices, ]
  
  s_data <- myd %>%
    filter(condition == "sound")

  v_data <- myd %>%
    filter(condition == "concept") 

  j_data <- myd %>%
    filter(condition == "joint")
    
  s_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=s_data, start = list(e=0, vrA=2), nls.control(warnOnly = TRUE))

 c_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=v_data, start = list(e=0, vrV=2), nls.control(warnOnly = TRUE))

 j_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=j_data, start = list(e=0, vrA_j=2, vrV_j=2), nls.control(warnOnly = TRUE))

  
  s_va <- coef(s_nl)["vrA"]
  s_e <- coef(s_nl)["e"]
  
  v_va <- coef(c_nl)["vrV"]
  v_e <- coef(c_nl)["e"]
  
  decision_ideal=v_va/s_va

  j_va_s=coef(j_nl)["vrA_j"]
  j_va_v=coef(j_nl)["vrV_j"]
  j_e=coef(j_nl)["e"]
  
  decision_fit=j_va_v/j_va_s
  
  preference = decision_fit/decision_ideal
  
  MyBoot=c(s_va, s_e, v_va, v_e, j_va_s, j_va_v, j_e, decision_ideal, decision_fit, preference)
  
  
  return(MyBoot) 

  }

#results1 <- boot(data=exp1_good, statistic = lmfit, R = 10000)

#dataSave1 <- data.frame(matrix(ncol = 5, nrow = 0))
#data_names <- c("variable", "estimate", "lower","upper", "Experiment")
#colnames(dataSave1) <- data_names

#Auditory variance:
#audVar=boot.ci(results1, index = 1, type = c("bca"), conf = 0.95)

#v1 <- data.frame('audVar', as.numeric(audVar$t0), as.numeric(audVar$bca[4]), as.numeric(audVar$bca[5]), 'Exp1')
#  colnames(v1) <- data_names

#Auditory bias:
#audBias=boot.ci(results1, index = 2, type = c("bca"), conf = 0.95)

#v2 <- data.frame('audBias', as.numeric(audBias$t0), as.numeric(audBias$bca[4]), as.numeric(audBias$bca[5]), 'Exp1')
#colnames(v2) <- data_names

#Visual variance:
#visVar=boot.ci(results1, index = 3, type = c("bca"), conf = 0.95)

#v3 <- data.frame('visVar', as.numeric(visVar$t0), as.numeric(visVar$bca[4]), as.numeric(visVar$bca[5]), 'Exp1')
#colnames(v3) <- data_names


#Visual bias:
#visBias=boot.ci(results1, index = 4, type = c("bca"), conf = 0.95)

#v4 <- data.frame('visBias', as.numeric(visBias$t0), as.numeric(visBias$bca[4]), as.numeric(visBias$bca[5]), 'Exp1')
#colnames(v4) <- data_names


#Bimodal Auditory variance:
#audVarBi=boot.ci(results1, index = 5, type = c("bca"), conf = 0.95)

#v5 <- data.frame('audVarBi', as.numeric(audVarBi$t0), as.numeric(audVarBi$bca[4]), as.numeric(audVarBi$bca[5]), 'Exp1')
#colnames(v5) <- data_names


#Bimodal Visual variance:
#visVarBi=boot.ci(results1, index = 6, type = c("bca"), conf = 0.95)

#v6 <- data.frame('visVarBi', as.numeric(visVarBi$t0), as.numeric(visVarBi$bca[4]), as.numeric(visVarBi$bca[5]), 'Exp1')
#colnames(v6) <- data_names

#Bimodal bias:
#BiasBi=boot.ci(results1, index = 7, type = c("bca"), conf = 0.95)

#v7 <- data.frame('BiasBi', as.numeric(BiasBi$t0), as.numeric(BiasBi$bca[4]), as.numeric(BiasBi$bca[5]), 'Exp1')
#colnames(v7) <- data_names

#Ideal modality wieghing
#prefIdeal=boot.ci(results1, index = 8, type = c("bca"), conf = 0.95)

#v8 <- data.frame('prefIdeal', as.numeric(prefIdeal$t0), as.numeric(prefIdeal$bca[4]), as.numeric(prefIdeal$bca[5]), 'Exp1')
#colnames(v8) <- data_names


#fit modality wieghing
#prefFit=boot.ci(results1, index = 9, type = c("bca"), conf = 0.95)

#v9 <- data.frame('prefFit', as.numeric(prefFit$t0), as.numeric(prefFit$bca[4]), as.numeric(prefFit$bca[5]), 'Exp1')
#colnames(v9) <- data_names


#Modality bias 
#bias=boot.ci(results1, index = 10, type = c("bca"), conf = 0.95)

#v10 <- data.frame('bias', as.numeric(bias$t0), as.numeric(bias$bca[4]), as.numeric(bias$bca[5]), 'Exp1')
#colnames(v10) <- data_names



#dataSave1 <- bind_rows(dataSave1, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10)

#feather::write_feather(dataSave1, "dataExp1.feather")

dataSave1 <- feather::read_feather("dataExp1.feather")

###### Extract values from saved data 

audVar_val=dataSave1$estimate[which(dataSave1$variable == "audVar")]
audVar_ci1=dataSave1$lower[which(dataSave1$variable == "audVar")]
audVar_ci2=dataSave1$upper[which(dataSave1$variable == "audVar")]

audBias_val=-dataSave1$estimate[which(dataSave1$variable == "audBias")]
audBias_ci1=-dataSave1$upper[which(dataSave1$variable == "audBias")]
audBias_ci2=-dataSave1$lower[which(dataSave1$variable == "audBias")]

visVar_val=dataSave1$estimate[which(dataSave1$variable == "visVar")]
visVar_ci1=dataSave1$lower[which(dataSave1$variable == "visVar")]
visVar_ci2=dataSave1$upper[which(dataSave1$variable == "visVar")]

visBias_val=-dataSave1$estimate[which(dataSave1$variable == "visBias")]
visBias_ci1=-dataSave1$upper[which(dataSave1$variable == "visBias")]
visBias_ci2=-dataSave1$lower[which(dataSave1$variable == "visBias")]

audVarBi_val=dataSave1$estimate[which(dataSave1$variable == "audVarBi")]
audVarBi_ci1=dataSave1$lower[which(dataSave1$variable == "audVarBi")]
audVarBi_ci2=dataSave1$upper[which(dataSave1$variable == "audVarBi")]

visVarBi_val=dataSave1$estimate[which(dataSave1$variable == "visVarBi")]
visVarBi_ci1=dataSave1$lower[which(dataSave1$variable == "visVarBi")]
visVarBi_ci2=dataSave1$upper[which(dataSave1$variable == "visVarBi")]

BiasBi_val=-dataSave1$estimate[which(dataSave1$variable == "BiasBi")]
BiasBi_ci1=-dataSave1$upper[which(dataSave1$variable == "BiasBi")]
BiasBi_ci2=-dataSave1$lower[which(dataSave1$variable == "BiasBi")]

prefIdeal_val_1=dataSave1$estimate[which(dataSave1$variable == "prefIdeal")]
prefIdeal_ci1_1=dataSave1$lower[which(dataSave1$variable == "prefIdeal")]
prefIdeal_ci2_1=dataSave1$upper[which(dataSave1$variable == "prefIdeal")]


bias_val_1=dataSave1$estimate[which(dataSave1$variable == "bias")]
bias_ci1_1=dataSave1$lower[which(dataSave1$variable == "bias")]
bias_ci2_1=dataSave1$upper[which(dataSave1$variable == "bias")]

```



```{r}
#Michael suggestion:
#For a given particiapnts, sample 


```



```{r}

#Fit the non-linear function

##Experiment 1
#############
sound_nl1 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=sound_all_exp1, start = list(e=0, vrA=2))

concept_nl1 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=concept_all_exp1, start = list(e=0, vrV=2))

#Here I should test the cross-validation

joint_nl1 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=joint_all_exp1, start = list(e=0, vrA_j=2, vrV_j=2))

##extract coefficient
eA_nl1 <- coef(sound_nl1)["e"]
vrA_nl1 <- coef(sound_nl1)["vrA"]

eV_nl1 <- coef(concept_nl1)["e"]
vrV_nl1 <- coef(concept_nl1)["vrV"]

eJ_nl1 <- coef(joint_nl1)["e"]
vrJ_A_nl1 <- coef(joint_nl1)["vrA_j"]
vrJ_V_nl1 <- coef(joint_nl1)["vrV_j"]
#######

x <- seq(0, 4, 0.01)

y_sound_nl1 <- predict(sound_nl1, list(sound_dist = x), type="response")
y_concept_nl1 <- predict(concept_nl1, list(concept_dist = x), type="response")

uniS_nl1 <- data.frame(distance=x, prediction=y_sound_nl1) %>%
  mutate(Condition = 'Auditory', 
         Experiment ='Experiment 1')

uniV_nl1 <- data.frame(distance=x, prediction=y_concept_nl1) %>%
  mutate(Condition = 'Visual',
         Experiment ='Experiment 1')


##Experiment 2
###############

sound_nl2 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=sound_all_exp2, start = list(e=0, vrA=2))

concept_nl2 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=concept_all_exp2, start = list(e=0, vrV=2))



joint_nl2 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=joint_all_exp2, start = list(e=0, vrA_j=2, vrV_j=2))

##extract coefficient
eA_nl2 <- coef(sound_nl2)["e"]
vrA_nl2 <- coef(sound_nl2)["vrA"]

eV_nl2 <- coef(concept_nl2)["e"]
vrV_nl2 <- coef(concept_nl2)["vrV"]

eJ_nl2 <- coef(joint_nl2)["e"]
vrJ_A_nl2 <- coef(joint_nl2)["vrA_j"]
vrJ_V_nl2 <- coef(joint_nl2)["vrV_j"]
#######

x <- seq(0, 4, 0.01)

y_sound_nl2 <- predict(sound_nl2, list(sound_dist = x), type="response")
y_concept_nl2 <- predict(concept_nl2, list(concept_dist = x), type="response")

uniS_nl2 <- data.frame(distance=x, prediction=y_sound_nl2) %>%
  mutate(Condition = 'Auditory', 
         Experiment ='Experiment 2')

uniV_nl2 <- data.frame(distance=x, prediction=y_concept_nl2) %>%
  mutate(Condition = 'Visual', 
         Experiment ='Experiment 2')


##Experiment 3
###############

sound_nl3 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=sound_all_exp3, start = list(e=0, vrA=2))

concept_nl3 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=concept_all_exp3, start = list(e=0, vrV=2))

joint_nl3 <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=joint_all_exp3, start = list(e=0, vrA_j=2, vrV_j=2))

##extract coefficient
eA_nl3 <- coef(sound_nl3)["e"]
vrA_nl3 <- coef(sound_nl3)["vrA"]

eV_nl3 <- coef(concept_nl3)["e"]
vrV_nl3 <- coef(concept_nl3)["vrV"]

eJ_nl3 <- coef(joint_nl3)["e"]
vrJ_A_nl3 <- coef(joint_nl3)["vrA_j"]
vrJ_V_nl3 <- coef(joint_nl3)["vrV_j"]
#######

x <- seq(0, 4, 0.01)

y_sound_nl3 <- predict(sound_nl3, list(sound_dist = x), type="response")
y_concept_nl3 <- predict(concept_nl3, list(concept_dist = x), type="response")

uniS_nl3 <- data.frame(distance=x, prediction=y_sound_nl3) %>%
  mutate(Condition = 'Auditory', 
         Experiment ='Experiment 3')

uniV_nl3 <- data.frame(distance=x, prediction=y_concept_nl3) %>%
  mutate(Condition = 'Visual', 
         Experiment ='Experiment 3')

all_uni_model <- bind_rows(uniS_nl1, uniV_nl1,
                           uniS_nl2, uniV_nl2,
                           uniS_nl3, uniV_nl3)



all_plot <- ggplot(exp_uni_data, 
       aes(x = distance, y = mean)) + 
  geom_point()+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1)) + 
  #geom_line(data=uniV,aes(x=xV, y=yV))+
  geom_line(data=all_uni_model, aes(x=distance, y=prediction), col='black')+
  xlab("Distance") +ylab("Prob. different")+
  scale_y_continuous(limits = c(0, 1))+#theme(aspect.ratio = 0.7)+
  theme(aspect.ratio = 0.7) + facet_grid(Experiment ~ Condition)
  #stat_function(fun = Logistic_v, colour="red"))


```

```{r unimodal, fig.cap = "Human responses in the unimodal condition. Points represent the proportion of `different' to `same' responses in the auditory-only condition (left), and visual-only condition (right). Error bars are 95\\% confidence intervals. Solid lines represent logistic fits.", fig.align = "center", out.width = "400px"}

all_plot

```

### Model fitting details
#### Unimodal condition
Remember that data in this conditions allow us to derive the variances of both the auditory and the visual categories, and that these variances are used to make predictions about bimodal data (in the visual and auditory baselines as well as in the optimal model). These individual variances were derived as follows (we explain the derivation for the auditory-only case, but the same applies for the visual-only case). We use the same Bayesian reasoning as we did in the derivation of the bimodal model: When presented with an audio instance $a$, the probability of choosing the sound category 2 (that is, to answer "different") is the posterior probability of this category $p(A_2|a)$. If we assume that both sound categories have equal variances, the posterior probability reduces to:

$$p(A_2 | a)=\frac{1}{1+(1+\epsilon_A)\exp(\beta_{a0}+\beta_aa)}$$

with $\beta_a=\frac{\mu_{A_1}-\mu_{A_2}}{\sigma^2_{A}}$ and  $\beta_{a0}=\frac{\mu^2_{A_2}-\mu^2_{A_1}}{2\sigma^2_{A}}$. $\epsilon_A$ is the response bias in the auditory-only condition. For this model (as well as all other models in this study), we fixed the values of the means to be the end-points of the corresponding continuum, since these points are the most typical instances in our stimuli. Thus, we have $\mu_{A1}=0$ and $\mu_{A2}=4$ (and similarly $\mu_{V1}=0$, and $\mu_{V2}=4$). This leaves us with two free parameters: the bias $\epsilon_A$ and the variance $\sigma^2_{A}$. To determine the values of these parameters, we fit the unimodal posterior to human data in the unimodal case. 

#### Bimodal condition
In this condition, only the descriptive model is fit to the data, using the expression of the posterior (Equation 1). Since the values of the means are fixed, we have 3 free parameters: the variances for the visual and the auditory modalities, respectively, and $\epsilon$, the response bias.  The visual and auditory baselines as well as the optimal model are not fit to bimodal data, but their predictions are tested against these bimodal data. All these normative models use the variances derived from the unimodal data and the bias term derived from the bimodal data. 

Although the paradigm is within-subjects, we did not have enough statistical power to fit a different model for each individual participant. Instead, models are constructed with data collapsed across all participants. The fit was done with a nonlinear least squares regression using the NLS package in R [@bates88]. We computed the values of the parameters, as well as their 95% confidence intervals, through non-parametric bootstrap (using 10000 iterations).

## Results and analysis

### Unimodal conditions

Average categorization judgments and best fits are shown in Figure\ \@ref(fig:unimodal). The categorization function of the auditory condition was slightly steeper than that of the visual condition, meaning that participants perceived the sound tokens slightly more categorically and whih higher certainty than they did with the visual tokens.  For the auditory modality, we obtained the following values:^[all CIs in the paper are 95% confidence intervals] $\epsilon_A=$ `r audBias_val`  [`r audBias_ci2`, `r audBias_ci1`] and $\sigma^2_A=$ `r audVar_val` [`r audVar_ci1`, `r audVar_ci2`]. For the visual modality, we obtained $\epsilon_V=$ `r visBias_val` [`r visBias_ci2`, `r visBias_ci1`] and $\sigma^2_V=$ `r visVar_val` [`r visVar_ci1`, `r visVar_ci2`]`.

### Bimodal condition

```{r  echo=FALSE}

#All models

#Here we use the values obtained from the fitting using bootstrap sampling to 

##Experiment 1
##############

#The fit
model_fit1 <- function (x,y) {
   1/(1 + (1-eJ_nl1)*exp((-4/vrJ_A_nl1)*x+(-4/vrJ_V_nl1)*y+(8/vrJ_A_nl1)+(8/vrJ_V_nl1)))
}

#The ideal
model_ideal1 <- function (x,y) {
    1/(1 + (1-eJ_nl1)*exp((-4/vrA_nl1)*x+(-4/vrV_nl1)*y+(8/vrA_nl1)+(8/vrV_nl1)))
}

#The sound-only model
model_sound1 <- function (x,y) {
  1/(1 + (1-eJ_nl1)*exp((-4/vrA_nl1)*x+(8/vrA_nl1)))
}

#The visual-only model
model_concept1 <- function (x,y) {
  1/(1 + (1-eJ_nl1)*exp((-4/vrV_nl1)*y+(8/vrV_nl1)))
}

##Experiment 2
##############

#The fit
model_fit2 <- function (x,y) {
   1/(1 + (1-eJ_nl2)*exp((-4/vrJ_A_nl2)*x+(-4/vrJ_V_nl2)*y+(8/vrJ_A_nl2)+(8/vrJ_V_nl2)))
}

#The ideal
model_ideal2 <- function (x,y) {
    1/(1 + (1-eJ_nl2)*exp((-4/vrA_nl2)*x+(-4/vrV_nl2)*y+(8/vrA_nl2)+(8/vrV_nl2)))
}

#The sound-only model
model_sound2 <- function (x,y) {
  1/(1 + (1-eJ_nl2)*exp((-4/vrA_nl2)*x+(8/vrA_nl2)))
}

#The visual-only model
model_concept2 <- function (x,y) {
  1/(1 + (1-eJ_nl2)*exp((-4/vrV_nl2)*y+(8/vrV_nl2)))
}


##Experiment 3
##############

#The fit
model_fit3 <- function (x,y) {
   1/(1 + (1-eJ_nl3)*exp((-4/vrJ_A_nl3)*x+(-4/vrJ_V_nl3)*y+(8/vrJ_A_nl3)+(8/vrJ_V_nl3)))
}

#The ideal
model_ideal3 <- function (x,y) {
    1/(1 + (1-eJ_nl3)*exp((-4/vrA_nl3)*x+(-4/vrV_nl3)*y+(8/vrA_nl3)+(8/vrV_nl3)))
}

#The sound-only model
model_sound3 <- function (x,y) {
  1/(1 + (1-eJ_nl3)*exp((-4/vrA_nl3)*x+(8/vrA_nl3)))
}

#The visual-only model
model_concept3 <- function (x,y) {
  1/(1 + (1-eJ_nl3)*exp((-4/vrV_nl3)*y+(8/vrV_nl3)))
}


```

```{r echo=FALSE, fig.width=7, fig.height=8}


models_exp1 <- joint_exp1 %>% 
  rename(joint = mean) %>%
  mutate(Descriptive = model_fit1(sound_dist, concept_dist)) %>%
  mutate(Optimal = model_ideal1(sound_dist, concept_dist)) %>%
  mutate(Auditory = model_sound1(sound_dist, concept_dist)) %>%
  mutate(Visual = model_concept1(sound_dist, concept_dist)) %>%
  gather(model, pred, Visual, Auditory, Optimal, Descriptive) %>%
  mutate(experiment = 'Experiment 1')

models_exp2 <- joint_exp2 %>% 
  rename(joint = mean) %>%
  mutate(Descriptive = model_fit2(sound_dist, concept_dist)) %>%
  mutate(Optimal = model_ideal2(sound_dist, concept_dist)) %>%
  mutate(Auditory = model_sound2(sound_dist, concept_dist)) %>%
  mutate(Visual = model_concept2(sound_dist, concept_dist)) %>%
  gather(model, pred, Visual, Auditory, Optimal, Descriptive) %>%
  mutate(experiment = 'Experiment 2')

models_exp3 <- joint_exp3 %>% 
  rename(joint = mean) %>%
  mutate(Descriptive = model_fit3(sound_dist, concept_dist)) %>%
  mutate(Optimal = model_ideal3(sound_dist, concept_dist)) %>%
  mutate(Auditory = model_sound3(sound_dist, concept_dist)) %>%
  mutate(Visual = model_concept3(sound_dist, concept_dist)) %>%
  gather(model, pred, Visual, Auditory, Optimal, Descriptive) %>%
  mutate(experiment = 'Experiment 3')


models_all <- bind_rows (models_exp1, models_exp2, models_exp3) 

models_all$model <- factor(models_all$model, levels = c('Visual','Auditory', 'Optimal', 'Descriptive'))

correlation_plot <- ggplot(models_all, 
       aes(x = pred, y = joint, col = factor(concept_dist), 
           shape = factor(sound_dist))) + 
  geom_point()+
 #geom_pointrange(aes(ymin = summary_ci_lower, ymax = summary_ci_upper), 
  #                position = position_dodge(width = .1), size=0.2) + 

  geom_abline(slope = 1, lty = 2) +
  xlab("Predictions") +ylab("Human data")+
  facet_grid(experiment ~ model)+
theme(aspect.ratio = 0.7, 
      axis.text=element_text(size=6),
      strip.text.y = element_text(size = 8))+
  guides(color=guide_legend(title="Visual Distance")) +
  guides(shape=guide_legend(title="Auditory Distance")) 


#Correlation values 

#Exp 1
optimal_1 <- subset(models_all, model=='Optimal' & experiment=='Experiment 1')
descriptive_1 <- subset(models_all, model=='Descriptive' & experiment=='Experiment 1')
auditory_1 <- subset(models_all, model=='Auditory' & experiment=='Experiment 1')
visual_1 <- subset(models_all, model=='Visual' & experiment=='Experiment 1')

R2_optimal_1  <- cor(optimal_1$joint, optimal_1$pred)^2 %>%
  round(2)
R2_descriptive_1 <- cor(descriptive_1$joint, descriptive_1$pred)^2 %>%
  round(2)
R2_auditory_1  <- cor(auditory_1$joint, auditory_1$pred)^2 %>%
  round(2)
R2_visual_1  <- cor(visual_1$joint, visual_1$pred)^2 %>%
  round(2)

#Exp 2
optimal_2 <- subset(models_all, model=='Optimal' & experiment=='Experiment 2')
descriptive_2 <- subset(models_all, model=='Descriptive' & experiment=='Experiment 2')
auditory_2 <- subset(models_all, model=='Auditory' & experiment=='Experiment 2')
visual_2 <- subset(models_all, model=='Visual' & experiment=='Experiment 2')

R2_optimal_2  <- cor(optimal_2$joint, optimal_2$pred)^2  %>%
  round(2)
R2_descriptive_2 <- cor(descriptive_2$joint, descriptive_2$pred)^2 %>%
  round(2)
R2_auditory_2  <- cor(auditory_2$joint, auditory_2$pred)^2 %>%
  round(2)
R2_visual_2  <- cor(visual_2$joint, visual_2$pred)^2 %>%
  round(2)


#Exp 3
optimal_3 <- subset(models_all, model=='Optimal' & experiment=='Experiment 3')
descriptive_3 <- subset(models_all, model=='Descriptive' & experiment=='Experiment 3')
auditory_3 <- subset(models_all, model=='Auditory' & experiment=='Experiment 3')
visual_3 <- subset(models_all, model=='Visual' & experiment=='Experiment 3')

R2_optimal_3  <- cor(optimal_3$joint, optimal_3$pred)^2 %>%
  round(2)
R2_descriptive_3 <- cor(descriptive_3$joint, descriptive_3$pred)^2 %>%
  round(2)
R2_auditory_3  <- cor(auditory_3$joint, auditory_3$pred)^2 %>%
  round(2)
R2_visual_3  <- cor(visual_3$joint, visual_3$pred)^2 %>%
  round(2)



```




```{r}

#Explore cross-validation here. As the numbers show, the predictions on the held-out subset is almost as acurate as the predcitions on the training set.  Over-fitting is generally not a problem with such simple models)

#Here is use hald the responses to predic the other half
#The function takes the data to be cross-validated and return a score (coefficient of determination)

crossValid <- function(data) {

n <- nrow(data)
frac <- 0.5
ix <- sample(n, frac * n) # indexes of in sample rows


fo <- answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j)))

fm <- nls(fo, data, start = list(e=0, vrA_j=2, vrV_j=2), subset = ix) # fit the model to a subset (here half) the responses 

#Extract the values of the parameter

data.test <- data[-ix, ] # out of sample data
data.train <- data[ix, ]


data.train.means <- data.train %>%
  group_by(concept_dist, sound_dist) %>%
  summarise(mean = mean(answer)) 

data.test.means <- data.test %>%
  group_by(concept_dist, sound_dist) %>%
  summarise(mean = mean(answer)) 

pred.train <-  predict(fm, new = data.train.means)
pred.test <- predict(fm, new = data.test.means)

cor.train <- cor(data.train.means$mean, pred.train)^2 
cor.test <- cor(data.test.means$mean, pred.test)^2

return(cor.test)

}


crossValid_exp1 <- replicate(10, {
  crossValid(joint_all_exp1)
  })


CV_exp1_mean = mean(crossValid_exp1)
error <- qt(0.975,df=length(crossValid_exp1)-1)*sd(crossValid_exp1)/sqrt(length(crossValid_exp1))
CV_exp1_lower = CV_exp1_mean - error
CV_exp1_upper = CV_exp1_mean + error

crossValid_exp2 <- replicate(10, {
  crossValid(joint_all_exp2)
  })

CV_exp2_mean = mean(crossValid_exp2)
error <- qt(0.975,df=length(crossValid_exp2)-1)*sd(crossValid_exp2)/sqrt(length(crossValid_exp2))
CV_exp2_lower = CV_exp2_mean - error
CV_exp2_upper = CV_exp2_mean + error


crossValid_exp3 <- replicate(10, {
  crossValid(joint_all_exp3)
  })

CV_exp3_mean = mean(crossValid_exp3)
error <- qt(0.975,df=length(crossValid_exp3)-1)*sd(crossValid_exp3)/sqrt(length(crossValid_exp3))
CV_exp3_lower = CV_exp3_mean - error
CV_exp3_upper = CV_exp3_mean + error


```

```{r bimodal, fig.cap = "Human responses vs. models' predictions in the bimodal condition. Shape represents auditory distance from the target, and color represents visual distance from the target.", fig.align = "center", out.width = "400px", fig.width=7, fig.height=8}

correlation_plot

```

#### Normative models
Figure\ \@ref(fig:bimodal) compares the predictions of the normative models against human responses. The visual, auditory and optimal model explained, respectively, `r 100*R2_visual_1`%, `r 100*R2_auditory_1`%, and `r 100*R2_optimal_1`% of total variance in mean responses.

#### Descriptive model
In the descriptive model, all parameters are fit to human responses in the bimodal condition. We found $\epsilon=$ `r BiasBi_val` [`r BiasBi_ci2`, `r BiasBi_ci1`], $\sigma^2_{Ab}=$ `r audVarBi_val` [`r audVarBi_ci1`, `r audVarBi_ci2`] and $\sigma^2_{Vb}=$ `r visVarBi_val` [`r visVarBi_ci1`, `r visVarBi_ci2`]. Note that the variance of both the auditory and visual modalities increased compared to the unimodal conditions. 

The descriptive model explained `r 100*R2_descriptive_1`% of total variance.  However, since the descriptive model was fit to the same data, there is a risque that this high correlation is due to overfitting. To examine this possibility, we cross-validated the model using half the responses to predict the other half (averaging across 1000 random partitions). The predictive power of the model remained very high ($r^2$=`r CV_exp1_mean`).

#### Cue combination and Modality preference
We next analyzed if cue combination was performed in an optimal way, or if there was a systematic preference for one modality when making decisions in the bimodal condition.
As explained above, modality preference can be characterized formally as a deviation from the decision threshold predicted by the optimal model. Figure\ \@ref(fig:bias) (top) shows both the decision threshold derived from the descriptive model (in black) and the decision threshold predicted by the optimal model (in red). The deviation from optimality is compared to two hypothetical cases of modality preference (dotted lines). We found that the descriptive and optimal decision thresholds were almost identical. Indeed, non-parametric resampling of the data showed no evidence of a deviation from the optimal prediction (Figure\ \@ref(fig:bias), bottom).

<!--the value of the slope derived from the descriptive model (i.e., $-\frac{\sigma^2_{Vb}}{\sigma^2_{Ab}}$) relative the slope of the optimal model (i.e., $-\frac{\sigma^2_V}{\sigma^2_A}$). The red dotted line represents the case where this proportion is optimal, meaning that both slopes are equal. The blue dotted lines represents the cases where the value of the slope from the descriptive model is double or half that of the optimal model, suggesting a preference for the auditory or the visual modality, respectively. Non-parametric resampling of the data showed no evidence of a deviation from the optimal prediction. -->

## Discussion
Overall, we found that the optimal model explained much of the variance in the mean judgments, and largely more than what can be explained with the auditory or the visual models alone. Moreover, the high value of the coefficient of determination in the optimal model ($r^2$=`r R2_optimal_1`) suggests that the population was near-optimal. However, we see in Figure\ \@ref(fig:bimodal) that the mean responses deviated systematically from the optimal prediction in that they were slightly pulled toward chance (i.e., the probability 0.5). This is due to the increase in the value of the variance associated with each modality. Note however that, despite this increase in randomness, our analysis of modality preference showed that the relative values of these variances were not different (Figure\ \@ref(fig:bias)), meaning that there was no evidence for a modality preference.<!--\footnote{The absence of modality preference does not mean that participants weight modalities equally in the bimodal condtion. Rather, it means that they weight modalities according to their relative relaibility, as measure in the unimodal conditions.}In the unimodal analysis, we saw that the precision of the auditory modality was slightly higher than that of the visual modality (i.e., $\frac{1}{\sigma^2_A} > \frac{1}{\sigma^2_V}$ ). If modalities were weighed according to their relative reliability (that is, optimally), we would naturally expect the auditory modality to have a higher weight in the bimodal presentation. But this is not what we mean by modality preference. Rather, the phrase refers to the case of potential sub-optimality where participants would rely on one modality beyond what is explained by the relative reliability of the optimal model. --> Thus, 1) There was a simultaneous increase in the values of the auditory and visual variances in the bimodal condition compared to the unimodal condition, meaning that the bimodal input lead to an increase in response randomness, and 2) this increased randomness did not affect the relative weighting of both modalities, i.e., the population was weighting modalities according to the relative reliability predicted by the optimal model. This situation corresponds to the first case of sub-optimally described in Figure\ \@ref(fig:subOptim). 


```{r echo=FALSE, warning = FALSE}

joint_bysub_1 <- joint_all_exp1 %>%
  group_by(ID) %>%
  do(fit_joint_1 = glm(answer ~ concept_dist+ sound_dist, data=., family = binomial()))

joint_bysub_2 <- joint_all_exp2 %>%
  group_by(ID) %>%
  do(fit_joint_2 = glm(answer ~ concept_dist+ sound_dist, data=., family = binomial()))

joint_bysub_3 <- joint_all_exp3 %>%
  group_by(ID) %>%
  do(fit_joint_3 = glm(answer ~ concept_dist+ sound_dist, data=., family = binomial()))


bimodal_bysub_1 = tidy(joint_bysub_1, fit_joint_1) %>%
  filter(term == 'concept_dist' | term == 'sound_dist') %>%
  mutate(j_var = 4/(estimate)) %>%
  select(ID, term, j_var) %>%
  spread(term, j_var) %>%
  rename(j_c_var = concept_dist, 
         j_s_var = sound_dist) %>%
  mutate(bimod = j_c_var/j_s_var)

bimodal_bysub_2 = tidy(joint_bysub_2, fit_joint_2) %>%
  filter(term == 'concept_dist' | term == 'sound_dist') %>%
  mutate(j_var = 4/(estimate)) %>%
  select(ID, term, j_var) %>%
  spread(term, j_var) %>%
  rename(j_c_var = concept_dist, 
         j_s_var = sound_dist) %>%
  mutate(bimod = j_c_var/j_s_var)

bimodal_bysub_3 = tidy(joint_bysub_3, fit_joint_3) %>%
  filter(term == 'concept_dist' | term == 'sound_dist') %>%
  mutate(j_var = 4/(estimate)) %>%
  select(ID, term, j_var) %>%
  spread(term, j_var) %>%
  rename(j_c_var = concept_dist, 
         j_s_var = sound_dist) %>%
  mutate(bimod = j_c_var/j_s_var)
  
```

```{r echo=FALSE}
#number of participants relying exlusiviely on one modality
#cut off is a factor of 10

bimodal_bysub_good_1 <- bimodal_bysub_1 %>%
  filter(bimod > 0.1) %>%
  filter(bimod < 10) 



#Visual modality 
only_visual_N_1 <- bimodal_bysub_1 %>%
  filter(abs(bimod) < 0.1) %>%
  nrow()

#Sound modality 
only_sound_N_1 <- bimodal_bysub_1 %>%
  filter(abs(bimod) > 10) %>%
  nrow()

#the rest
both_N_1 <- bimodal_bysub_1 %>%
  filter(abs(bimod) > 0.1) %>%
  filter(abs(bimod) < 10) %>%
  nrow()

neg_N_1 <- bimodal_bysub_1 %>%
  filter(bimod <  -0.1) %>%
  filter(bimod > -10) %>%
  nrow()

exlusive_prop_1 = only_visual_N_1/(only_sound_N_1+only_visual_N_1)

```



```{r echo=FALSE, warning = FALSE}
#What does the mean data say 
#generate two data point from a binomial with probabilty model_fit1()
#loop over visual and auditory distance/ or create  adataframe with 


simulations <- data.frame()
#x <- c("ID", "bimod")
#colnames(simlations) <- x

for (i in 1:50) {
Simul_exp1 <- joint_all_exp1 %>%
  select(ID, sound_dist, concept_dist) %>%
  mutate(prob = model_fit1(sound_dist, concept_dist)) %>%
  rowwise() %>%
  mutate(answer = rbinom(1, 1, prob=prob))

joint_bysub_sim_1 <- Simul_exp1 %>%
  group_by(ID) %>%
  do(fit_joint_sim_1 = glm(answer ~ concept_dist+ sound_dist, data=., family = binomial()))

bimodal_bysub_sim_1 = tidy(joint_bysub_sim_1, fit_joint_sim_1) %>%
  filter(term == 'concept_dist' | term == 'sound_dist') %>%
  mutate(j_var = 4/(estimate)) %>%
  select(ID, term, j_var) %>%
  spread(term, j_var) %>%
  rename(j_c_var = concept_dist, 
         j_s_var = sound_dist) %>%
  mutate(bimod = j_c_var/j_s_var) %>%
  select(ID, bimod) %>%
  filter(bimod > 0.1) %>%
  filter(bimod < 10) 


  simulations <- bind_rows(simulations, bimodal_bysub_sim_1)
}
  
  #I need to take the average of this over multiple runs 
  

  bimod_sim <- simulations  %>%
    mutate(data='simulation') 

  bimod_real <- bimodal_bysub_good_1  %>%
    select(ID, bimod) %>%
    mutate(data='real') 
  
  #n_remove <- nrow(bimod_sim)-nrow(bimod_real)
  
  #bimod_sim <- head(bimod_sim, -n_remove)
                    
  bimod_all <- bind_rows(bimod_real, bimod_sim) 
```

```{r echo=FALSE, individual, fig.cap = "Histograms of the values of the visual variance relative to the auditory variance in Experiment 1. Light color represents the values derived from each individual participant, and dark color represents simulated data sampled from the descriptive model.", fig.align = "center", out.width = "400px", fig.width=7, fig.height=8}

ggplot(data=bimod_all, aes(x=bimod, y=..ncount..)) +
  geom_histogram(data=subset(bimod_all, data=='real'), fill="red", alpha=0.2, binwidth = 0.2)+
  geom_histogram(data=subset(bimod_all, data=='simulation'), fill="blue", alpha=0.2, binwidth = 0.2)+
  #geom_histogram(binwidth = 0.2) +
  scale_x_log10() +
   xlab("Visual variance relative to sound variance") +ylab("Count")+
  theme(aspect.ratio = 0.7)

```

As we noted earlier, the model addresses the question of optimality at the population level. However, it is important to  know how individual responses are distributed. In fact, one could think of an extreme case where optimality at the population level would be misleading. Imagine, for instance, that in the bimodal condition half the participants relied exclusively on the visual modality, whereas the other half relied exclusively on the auditory modality. This case could still lead to an aggregate behaviour which appears optimal, but this optimality would be spurious.  

To examine this possibility, we consider the distribution of individual cross-modal weighting in the bimodal condition (i.e., $\frac{\sigma^2_{Vb}}{\sigma^2_{Ab}}$).  Using a factor of 10 as a cut-off, we found that `r only_visual_N_1` participants relied almost exclusively on the visual modality, and `r only_sound_N_1` relied almost exclusively on the auditory modality. The percentage of both cases was relatively small compared to the total number of participants (`r 100*(only_visual_N_1+only_sound_N_1)/(only_visual_N_1+only_sound_N_1+both_N_1)`%). An additional number of participants (N=`r neg_N_1`) relied on both modalities, but provided noisy responses which lead to negative variances (probably due to mistaking 'same' for 'different' or vice versa).  When these outliers  were removed, the distribution had a rather unimodal shape (Figure\ \@ref(fig:individual)). This finding indicates that the population's near optimality is not spurious, but based mostly on genuine cue combination at the individual level.

As a second analysis, we asked whether the observed variance in the individual distribution was due to mere sampling errors or whether it corresponded to a real between-subject variability. We simulated individual responses from the posterior distribution whose parameters were fit to the population as a whole (i.e., the descriptive posterior). The resulting distribution is shown in Figure\ \@ref(fig:individual). For ease of comparison, the simulated distribution was superimposed to the real distribution. We found that the real distribution was broader than the simulated distribution, indicating real between-subject variation beyond sampling errors. This means that the participants varied in terms of how they wheigthed modalities: Compared to the predictions of the global descriptive model, some participants relied more on the auditory modality, and some relied more on the visual modality.

<!--If, according to the extreme hypothetical case, the observed near-optimality is spurious, with participants relying exclusively on one or the other modality, we should observe a bimodal distribution (with modes centred around a very small and a very large number). If, instead, participants relied on both modalities, we should observe a unimodal distribution centered around a number whose order of magnitude is similar to the one observed with collapsed data. -->

In Experiment 1, we tested word recognition when there was multimodal uncertainty in terms of category membership only. In real life, however, tokens can undergo distortions due to noisy factors in the environment (e.g., car noise in the background, blurry vision in a foggy weather,..). In Experiment 2 and 3, we explore this additional level of uncertainty. 

# Experiment 2

In this Experiment, we explored the effect of added noise on performance. We tested a case where the background noise was added to the auditory modality. We were interested to know if participants would treat this new source of uncertainty as predicted by the optimal model, that is, according to the following weighting scheme  $$\beta_a \propto \frac{1}{\sigma^2_{A}+\sigma^2_{N_A}}$$ $$\beta_v \propto \frac{1}{\sigma^2_{V}}$$
The alternative hypothesis is that noise in one modality leads to a systematic preference for the non-noisy modality.  

## Methods

### Participants

A sample of `r N_all_2` participants was recruited online through Amazon Mechanical Turk. We used the same exclusion criteria as in Experiment 1.  `r N_noProb_2 - N_good_2` participants were excluded because they had less than 50\% accurate responses on the unambiguous training trials. The final sample consisted of (N = `r N_good_2`) participants.

### Stimuli and Procedure

We used the same visual stimuli as in Experiment 1. We also used the same auditory stimuli, but we convolved each item with Brown noise of amplitude 1 using the free sound editor Audacity (2.1.2). The average signal-to-noise ratio was - 4.4 dB. The procedure was exactly the same as in the previous experiment, except that the test stimuli (but not the target) were presented with the new noisy auditory stimuli.

## Results and analysis

```{r echo=FALSE}

#Bootstrap sample parameters

lmfit <- function(data, indices) {
  
  myd = data[indices, ]
  
  s_data <- myd %>%
    filter(condition == "sound")

  v_data <- myd %>%
    filter(condition == "concept") 

  j_data <- myd %>%
    filter(condition == "joint")
    
  s_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=s_data, start = list(e=0, vrA=2), nls.control(warnOnly = TRUE))

 c_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=v_data, start = list(e=0, vrV=2), nls.control(warnOnly = TRUE))

 j_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=j_data, start = list(e=0, vrA_j=2, vrV_j=2), nls.control(warnOnly = TRUE))

  
  s_va <- coef(s_nl)["vrA"]
  s_e <- coef(s_nl)["e"]
  
  v_va <- coef(c_nl)["vrV"]
  v_e <- coef(c_nl)["e"]
  
  decision_ideal=v_va/s_va

  j_va_s=coef(j_nl)["vrA_j"]
  j_va_v=coef(j_nl)["vrV_j"]
  j_e=coef(j_nl)["e"]
  
  decision_fit=j_va_v/j_va_s
  
  preference = decision_fit/decision_ideal
  
  MyBoot=c(s_va, s_e, v_va, v_e, j_va_s, j_va_v, j_e, decision_ideal, decision_fit, preference)
  
  
  return(MyBoot) 

  }

#results2 <- boot(data=exp2_good, statistic = lmfit, R = 10000)

#dataSave2 <- data.frame(matrix(ncol = 5, nrow = 0))
#data_names <- c("variable", "estimate", "lower","upper", "Experiment")
#colnames(dataSave2) <- data_names

#Auditory variance:
#audVar=boot.ci(results2, index = 1, type = c("bca"), conf = 0.95)

#v1 <- data.frame('audVar', as.numeric(audVar$t0), as.numeric(audVar$bca[4]), as.numeric(audVar$bca[5]), 'Exp2')
#  colnames(v1) <- data_names


#Auditory bias:
#audBias=boot.ci(results2, index = 2, type = c("bca"), conf = 0.95)

#v2 <- data.frame('audBias', as.numeric(audBias$t0), as.numeric(audBias$bca[4]), as.numeric(audBias$bca[5]), 'Exp2')
#  colnames(v2) <- data_names


#Visual variance:
#visVar=boot.ci(results2, index = 3, type = c("bca"), conf = 0.95)

#v3 <- data.frame('visVar', as.numeric(visVar$t0), as.numeric(visVar$bca[4]), as.numeric(visVar$bca[5]), 'Exp1')
#  colnames(v3) <- data_names
  

#Visual bias:
#visBias=boot.ci(results2, index = 4, type = c("bca"), conf = 0.95)

#v4 <- data.frame('visBias', as.numeric(visBias$t0), as.numeric(visBias$bca[4]), as.numeric(visBias$bca[5]), 'Exp2')
#colnames(v4) <- data_names


#Bimodal Auditory variance:
#audVarBi=boot.ci(results2, index = 5, type = c("bca"), conf = 0.95)

#v5 <- data.frame('audVarBi', as.numeric(audVarBi$t0), as.numeric(audVarBi$bca[4]), as.numeric(audVarBi$bca[5]), 'Exp2')
#  colnames(v5) <- data_names
#

#Bimodal Visual variance:
#visVarBi=boot.ci(results2, index = 6, type = c("bca"), conf = 0.95)

#v6 <- data.frame('visVarBi', as.numeric(visVarBi$t0), as.numeric(visVarBi$bca[4]), as.numeric(visVarBi$bca[5]), 'Exp2')
#  colnames(v6) <- data_names


#Bimodal bias:
#BiasBi=boot.ci(results2, index = 7, type = c("bca"), conf = 0.95)

#v7 <- data.frame('BiasBi', as.numeric(BiasBi$t0), as.numeric(BiasBi$bca[4]), as.numeric(BiasBi$bca[5]), 'Exp2')
#  colnames(v7) <- data_names
  

#Ideal modality wieghing
#prefIdeal=boot.ci(results2, index = 8, type = c("bca"), conf = 0.95)

#v8 <- data.frame('prefIdeal', as.numeric(prefIdeal$t0), as.numeric(prefIdeal$bca[4]), as.numeric(prefIdeal$bca[5]), 'Exp2')
#  colnames(v8) <- data_names


#fit modality wieghing
#prefFit=boot.ci(results2, index = 9, type = c("bca"), conf = 0.95)

#v9 <- data.frame('prefFit', as.numeric(prefFit$t0), as.numeric(prefFit$bca[4]), as.numeric(prefFit$bca[5]), 'Exp2')
#  colnames(v9) <- data_names


#Modality bias 
#bias=boot.ci(results2, index = 10, type = c("bca"), conf = 0.95)

#v10 <- data.frame('bias', as.numeric(bias$t0), as.numeric(bias$bca[4]), as.numeric(bias$bca[5]), 'Exp2')
#  colnames(v10) <- data_names
  

#dataSave2 <- bind_rows(dataSave2, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10)

#feather::write_feather(dataSave2, "dataExp2.feather")

dataSave2 <- feather::read_feather("dataExp2.feather")

#Extract values 
audVar_val=dataSave2$estimate[which(dataSave2$variable == "audVar")]
audVar_ci1=dataSave2$lower[which(dataSave2$variable == "audVar")]
audVar_ci2=dataSave2$upper[which(dataSave2$variable == "audVar")]

audBias_val=-dataSave2$estimate[which(dataSave2$variable == "audBias")]
audBias_ci1=-dataSave2$upper[which(dataSave2$variable == "audBias")]
audBias_ci2=-dataSave2$lower[which(dataSave2$variable == "audBias")]

visVar_val=dataSave2$estimate[which(dataSave2$variable == "visVar")]
visVar_ci1=dataSave2$lower[which(dataSave2$variable == "visVar")]
visVar_ci2=dataSave2$upper[which(dataSave2$variable == "visVar")]

visBias_val=-dataSave2$estimate[which(dataSave2$variable == "visBias")]
visBias_ci1=-dataSave2$upper[which(dataSave2$variable == "visBias")]
visBias_ci2=-dataSave2$lower[which(dataSave2$variable == "visBias")]

audVarBi_val=dataSave2$estimate[which(dataSave2$variable == "audVarBi")]
audVarBi_ci1=dataSave2$lower[which(dataSave2$variable == "audVarBi")]
audVarBi_ci2=dataSave2$upper[which(dataSave2$variable == "audVarBi")]

visVarBi_val=dataSave2$estimate[which(dataSave2$variable == "visVarBi")]
visVarBi_ci1=dataSave2$lower[which(dataSave2$variable == "visVarBi")]
visVarBi_ci2=dataSave2$upper[which(dataSave2$variable == "visVarBi")]

BiasBi_val=-dataSave2$estimate[which(dataSave2$variable == "BiasBi")]
BiasBi_ci1=-dataSave2$upper[which(dataSave2$variable == "BiasBi")]
BiasBi_ci2=-dataSave2$lower[which(dataSave2$variable == "BiasBi")]

prefIdeal_val_2=dataSave2$estimate[which(dataSave2$variable == "prefIdeal")]
prefIdeal_ci1_2=dataSave2$lower[which(dataSave2$variable == "prefIdeal")]
prefIdeal_ci2_2=dataSave2$upper[which(dataSave2$variable == "prefIdeal")]

bias_val_2=dataSave2$estimate[which(dataSave2$variable == "bias")]
bias_ci1_2=dataSave2$lower[which(dataSave2$variable == "bias")]
bias_ci2_2=dataSave2$upper[which(dataSave2$variable == "bias")]

```

### Unimodal conditions

We fit a model for each modality. For the auditory modality, our parameter estimates were $\epsilon_A=$ `r audBias_val` [`r audBias_ci2`, `r audBias_ci1`] and $\sigma^2_A+\sigma^2_N=$ `r audVar_val` [`r audVar_ci1`, `r audVar_ci2`]. For the visual modality, we found $\epsilon_V=$ `r visBias_val` [`r visBias_ci2`, `r visBias_ci1`] and $\sigma^2_V=$ `r visVar_val` [`r visVar_ci1`, `r visVar_ci2`].  Figure\ \@ref(fig:unimodal) shows responses in the unimodal conditions as well as the corresponding best fits. The visual data is a replication of the visual data in Experiment 1. As for the auditory data, in contrast to Experiment 1, responses were flatter, showing more uncertainty.

### Bimodal condition

#### Normative models
Figure\ \@ref(fig:bimodal) compares the predictions of the visual, auditory and optimal models to human responses. These normative models explained, respectively, `r 100*R2_visual_2`%, `r 100*R2_auditory_2`%, and `r 100*R2_optimal_2`% of total variance in mean judgements. Note that, in contrast to Experiment 1, the visual model explains more variance than the auditory model.

#### Descriptive model
We estimated $\epsilon=$ `r BiasBi_val` [`r BiasBi_ci2`, `r BiasBi_ci1`], $\sigma^2_{Ab}+\sigma^2_{Nb}=$ `r audVarBi_val` [`r audVarBi_ci1`, `r audVarBi_ci2`], and $\sigma^2_{Vb}=$ `r visVarBi_val` [`r visVarBi_ci1`, `r visVarBi_ci2`]. The fit explained `r R2_descriptive_2`% of total variance. Cross-validation using half the responses to predict the other half yielded $r^2 =$ `r CV_exp2_mean`.

#### Modality preferences
Figure\ \@ref(fig:bias) (top) shows that the participants' decision threshold deviated from optimality, and that this deviation was biased towards the visual modality (the non-noisy modality). Indeed non-parametric resampling of the data showed a decrease in the value of the slope in the descriptive model compared to the optimal model (Figure\ \@ref(fig:bias), bottom).

## Discussion
We found, similar to Experiment 1, that the population was generally near optimal ($r^2 =$ `r R2_optimal_2`), and that the optimal model explained more variance than the auditory or the visual models alone.  We also found a similar discrepancy from the optimal model as precision dropped for both the auditory and the visual modalities. As for the weighting scheme used by participants, contrary to Experiment 1 where modalities were weighted according to their relative reliability, we found in this experiment that the visual modality had a greater weight than what is expected from its relative reliability. This situation corresponds to the second case of sub-optimally described in Figure\ \@ref(fig:subOptim).

```{r echo=FALSE}
#number of participants relying exlusiviely on one modality
#cut off is a factor of 10

bimodal_bysub_good_2 <- bimodal_bysub_2 %>%
  filter(bimod > 0.1) %>%
  filter(bimod < 10) 

#Visual modality 
only_visual_N_2 <- bimodal_bysub_2 %>%
  filter(abs(bimod) < 0.1) %>%
  nrow()

#Sound modality 
only_sound_N_2 <- bimodal_bysub_2 %>%
  filter(abs(bimod) > 10) %>%
  nrow()

#the rest
both_N_2 <- bimodal_bysub_2 %>%
  filter(abs(bimod) > 0.1) %>%
  filter(abs(bimod) < 10) %>%
  nrow()

neg_N_2 <- bimodal_bysub_2 %>%
  filter(bimod <  -0.1) %>%
  filter(bimod > -10) %>%
  nrow()


exlusive_prop_2 = only_visual_N_2/(only_sound_N_2+only_visual_N_2)

```
We were also interested in whether noise in the auditory modality lead more participants to rely exclusively on the visual modality at the individual level. Using the same cut-off as in Experiment 1 (a factor of 10), the percentage of participants who relied exclusively on either modalities was `r 100*(only_visual_N_2+only_sound_N_2)/(only_visual_N_2+only_sound_N_2+both_N_2)`%, which is much higher than the percentage obtained in Experiment 1 (`r 100*(only_visual_N_1+only_sound_N_1)/(only_visual_N_1+only_sound_N_1+both_N_1)`%).  Moreover, the subset of participants relying exclusively on the visual modality (compared to those who relied exclusively on the auditory modality)  increased from `r 100*exlusive_prop_1`% in Experiment 1 to `r 100*exlusive_prop_2`% in Experiment 2, indicating that noise in the auditory modality prompted more participants to rely exclusively and disproportionately on the visual modality. 

In Experiment 2, we tested the case of added background noise to the auditory modality. In Experiment 3, we test the case of added noise to the visual modality.

# Experiment 3

In this Experiment, we tested the case where the background noise was added to the visual modality. Similar to Experiment 2, we were interested to know if participants would treat this new source of uncertainty as predicted by the optimal model, that is, according to the following weighting scheme  $$\beta_a \propto \frac{1}{\sigma^2_{A}}$$ $$\beta_v \propto \frac{1}{\sigma^2_{V}+\sigma^2_{N_V}}$$
The alternative hypothesis is that, just like noise in the auditory modality lead to a preference for the visual input in Experiment 2, noise in the visual modality would lead to a  preference for the auditory input.  

## Methods

### Participants

A planned sample of `r N_all_3` participants was recruited online through Amazon Mechanical Turk. We used the same exclusion criteria as in both previous experiments. `r N_all_3 - N_noProb_3` participants were excluded because they reported having a technical problem, and `r N_noProb_3 - N_good_3` participants were excluded because they had less than 50\% accurate responses on the unambiguous training trials. The final sample consisted of (N = `r N_good_3`) participants.

### Stimuli and Procedure
We used the same auditory stimuli as in Experiment 1. We also used the same visual stimuli, but we blurred the tokens using the free image editor GIMP (2.8.20). We used a Gaussian blur with a radius\footnote{A features of the filter that affects the intensity of the blur} of 10 pixels. The experimental procedure was exactly the same as in the previous Experiments.

## Results and analysis

```{r echo=FALSE}

#Bootstrap sample parameters

lmfit <- function(data, indices) {
  
  myd = data[indices, ]
  
  s_data <- myd %>%
    filter(condition == "sound")

  v_data <- myd %>%
    filter(condition == "concept") 

  j_data <- myd %>%
    filter(condition == "joint")
    
  s_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA)*sound_dist+(8/vrA))), data=s_data, start = list(e=0, vrA=2), nls.control(warnOnly = TRUE))

 c_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrV)*concept_dist+(8/vrV))), data=v_data, start = list(e=0, vrV=2), nls.control(warnOnly = TRUE))

 j_nl <- nls(answer ~ 1/(1+(1-e)*exp((-4/vrA_j)*sound_dist+(-4/vrV_j)*concept_dist+(8/vrA_j)+(8/vrV_j))), data=j_data, start = list(e=0, vrA_j=2, vrV_j=2), nls.control(warnOnly = TRUE))

  
  s_va <- coef(s_nl)["vrA"]
  s_e <- coef(s_nl)["e"]
  
  v_va <- coef(c_nl)["vrV"]
  v_e <- coef(c_nl)["e"]
  
  decision_ideal=v_va/s_va

  j_va_s=coef(j_nl)["vrA_j"]
  j_va_v=coef(j_nl)["vrV_j"]
  j_e=coef(j_nl)["e"]
  
  decision_fit=j_va_v/j_va_s
  
  preference = decision_fit/decision_ideal
  
  MyBoot=c(s_va, s_e, v_va, v_e, j_va_s, j_va_v, j_e, decision_ideal, decision_fit, preference)
  
  
  return(MyBoot) 

  }


#results3 <- boot(data=exp3_good, statistic = lmfit, R = 10000)

#dataSave3 <- data.frame(matrix(ncol = 5, nrow = 0))
#data_names <- c("variable", "estimate", "lower","upper", "Experiment")
#colnames(dataSave3) <- data_names

#Auditory variance:
#audVar=boot.ci(results3, index = 1, type = c("bca"), conf = 0.95)

#v1 <- data.frame('audVar', as.numeric(audVar$t0), as.numeric(audVar$bca[4]), as.numeric(audVar$bca[5]), 'Exp3')
#  colnames(v1) <- data_names


#Auditory bias:
#audBias=boot.ci(results3, index = 2, type = c("bca"), conf = 0.95)

#v2 <- data.frame('audBias', as.numeric(audBias$t0), as.numeric(audBias$bca[4]), as.numeric(audBias$bca[5]), 'Exp3')
#  colnames(v2) <- data_names

#Visual variance:
#visVar=boot.ci(results3, index = 3, type = c("bca"), conf = 0.95)

#v3 <- data.frame('visVar', as.numeric(visVar$t0), as.numeric(visVar$bca[4]), as.numeric(visVar$bca[5]), 'Exp3')
#  colnames(v3) <- data_names

#Visual bias:
#visBias=boot.ci(results3, index = 4, type = c("bca"), conf = 0.95)

#v4 <- data.frame('visBias', as.numeric(visBias$t0), as.numeric(visBias$bca[4]), as.numeric(visBias$bca[5]), 'Exp3')
#  colnames(v4) <- data_names


#Bimodal Auditory variance:
#audVarBi=boot.ci(results3, index = 5, type = c("bca"), conf = 0.95)

#v5 <- data.frame('audVarBi', as.numeric(audVarBi$t0), as.numeric(audVarBi$bca[4]), as.numeric(audVarBi$bca[5]), 'Exp3')
#  colnames(v5) <- data_names


#Bimodal Visual variance:
#visVarBi=boot.ci(results3, index = 6, type = c("bca"), conf = 0.95)

#v6 <- data.frame('visVarBi', as.numeric(visVarBi$t0), as.numeric(visVarBi$bca[4]), as.numeric(visVarBi$bca[5]), 'Exp3')
#  colnames(v6) <- data_names


#Bimodal bias:
#BiasBi=boot.ci(results3, index = 7, type = c("bca"), conf = 0.95)

#v7 <- data.frame('BiasBi', as.numeric(BiasBi$t0), as.numeric(BiasBi$bca[4]), as.numeric(BiasBi$bca[5]), 'Exp3')
#  colnames(v7) <- data_names
  

#Ideal modality wieghing
#prefIdeal=boot.ci(results3, index = 8, type = c("bca"), conf = 0.95)

#v8 <- data.frame('prefIdeal', as.numeric(prefIdeal$t0), as.numeric(prefIdeal$bca[4]), as.numeric(prefIdeal$bca[5]), 'Exp3')
#  colnames(v8) <- data_names


#fit modality wieghing
#prefFit=boot.ci(results3, index = 9, type = c("bca"), conf = 0.95)

#v9 <- data.frame('prefFit', as.numeric(prefFit$t0), as.numeric(prefFit$bca[4]), as.numeric(prefFit$bca[5]), 'Exp3')
#  colnames(v9) <- data_names


#Modality bias 
#bias=boot.ci(results3, index = 10, type = c("bca"), conf = 0.95)

#v10 <- data.frame('bias', as.numeric(bias$t0), as.numeric(bias$bca[4]), as.numeric(bias$bca[5]), 'Exp3')
#  colnames(v10) <- data_names
  

#dataSave3 <- bind_rows(dataSave3, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10)
#feather::write_feather(dataSave3, "dataExp3.feather")


dataSave3 <- feather::read_feather("dataExp3.feather")

#Extract values 
audVar_val=dataSave3$estimate[which(dataSave3$variable == "audVar")]
audVar_ci1=dataSave3$lower[which(dataSave3$variable == "audVar")]
audVar_ci2=dataSave3$upper[which(dataSave3$variable == "audVar")]

audBias_val=-dataSave3$estimate[which(dataSave3$variable == "audBias")]
audBias_ci1=-dataSave3$upper[which(dataSave3$variable == "audBias")]
audBias_ci2=-dataSave3$lower[which(dataSave3$variable == "audBias")]

visVar_val=dataSave3$estimate[which(dataSave3$variable == "visVar")]
visVar_ci1=dataSave3$lower[which(dataSave3$variable == "visVar")]
visVar_ci2=dataSave3$upper[which(dataSave3$variable == "visVar")]

visBias_val=-dataSave3$estimate[which(dataSave3$variable == "visBias")]
visBias_ci1=-dataSave3$upper[which(dataSave3$variable == "visBias")]
visBias_ci2=-dataSave3$lower[which(dataSave3$variable == "visBias")]

audVarBi_val=dataSave3$estimate[which(dataSave3$variable == "audVarBi")]
audVarBi_ci1=dataSave3$lower[which(dataSave3$variable == "audVarBi")]
audVarBi_ci2=dataSave3$upper[which(dataSave3$variable == "audVarBi")]

visVarBi_val=dataSave3$estimate[which(dataSave3$variable == "visVarBi")]
visVarBi_ci1=dataSave3$lower[which(dataSave3$variable == "visVarBi")]
visVarBi_ci2=dataSave3$upper[which(dataSave3$variable == "visVarBi")]

BiasBi_val=-dataSave3$estimate[which(dataSave3$variable == "BiasBi")]
BiasBi_ci1=-dataSave3$upper[which(dataSave3$variable == "BiasBi")]
BiasBi_ci2=-dataSave3$lower[which(dataSave3$variable == "BiasBi")]

prefIdeal_val_3=dataSave3$estimate[which(dataSave3$variable == "prefIdeal")]
prefIdeal_ci1_3=dataSave3$lower[which(dataSave3$variable == "prefIdeal")]
prefIdeal_ci2_3=dataSave3$upper[which(dataSave3$variable == "prefIdeal")]

bias_val_3=dataSave3$estimate[which(dataSave3$variable == "bias")]
bias_ci1_3=dataSave3$lower[which(dataSave3$variable == "bias")]
bias_ci2_3=dataSave3$upper[which(dataSave3$variable == "bias")]


```

### Unimodal conditions
For the auditory modality, our parameter estimates were $\epsilon_A=$ `r audBias_val` [`r audBias_ci2`, `r audBias_ci1`] and $\sigma^2_A=$ `r audVar_val` [`r audVar_ci1`, `r audVar_ci2`]. For the visual modality, we found $\epsilon_V=$ `r visBias_val` [`r visBias_ci2`, `r visBias_ci1`] and $\sigma^2_V+\sigma^2_N=$ `r visVar_val` [`r visVar_ci1`, `r visVar_ci2`].  Figure\ \@ref(fig:unimodal) shows responses in the unimodal conditions as well as the corresponding fits.
The auditory data is a replication of the auditory data in Experiment 1. As for the visual data, we see that, in contrast to Experiment 1 and 2, responses were flatter, showing much more uncertainty.

### Bimodal condition

#### Normative models
Figure\ \@ref(fig:bimodal) compares the predictions of the visual, auditory and optimal models to human responses. These normative models explained, respectively, `r 100*R2_visual_3`%, `r 100*R2_auditory_3`%, and `r 100*R2_optimal_3`% of total variance in mean judgement. 

#### Descriptive model
We estimated $\epsilon=$ `r BiasBi_val` [`r BiasBi_ci2`, `r BiasBi_ci1`], $\sigma^2_{Ab}=$ `r audVarBi_val` [`r audVarBi_ci1`, `r audVarBi_ci2`], and $\sigma^2_{Vb}+\sigma^2_{Nb}=$ `r visVarBi_val` [`r visVarBi_ci1`, `r visVarBi_ci2`]. The fit explained `r 100*R2_descriptive_3`% of total variance. Cross-validation using half the responses to predict the other half yielded $r^2= $ `r CV_exp3_mean`.

#### Modality preferences
Participants' decision threshold suggested a preference for the auditory modality (the non-noisy modality). Indeed non-parametric resampling of the data showed an increase in the value of the slope in the descriptive model compared to the optimal model (Figure\ \@ref(fig:bias)).

## Discussion
We found that the optimal model accounted for almost all the variance ($r^2 =$ `r R2_optimal_3`). However, whereas in previous experiments, the optimal model explained more variance than the auditory or the visual models, here the auditory model explained at least as much variance ($r^2 =$ `r R2_auditory_3`). Thus, though participants were still sensitive to variation in the noisy visual data in the unimodal condition, they tended to ignore this information in the bimodal condition, and relied almost exclusively on the non-noisy auditory modality. The reason why we saw this (floor) effect when we added noise to the visual modality (Experiment 3), and not when we added noise to the auditory modality (Experiment 2), is the fact that our visual stimuli were originally perceived less categorically and with less certainty than the auditory stimuli. This made it more likely for the visual categorization function to become flat and uninformative after a few drops in precision due to noise on the one had, and to the additional randomness induced by the bimodal presentation on the other hand. 

<!--This behaviour is not so much a qualitative difference from Experiment 2 (where participants relied on both the noisy and the non-noisy modality in the bimodal condition), as much as it is the result of a floor effect. Indeed, noise made the visual modality way less steep and barely perceptible in the unimodal condition Figure\ \@ref(fig:unimodal). So when we had an additional general drop in precision in the bimodal condition (similar to what we found in Experiment 1 and 2),  participants were no longer able to extract any useful information. -->
The general finding corresponds to the third case of sub-optimality described in Figure\ \@ref(fig:subOptim). Indeed, precision dropped for both modalities in the bimodal condition compared to the unimodal condition. But the drop was much greater for the visual modality, resulting in a much lower weight assigned to it than what is expected from its optimal relative reliability. Therefore, just like participants over-relied on the visual modality when the auditory modality was noisy (Experiment 2), they also over-relied on the auditory modality when the visual modality was noisy (Experiment 3).

```{r echo=FALSE}
#number of participants relying exlusiviely on one modality
#cut off is a factor of 10

bimodal_bysub_good_3 <- bimodal_bysub_3 %>%
  filter(bimod > 0.1) %>%
  filter(bimod < 10) 

#Visual modality 
only_visual_N_3 <- bimodal_bysub_3 %>%
  filter(abs(bimod) < 0.1) %>%
  nrow()

#Sound modality 
only_sound_N_3 <- bimodal_bysub_3 %>%
  filter(abs(bimod) > 10) %>%
  nrow()

#the rest
both_N_3 <- bimodal_bysub_3 %>%
  filter(abs(bimod) > 0.1) %>%
  filter(abs(bimod) < 10) %>%
  nrow()

neg_N_3 <- bimodal_bysub_3 %>%
  filter(bimod <  -0.1) %>%
  filter(bimod > -10) %>%
  nrow()


exlusive_prop_3 = only_visual_N_3/(only_sound_N_3+only_visual_N_3)

```
The percentage of participants who relied exclusively on either the visual modality or the auditory modality was `r 100*(only_visual_N_3+only_sound_N_3)/(only_visual_N_3+only_sound_N_3+both_N_3)`%, which is closer to the percentage of Experiment 2, except that now almost all of them relied on the auditory modality (`r 100*(1-exlusive_prop_3)`%).


```{r echo=FALSE, warning = FALSE}

#Modality preference in an audio-vidual space

prefIdeal_val_1=1.637216
prefFit_val_1=1.454367

prefIdeal_val_2=0.8367816
prefFit_val_2=0.529517

prefIdeal_val_3=6.68829 
prefFit_val_3=13.15958 

# Decision threshold
classif = function (x, A) {
    2*A+2-A*x
}

pref_data <- data.frame(matrix(ncol = 7, nrow = 0))
names <- c("Auditory", "Visual", "Optimal", "Auditory bias", "Visual bias", "Fit", "Experiment")
colnames(pref_data) <- names

x=seq(-1,5,0.02)

pref_exp1 <- data.frame(x, x, 
                        classif(x, prefIdeal_val_1),
                        classif(x, 2*prefIdeal_val_1),
                        classif(x, 0.5*prefIdeal_val_1),
                        classif(x, prefFit_val_1),
                        'Exp1')
colnames(pref_exp1) <- names

pref_exp2 <- data.frame(x, x, 
                        classif(x, prefIdeal_val_2),
                        classif(x, 2*prefIdeal_val_2),
                        classif(x, 0.5*prefIdeal_val_2),
                        classif(x, prefFit_val_2),
                        'Exp2')
colnames(pref_exp2) <- names

pref_exp3 <- data.frame(x, x, 
                        classif(x, prefIdeal_val_3),
                        classif(x, 2*prefIdeal_val_3),
                        classif(x, 0.5*prefIdeal_val_3),
                        classif(x, prefFit_val_3),
                        'Exp3')
colnames(pref_exp3) <- names

pref_data <- bind_rows(pref_data, pref_exp1, pref_exp2, pref_exp3) %>%
  gather(model, value, Optimal:Fit)

pref_data$model <- factor(pref_data$model, levels = c("Optimal", "Fit","Auditory bias", "Visual bias"))
  
pref_thres <- ggplot(pref_data, aes(x=Auditory, y=value, col = factor(model))) +
  geom_line(aes(linetype = factor(model) )) +
  facet_grid(.~Experiment)+
  scale_colour_manual(values = c("Optimal" = "red", "Auditory bias" = "blue", "Visual bias" = "green", "Fit" = "black"))+
  scale_linetype_manual(values = c("Optimal" = "solid", "Auditory bias" = "dashed", "Visual bias" = "dashed", "Fit" = "solid"))+
  xlab("Auditory") +ylab("Visual") +
  scale_x_continuous(limits = c(1.7, 2.3))+
  scale_y_continuous(limits = c(1.7, 2.3))+theme(aspect.ratio = 1)+
  theme(legend.title = element_blank())

```



```{r echo=FALSE, warning = FALSE}
#Modality bias

preference = c(bias_val_1, bias_val_2, bias_val_3 )
experiment =c("Exp 1 \n", "Exp 2", "Exp 3")
ci_low=c(bias_ci1_1, bias_ci1_2, bias_ci1_3)
ci_up=c(bias_ci2_1, bias_ci2_2, bias_ci2_3)

pref = data.frame(preference, ci_low, ci_up)

bias <- ggplot(pref, 
       aes(x = experiment, y=preference)) +
geom_point(size=3)+
  geom_errorbar(aes(ymin = ci_low, ymax = ci_up), 
                  width = 0.1,
                  position = position_dodge(width = 0.1))+
  geom_hline(yintercept = 1, linetype='solid', color="red", size=1)+
  
  geom_hline(yintercept = 0.5, linetype=2, color="green")+
  
  geom_hline(yintercept = 2, linetype=2, color="blue")+
  
  theme(aspect.ratio = 1, axis.text=element_text(size=10))+
  xlab("") +ylab("Relative weighing")+
  scale_y_log10(breaks=c(0.5,1,2),labels=c("Visual bias","Optimal","Auditory bias"))
  #coord_cartesian(ylim=c(0, 2.5))

```

```{r bias, echo=FALSE, warning = FALSE, fig.cap = "Modality preference is characterized as a deviation from the optimal decision threshold. A) The decision thresholds of both the optimal and the descriptive models (solid red and black lines respectively). Deviation from optimality is compared to two hypothetical cases of modality preference. In these cases, deviation from  optimality is due to over-lying on the visual or the auditory input (green and blue dotted lines, respectively) by a factor of 2. B) The value of the decision threshold's slope derived from the descriptive model relative to that of the optimal model. Error bars represent 95\\% confidence intervals over the distribution obtained through non-parametric resampling.", fig.align = "center", out.width = "400px"}

legend <- get_legend(pref_thres)
plot_noLegend <- plot_grid(pref_thres + theme(legend.position="none"), NULL, bias, labels = c("A", "", "B"), ncol = 1, align = "v", rel_heights = c(1.1, 0.1, 1.3))
plot_grid(plot_noLegend, legend, rel_widths = c(2, .5))

```

# General Discussion

When identifying a spoken word under uncertainty, one often needs to combine and make the most of the available multimodal cues. While most previous studies focused on the case of audio-visual speech where people combine cues from speech and facial features, this work explored the case of identifying an ambiguous word when the available multimodal cues are speech and the visual referent. More specifically, we conducted an ideal observer analysis of the task whereby a model provided predictions about how information from each modality should be combined in an optimal fashion. The predictions of the model were tested in a series of three experiments where instances of both the form and the referent were ambiguous with respect to their category membership only (Experiment 1), when instances of the form were perturbed with additional background noise (Experiment 2), and when instances of the referent were perturbed with additional visual noise (Experiment 3).

In all Experiments, we found many patterns of optimal behaviour. Quantitatively speaking, the optimal model accounted, respectively, for `r 100*R2_optimal_1`%, `r 100*R2_optimal_2`%, and `r 100*R2_optimal_3`% of the variance in mean responses.  When compared to the predictions of the visual or the auditory models, participants generally relied on both modalities to make their decisions in the bimodal condition. Indeed, in Experiment 1 and 2, the optimal model accounted for more variance in mean responses than the auditory or the visual models did. In Experiment 3, participants appeared to rely on one modality, but this was likely a floor effect, due to the fact that noise made the visual input barely perceivable. In Experiment 1, which did not involve background noise, participants not only relied on both modalities, but generally weighted these modalities according to the prediction of the optimal model, that is, according to their relative reliability.  At the individual level, however, we found evidence of a between-subject variation: Some participants relied slighly more on the visual modality, whereas others relied slighly more on the auditory modality.

We documented two major cases of sub-optimality. First, in all Experiments, the variance associated with each modality increased in the bimodal condition compared to the unimodal conditions. This means that participants responded slightly more randomly in the bimodal condition than they did in the unimodal conditions. This finding contrasts with research on  multisensory integration where associations tend to lead to a higher precision [e.g., @ernst02]. Nevertheless, there is a crucial difference between these two situations. Research on  multisensory integration (of which audio-visual speech is arguably an instance) deals with redundant multimodal cues, and these cues are integrated into a unified percept. In contrast, the word-referent association is usually arbitrary and, in particular, the cues are not expected to be correlated perceptually. Therefore the mind cannot form a unified percept, rather, it must encode information separately from both modalities and retain this encoding through the decision making process. Retaining two separate cues at the same time instead of forming one unified percept (as in multisensory integration of redundant cues), or instead of retaining only one cue (as in the unimodal case), is likely to place extra-demand on cognitive resources, which, in turn, can cause general performance to drop. Indeed, there is evidence that cognitive load has a detrimental effect on word recognition, which can be due to a reduction in perceptual acuity [@mattys11].  Similar to our finding, previous experimental studies reported that in both children and adults, identifying new ambiguous form-referent mappings is usually associated with what appears to be a decrease in speech perception acuity [@stager1997; @pajak2016]. @hofer2017 provided a probabilistic model of this phenomenon, which they similarly characterized as an increase in the noise variance of the auditory modality. Our finding replicates this previously documented fact, and additionally suggests that the reduction in perceptual acuity occurs simultaneously in both the auditory and the visual modalities.

The second case of sub-optimality is related to how participants weighed the cues from the visual and the auditory modalities in a noisy context. In contrast to Experiment 1 where the combination was indistinguishable from the optimal prediction, results of Experiment 2 and 3 which both involved background noise in one modality, showed that participants had a systematic preference for the other (non-noisy) modality. From previous empirical studies, we know that when the speech signal is degraded, people tend to compensate by relying more on other sources of information such as the accompanying visual cues (i.e., lip movements) or the semantic/syntactic context [see @mattys12 for a review]. However, and generally speaking, these studies do not differentiate between an optimal compensatory strategy (i.e., relying more on the alternative source while using all information still available in the distorted signal), and a sub-optimal strategy (i.e., relying more on the alternative source while ignoring at least some of the information still available in the distorted signal). The formal approach followed in this paper allowed us to tease apart these two possibilities, and the analysis supports the sub-optimal compensatory strategy: The preference for the non-noisy modality is above and beyond what can be explained by the relative reliability alone, meaning that the participant tend to ignore at least part of the information still available in the noisy modality.

This second case of sub-optimal behavior is possibly related to the fact that language understanding under degraded listening conditions is cognitively more taxing than language understanding under normal conditions [@Ronnberg10]. This fact can lead to a preference/bias towards alternative sources of information when these sources are available. One could explain this phenomenon in terms of the metacognitive experience about the fluency with which information is processed. The perceived perceptual fluency (e.g., the ease with which a stimulus’ physical identity can be identified) can affect a wide variety of human judgements [see @schwarz2004 for a review]. In particular, variables that improve fluency tends to increase liking/preference [@reber98]. In our case, the subjective experience of lower fluency in the noisy modality might cause people to underestimate information that can be extracted from this modality, especially when presented simultaneously with a higher fluency alternative.  

In this study, we tested the extent to which people would be optimal in combining uncertain auditory and visual cues to recognize words. This required that we use a case of double ambiguity, that is, a case where both the word forms ("ada"-"aba") and the referents (cat-dog) were similar and, thus, confusable. One could wonder about the extent to which such case occurs in real languages?  Cross-linguistic corpus analyses suggest that the lexical encoding tends towards double ambiguity in many languages [@dautriche17; @Monaghan2014; @Tamariz2008]. For instance, @dautriche17 analysed 100 languages and found that word that are similar phonologically  tend to be similar semantically as well, beyond what could be explained by chance. Crucially, they found this trend in a set of monomorphemic lemmas in four languages, suggesting that the effect is not driven by morphological regularities.  These studies suggest that the case of double uncertainty, though perhaps not pervasive, could be a real issue in language as it increase the probability of confusability.  Moreover, it is important to keep in mind that ambiguity in both the form and the referent can be induced by an external noisy context even when these forms and referents are not confusable in normal situations.

Though we only tested adults in this paper, the problem of word recognition under uncertainty, as well as the need to make the most of ambiguous cues, is a particularly pressing issue for children. In fact, whereas adults are mostly faced with uncertainty in the *input*, children have to deal with the additional uncertainty that results from their early unrefined *representations* of both phonological and semantic categories. For example, upon hearing a noisy instance of "bee", adults may have to decide whether the speaker intended to say "pea" or "bee", but children can additionally be uncertain whether "bee" is a different word from "pee" (as opposed to, say, a valid within category variation), especially if these similar sounding words are newly learned [@stager1997; @Merriman91; @Creel2012; @Swingley2016; @white2008b]. Though new word form representation can be shown to be differentiated under some circumstances [e.g., @yoshida2009], this differentiation is still not mature enough and is probably noisier than the adult-like representation and/or encoded with lower confidence [see @Swingley2007].  

At the semantic level, early representations have, similarly, an intrinsically fragile and uncertain status. For example, upon seeing a bee in a foggy weather, adults may be uncertain if they saw a bee or a fly. But on top of this perceptual uncertainty, children may not be certain if the semantic category being named is that of bees and only bees, or if it includes other small flying insects like flies and beetles. In fact,  though children can be fast at learning a first approximation of a given word's referent [@carey1978b], the refinement of this early approximation into a mature semantic category is a slow and gradual process [see also @bion2013;; @carey2010; @fernald2006; @mcmurray2012]. Among other things, children have to enrich this early representation with new features, and revise its extension in the light of new referential exposures. 

Thus, uncertainty in the representation associated with one modality (e.g., a bee and a fly) can be mitigated through the possibly more differentiated representations associated with the other modality (e.g., the sound "bee" is acoustically different from the sound "fly"). That being said, a multi-modal cue combination strategy might help children not only recognize an individual word instance, but also refine the underlying phonological and semantic representations in the process. Previous research in early word learning has---whether implicitly or explicitly---largely treated the process of learning form and of learning meaning as independent. <!--Babies are understood to first master the form, learning to ignore phonologically irrelevant variation (such as differences in talker, speech rate, emotion, and linguistic context) (see Kuhl, 2004 for a review). Only when the form representation is refined and ready, can they start mapping the form to meaning (Bloom, 2000)-->However, the developmental data reviewed above shows that children do not wait to have completed the acquisition of form to start learning meanings, and that both form and meaning representations develop, rather, in a parallel fashion. A few studies pointed to the possibility of an interaction between sound and meaning in early acquisition. For instance, @waxman1995 showed that labeling various objects with the same name helps infants form the broad semantic category [but see @sloutsky2003]. Vice versa, @yeung09 showed that pairing similar sounds with different objects help infants pay attention to subtle phonological contrasts. The present study proposes a first step towards a formal framework where accounts of sound-meaning synergy in development can be unified and further explored. For example, one could imagine that, initially, visual and auditory categories have relatively large noise variances, and that development consists in reducing the values of the variances through a mutually constraining process as further multimodal data accumulates. 

In conclusion, this work studied the mechanism of word identification under uncertainty in both the word form and the word meaning. To our knowledge, this is the first study that performs an ideal observer analysis of this task. We found people to be near optimal in their cue combination: They weighted each modality according to its relative reliability. However, they also showed patterns of sub-optimality especially when the stimuli were perturbed with additional background noise. Future work will extend the present formal framework in order to account for how children use multimodal cues to refine their early phonological and semantic representations.


# Appendix: derivation of the posterior (equation 1)

For an ideal observer, the probability of choosing category 2 when presented with an audio-visual instance $w = (a, v)$ is the posterior probability of this category:

$$p(W_2 | w)=\frac{p(w|W_2)p(W_2)}{p(w|W_2)p(W_2)+p(w|W_1)p(W_1)}$$

Which reduces to:

$$p(W_2 | w)=\frac{1}{1+\frac{p(w|W_1)}{p(w|W_2)} \frac{p(W_1)}{p(W_2)}}$$
In order to further simplify the quantity $\frac{p(w|W_1)}{p(w|W_2)}$, we use our assumption that the cues are uncorrelated, 
$$p(w | W) = p(a,v| W) = p(a| A)p(v| V)$$
Using the $\log$ transformation, we get:

$$ \ln(\frac{p(w |W_1)}{p(w|W_2)})=\ln(\frac{p(a|W_1)}{p(a|W_2)})+\ln(\frac{p(v|W_1)}{p(v|W_2)}) $$ 
Under the assumption that the categories are normally distributed and that, within each modality, the categories have equal variances, we get (after simplification):

$$\ln(\frac{p(a|W_1)}{p(a|W_2)})=\frac{\mu_{A1}-\mu_{A2}}{\sigma^2_{A}}\times a+ \frac{\mu^2_{A2}-\mu^2_{A1}}{2\sigma^2_{A}}$$

and similarly:

$$\ln(\frac{p(v|W_1)}{p(v|W_2)})=\frac{\mu_{V1}-\mu_{V2}}{\sigma^2_{V}}\times v+ \frac{\mu^2_{V2}-\mu^2_{V1}}{2\sigma^2_{V}}$$

When putting all these terms together, we the get this final expression for the posterior:
$$p(W_2 | w)=\frac{1}{1+(1+\epsilon)\exp(\beta_0+\beta_aa+\beta_vv)}$$

where 

$$1+\epsilon=\frac{p(W_1)}{p(W_2)}$$
$$\beta_0=\frac{\mu^2_{A2}-\mu^2_{A1}}{2\sigma^2_{A}}+\frac{\mu^2_{V2}-\mu^2_{V1}}{2\sigma^2_{V}}$$

$$\beta_a=\frac{\mu_{A1}-\mu_{A2}}{\sigma^2_{A}}$$
$$\beta_v=\frac{\mu_{V1}-\mu_{V2}}{\sigma^2_{V}}$$ 
<!--
as further data about sound-meanining associations accumulate, making the categories more refined and precise. 


Moreover, modeling studies the possibility of an interaction or synergy between early represention interaction between sound and menaning 

A few studies explicitely pointed to the possibility of an interaction between sound and meaning in early acqusition. For instance, Waxman and Markow (1995) showed that a 

 

It follows that Cue combination is not only necessary to recognize a word (as with ault), but also to lrefine the underlying representations, leading to a leaning synergy (Fourtass and Dupoux, 2014). Waxman and Yeung, but they assume an ambiguous state. Here we suggest that even if non of the representation 




If we take the perspective of infromation theory, communication is basically a process of information transfer in a noisy channel (Shannon, 1948), and information encoding should be such that the listener is able to recover what is being said. Therefore, the confusability of distinct words should be minimized. In particular, similar meanings should be encoded with phonologically distinct words. As it turns out, cross-lingusitc corpus analyses reveal an opposite trend, suggesting that the constraints of communication might not be the only force shaping the struture of the lexicon (Dautriche et al. 2016; Monaghan et al., 2014; Shillcock et al., 2001; Tamariz, 2008). For instace, Dautriche et al. 2015 analysed 100 languages and found that word that are similar phonologically  tend to be more similar semantically as well, above what could be explained by chance. Crucially, they found this trend in a set of monomorphemic lemmas in four languags, suggesting that the effect is not driven by morphological regularities.  These studies suggest that the case of souble uncertainty, though perhabps not pervasive, could be a real issue beyond our controlled experiment, and that an optimal cue combination strategy might be crucial for word recognition in real life situations.

Here talk about development



They found that minimal pairs are more likely to be found among semantically related words than semantically different words.  

Crucially, 


From an information theory perspective, communication can be modeled as the transfer of information over a noisy channel (Shannon, 1948). Successful commu- nication requires listeners to be able to recover what is being said; therefore, the likeli- hood that two distinct words would be confusable should be minimized according to this constraint. There is much evidence that perceptual distinctiveness plays an important role in shaping the phonology of languages

However, in order for the optimal combination strategy to be relevant to our understading of word recognion, we need   play any role beyond a mere controlled experiment, cross-lingsuitic corpus analyses indicate that this is not an infrequent phenomeon in real languages. For example, 

analyzed the relationship between semantic relatedness and the likelihood of finding a minimal pair and found that minimal pairs are more likely among semantically similar words than semantically distant words.

The arbitraniess of the sign in natural language means that the word form is not necessarilly related to the features of the semantic catgory, and a corrola   

in rather extreme situation  tested a rather extreme case of uncertainty where both 


To explore how the mind However, insights in this work can shed light on   but in a simple case when the underlying visual and auditory categories were familiar. Indeed, on the visual side, cat and dog tokens belong obviously to existing visual categories. On the auditory side, though the words ("ada"/"aba") are novel, the phonemic categories that differentiate the words are familiar (i.e., /d/ and /b/). In the context of early word learning, however, children are often learning both the categories and their mapping at the same time. In fact, children start learning visual and sound categories at an early age [e.g., @quinn93;@werker1984], and there is evidence that word-referent-like interaction operates at the same time, even when the categories are not yet fully mastered [e.g., @waxman1995; @yeung09]. In fact, @fourtassi2014a proposed that this interaction can create a synergy (or a bootstrapping effect) whereby progress made on one level (e.g., the meaning categories) can benefit the other level (the sound categories).

In what follows we explain how an account that combines both category learning and optimal multimodal integration can proceed, and how such account can provide us with a useful developmental framework. In the light of this framework, we speculate about possible new interpretation of some puzzling results in early word learning [@stager1997 and seq]. For starters, remember that one assumption of the model is that visual and auditory categories can be characterized with a variance that determines their precision/reliability. At the behavioral level, the variance is related to the shape of the categorization function: a small variance corresponds to a rather steep categorization function, meaning that one can tell with high certainty whether or not a token belongs to the category at hand. Vice versa, a large value of the variance corresponds to a rather flat categorization function, which means that for most tokens, one cannot tell with high certainty whether or not these tokens belong to the category at hand.  

How can this formal characterization fit in a developmental scenario? One could imagine that, initially, visual and auditory categories have relatively large variances, and that development consists in reducing the values of the variances as evidence accumulates, making the categories more refined and precise. A scenario similar to this one has been previously suggested to explain the development of early sound categories. For example, @yoshida2009 suggested that the early categories may be noisier or that they may be encoded with relatively lower confidence. For the semantic categories, we can find similar (although perhaps not identical) ideas. For instance, Clark (1973) suggested that the initial meaning categories tend to be larger than adult categories, since fewer features are used to pick out referents (e.g., dog for all four-legged animal, ball for all round objects).

The combination of this developmental scenario in terms of category variance with the optimal integration account introduced in this paper provides possible new insights into the mechanism of word learning. Let's elucidate this idea through an example in the developmental literature. @stager1997 showed that 14 m.o (as opposed to 17 m.o) have difficulties mapping two minimally different sounds (e.g., "bih"/"dih") to two novel objects, despite the fact that they can perceptually differentiate these sounds. Our proposal explains these patterns of failure and success in terms of developmental changes in the variances associated with the visual and phonological categories, combined with the ability to integrate sound and visual cues according to their reliability. Suppose that both members of a given sound contrast ("bih"/"dih") initially fall within the uncertainty range of a single sound category. If this uncertainty diminishes with age (as the category gets more refined), then what used to be likely at a certain point in development (e.g., 14 m.o), may later become unlikely. In particular, at 17, m.o, it may become necessary to attribute the sound contrast to two neighboring categories instead of a single one. But how can 14 m.o possibly consider one broad category for both sounds if these sounds are paired with different objects? This is where the optimal combination account may come into play. Indeed, even if the objects are perceptually different, they may still be similar in the 14 m.o's early semantic space. If babies make categorical decision based on both auditory and visual cues, then having evidence that the auditory tokens belong to the same category can prompt them to consider the possibility that even the objects are variations within the same (broad) semantic category. Thus, 14 m.o may be learning one broad word-referent association instead of two specific associations. According to the present account, what was considered a "failure" in learning might, in fact, be the optimal decision within the 14 m.o's own developmental context!

This account makes two main predictions. First, if the sounds are more acoustically different (e.g., "lif"/"neem" instead of "bih"/"dih"), babies would be more likely to posit and learn two word-referent associations, because there would be more evidence pointing in that direction from the auditory side. Second, if the objects are more semantically distant (a living object vs. a non-living object, instead of the two perceptually different toys used in the original study), babies would be more likely to posit and learn two word-referent associations, because now there would be more evidence pointing in that direction from the visual side. In fact, @stager1997 tested and proved the first prediction. The second prediction remains to be tested. If proven, it would provides support to the optimal combination account in early language learning.


In a similar study that used eye tracking in addition to overt responses, Creel (2012) replicted these findings and showed that the chidlren's visual fixations revealed slower recognition for the novel word, suggesting a hightened uncertainty.

(Merriman and Schuster, 1991; Creel, 2012).



In fact, though children start learning the categories at an early age, typically before their first birthday [e.g., @werker1984], this learning is not perfect from the start. In particular, early categories can be noisier and/or encoded with lower confidence. It follows that recognizing familar or novel words might be a greater challenge for children than it is for adults. For example, in a word recognition task, though toddlers prefer the correct pronounciation of a familiar word (e.g., "baby") over a novel but phononlogically similar word ("vaby"), they do not totally reject the novel word: they still look at the familiar word's referent more than a different object (Swingley and Aslin, 2000). A similar pattern was found with preschoolers using a mutual exclusivity paradigm. Mutual exclusivity is a bias that children have to interpret novel words as referring to novel objects rather than familiar objects (Markman & Wachtel, 1988). For example, when presented with a ball and a novel object and asked to select, say, a "dax", participants typically point to the novel object. Merriman and Schuster (1991) found that this effect decreases markedly in 4-year-olds if the novel word is phonologically similar to a familiar word. In other words, children do not necessarily interpret a novel word "bool" to be a label for the novel object, but tend to extend it to the familiar object (a ball). In a similar study that used eye tracking in addition to overt responses, Creel (2012) found that preschoolers, similar to Merriman and Schuster (1991), mostly select the familiar objet. Moreover, the chidlren's visual fixations indicated slower recognition for the novel word, suggesting a hightened uncertainty.  

Children representeation of newly learned words is even more fragile than their representation of familair words (Swingley, 2007). For example, @stager1997 showed that 14 m.o have difficulties mapping two minimally different sounds (e.g., "bih"/"dih") to two novel objects, despite the fact that they can perceptually differentiate these sounds. Specifically, their task required that infants reject the switch trials, that is, when an object (e.g. the one associated with the word "bih") is labelled with the wrong name ("dih").  Yoshida et al. (2009) tested infants with a preferential looking paradigm that reduced the memory demands in the orginal task. After the leanrning phase, they monitored the infants' looking patterns when presented with both objects at the same time, instead of comparing their attitude to the correct vs. the wrong associaton. In this simplified task, they found that infants succeeded in picking the correct object. In order to explain why infants fail in the switch task but succeed in the simpler preferential looking paradigm, Yoshida et al. (2009) proposed that phonological features might be encoded probabilistically. Early word representations for "din" and "bin"", though distinct, might be encoded with a relatively lower confidence.  Thus, uncertainty generated by  the matching bin label, may  not  surpass  the  threshold  levelrequired  for  rejection  of   the  pairing,  resulting  in  nodifference  between  the  same  and  Switch  trials 
In the visual choice paradigm, thecritical  criterion  involves  a  direct  comparison  of,  and  aforced choice between, the two objects. Here, the relativelyslight difference in uncertainty is sufficient; the 60% matchwould be considered to be a better fit than the 40% match,allowing infants to demonstrate their learning ability.

Actually this probabilistic accout can also explain the case of famialr words
Talk about Swingley 2007
For the semantic, see here:
https://www.ncbi.nlm.nih.gov/pubmed/12003515



Yoshida et al. (2009)




Our proposal explains these patterns of failure and success in terms of developmental changes in the variances associated with the visual and phonological categories, combined with the ability to integrate sound and visual cues according to their reliability. Suppose that both members of a given sound contrast ("bih"/"dih") initially fall within the uncertainty range of a single sound category. If t

There is evidence supporting (albeit indirectly) the possibility of effects of phonological similarityon induction. In particular,Merriman and Schuster (1991)demonstrated that phonological similarityof words led to attenuated ‘‘mutual exclusivity’’ effects in 2- and 4-year-olds. Mutual exclusivity is atendency of toddlers and young children to interpret novel words as referring to novel items ratherthan familiar items (Markman & Wachtel, 1988). For example, when presented with an apple and anovel object and asked to select adax, participants pointed to the novel object.Merriman and Schuster(1991)found that the effect decreases markedly or disappears if the novel word is phonologically sim-ilar to a familiar word; children would not necessarily extend a novel wordjappleto a novel object butmight instead extend it to an apple

More generally, toddler's eye fixation show a graded pattern in terms of recognition, that is, they look less often at the correct object as the mispronouciation gets gradually more different from the correct the pronounciation
more different the mispronouciation is to the correct pronooucition, the more 

children's looking patterns show a graded recognition uncertainty as the misponounciation's phonetic dissimilarity to the typical proniucaution increases (Morgan while...). 


One could even argue that the problem of word learning/recognition under multimodal uncertanty is a more pressing issue for children in the process of language acqiusition than it is for adults who already master the language.
forms with high phonetic details

Talk about Swingley work for familar words:

Talk about Yoshida for novel words

Talk about the fact that this uncertainty develops into early chidhood 


However, children do not wait to have fully mastered the speech and semantic categories before they start attempting to map form to meaning.

Because  of  the  greater  noise  inherent  in  children’s  perceptual- processing  systems,  the  same  acoustic  stimulus  may  effectively be less reliable for children than adults (Lyons &  Ghetti,  2011;  Neuman  &  Hochberg,  1983)


How can this formal characterization fit in a developmental scenario? One could imagine that, initially, visual and auditory categories have relatively large variances, and that development consists in reducing the values of the variances as evidence accumulates, making the categories more refined and precise. A scenario similar to this one has been previously suggested to explain the development of early sound categories. For example, @yoshida2009 suggested that the early categories may be noisier or that they may be encoded with relatively lower confidence. For the semantic categories, we can find similar (although perhaps not identical) ideas. For instance, Clark (1973) suggested that the initial meaning categories tend to be larger than adult categories, since fewer features are used to pick out referents (e.g., dog for all four-legged animal, ball for all round objects).



They also start to map sound a

However, this learning is not perfect from the start ,   


Indeed, since both the speech categories and the semantic categories are still not fully mastered by children, the lingusitic input can in generarl be more uncertain for the latter, which makes the problem of inference under uncertainty a more pressing issue. 


In early development, the combination of statistics from the auditory and the visual referent can be crucial to learn the  

Recognition of novel words 
For kids the uncertainty is higher 
word referent 
In this study, we explored the mechanism of word identification, but in a simple case when the underlying visual and auditory categories were familiar. Indeed, on the visual side, cat and dog tokens belong obviously to existing visual categories. On the auditory side, though the words ("ada"/"aba") are novel, the phonemic categories that differentiate the words are familiar (i.e., /d/ and /b/). In the context of early word learning, however, children are often learning both the categories and their mapping at the same time. In fact, children start learning visual and sound categories at an early age [e.g., @quinn93;@werker1984], and there is evidence that word-referent-like interaction operates at the same time, even when the categories are not yet fully mastered [e.g., @waxman1995; @yeung09]. In fact, @fourtassi2014a proposed that this interaction can create a synergy (or a bootstrapping effect) whereby progress made on one level (e.g., the meaning categories) can benefit the other level (the sound categories).

<!--This work, as well as the model, is about the mechanism of word-referent recognition when the underlying visual and auditory categories are supposed to be already familiar. On the visual side, cat and dog tokens belong obviousely to existing categories. Uncertainty is about where the boundary lies, and how this perceived boundary changes when evidence from the auditory data varies from clear to ambiguous. On the auditory side, though the words ("ada"/"aba") are novel, the phonemic categories that differentiate the words are familiar (i.e., /d/ and /b/). Thus, similar to the visual categories, uncertainty is about where the phonological boudary lies, and also how this perceived boundary changes when evidence from the visual data varies from clear to ambiguous. The main finding of this paper is that people shift the categories' boudary depending on evidence form both auditory and visual input, and that, generally speaking, evidence from both modalities are weighted in a near optimal way. -->

<!--

This, in turn, would make them interpret the sound difference as alternative pronunciations of the same word\footnote{The reverse reasining is plausible as well, and in fact, can explain why in the same experiment, the use of acoustically distant words ("lif"/"neem"" instead of "bin"/"din) made 14 m.o succeed in the same task}. A prediction of the optimal model would be that the use of semantically distant objects can induce success with the same similar sounding words. 


new explanations for some findings in the developmental literature. We will focus on two influenceial lines of work relatad to multimodal processing: the development of modality dominance (Robertson and Sloutsky, 2004), and the development of the learning of similar sounding words (Stagter and Werker, 1997).

Sloutsky and Napolitano (2003) reported a dominance for the auditory modality in children, and showed that this dominance disappears or reverses in adults. The authors attribute this dominance to the fact that auditory is more transient, and therefore tend to attract more attention. According to this interpretation, children should weight modalities in the same way regerdless of the stimuli used. Another interpretation, inspired by our model, would suggest that the precise sound and object contrasts used as stimuli in this series of experiments differ in their relative salience to children. One way to disentangle these two interpretations is to use an audio-visual stimuli where the sound contrast is reduced to a perceptual minimum and/or where the visual contrast is enhanced. An optimal-based account would suggest that the weighting would reverse in this case.   







it  may allow us to account for some findings in the dedvelopment literature  



The main finding of the present work is that when the input is multimodal, behavior is largely predicted by the value of the variance of the auditory category relative to the variance of the visual category. 







revisit some  findings in the develomental literature and speculate about possible explanations. We will focus on two influencial findings, one is the development of a phenomeon dubbed Auditory Overshadowing (Napolitano, Robertson, ...), and the other is the development of the ability to learn similar sounding words (Stager & Werker, 1997). 



we can still use its general idea (especially the fact that cues from a given modality are used according to their relative reliability) to revisit some findings in the literature and specualate about new interpretations. For instance, Sloutsky and Napolitano (2003) reported a peceptual dominance for the auditory modality in children, and showed that this dominance disappears or reverses in adults. The authors attribute this dominance to the fact that auditory is more transient, and therefore tend to attract more attention (I should elaborate more on this). According to this interpretation, children should weight modalities in the same way regerdless of the stimuli used. Another interpretation, inspired by our model, would suggest that the precise sound and object contrasts used as stimuli in this series of experiments differ in their relative salience to children. One way to disentangle these two interpretations is to use an audio-visual stimuli where the sound contrast is reduced to a perceptual minimum and/or where the visual contrast is enhanced. An optimal-based account would suggest that the weighting would reverse in this case.   

In a different line of work, it has been shown that infants have difficulties mapping two minimally different words to two novel objects, despite the fact that they can perceptually differentiate these words (e.g., Stager & Werker, 1997). This pattern of results has mostly been undertood as ‘failure’ in learning. Another interpretation  would explain this pattern of results based on developmental changes in the variance associated with the visual and phonological categories. If the categories get refined over time (that is, if the variances become smaller), then contrasts that used to fall under one category, may later fall under two separate categories. That is, rather than interpreting the null result as a failure in learning, these children might just have learned one word category instead of two categories. In particular, it is possible that the visual stimuli used in Stage and Werker (1997) were quite similar in the 14 m.o's early conceptual space, which made them believe that both objects were possible instances of the same semantic category. This, in turn, would make them interpret the sound difference as alternative pronunciations of the same word\footnote{The reverse reasining is plausible as well, and in fact, can explain why in the same experiment, the use of acoustically distant words ("lif"/"neem"" instead of "bin"/"din) made 14 m.o succeed in the same task}. A prediction of the optimal model would be that the use of semantically distant objects can induce success with the same similar sounding words. 

CONCLUSION


To estimate the reliability (e.g., whether a token belong to one or the other category), one has to have access to the full probability distribution of the tokens,  For instance, @sloutsky2003 reported a dominance for the auditory modality in children, and This dominance disappears or reverses in adults 

n account helps us better understand the mechanisms of word processing and learning.
 
We are working on extending this general framework to the case where neither the phonological, not the semantic categories are assumed to be present. Thus quantifying the synergy that might take place in development

Can we understand word learning (and failure in learning as in Stager and Werker, 1997) in terms of developmental changes in the reliability (or the relative reliability) in the visual and auditory modalities?








Computational principle behind bla bla

What influence the combination is both the category dstribution (example), and also how rliabile at all (regarless of whether the token in)
Here talk about theories of modality approp
riateness theory. 
Say that this is not what is going on here 


Talk about development
In Experiment 1, the unceratinty was 

 One possible explanation for this preference could be that people do not combine cross-modal uncertainties of a similar kind (e.g., ambiguity in both modalities) in the same way they would combine uncertainties of different kinds (e.g., ambiguity in one modality and noise in the other). For instance, it could be that the latter, but not the former, cause the over-reliance on a particular modality.








There were, however, quantitative differences. Audio-visual presentation increased the level of randomness in the participants' responses. One possible explanation is that this phenomenon was caused by the arbitrary nature of the form-meaning mapping. Previous studies suggest that while redundant multimodal information improves performance (e.g., determining the frequency of a bouncing ball from visual and auditory cues), arbitrary mappings generally tends to hinder performance [for review, see @robinson2010].

Interestingly, however, in Experiment 1 this increase in randomness occurred at a similar rate for both the auditory and the visual modality, and thus, it did not affect their relative weighting. The latter was primarily determined by informational reliability. Only when we intervened by adding  noise to one modality in Experiment 2, did participants show a systematic preference for the non-noisy modality. One possible explanation for this preference could be that people do not combine cross-modal uncertainties of a similar kind (e.g., ambiguity in both modalities) in the same way they would combine uncertainties of different kinds (e.g., ambiguity in one modality and noise in the other). For instance, it could be that the latter, but not the former, cause the over-reliance on a particular modality.

Overall, in both Experiments, the majority of the variance could be explained by an ideal observer that combined multimodal information optimally. In the light of this main result, we can revisit some previous findings in the literature. For instance, @sloutsky2003 reported a dominance for the auditory modality in children. This dominance disappears or reverses in adults. Could this difference be driven by changes across development in the level of perceptual noise affecting the intrinsic relative reliability of modalities (by analogy to Experiment 2)? More work is needed to carefully examine this (and other) speculations, and more generally, to determine the extent to which the optimal combination account helps us better understand the mechanisms of word processing and learning.

-->

# References
```{r create_r-references}
r_refs(file = "references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
