---
title             : "How Optimal is Word Recognition Under Multimodal Uncertainty?"
shorttitle        : "Word Identification Under Multimodal Uncertainty"
#numbersections: true

author: 
  - name          : "Abdellah Fourtassi"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "afourtas@stanford.edu"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"

author_note: |

  Abdellah Fourtassi
  
  Department of Psychology
  
  Stanford University
  
  50 Serra Mall
  
  Jordan Hall, Building 420
  
  Stanford, CA 94301
  

abstract: |

 Identifying a spoken word in a referential context requires both the ability to integrate multimodal input and the ability to reason under uncertainty. How do these tasks interact with one another? We study how adults identify novel words under joint uncertainty in the auditory and visual modalities and we propose an ideal observer model of how cues in these modalities are combined optimally. Model predictions are tested in four experiments where recognition is made under various sources of uncertainty. We found that participants use both auditory and visual cues to recognize novel words. When the signal is not distorted with environmental noise, participants weight the auditory and visual cues optimally, that is, according to the relative reliability of each modality. In contrast, when one modality has noise added to it, human perceivers systematically prefer the unperturbed modality to a greater extent than the optimal model does. This work extends the literature on perceptual cue combination to the case of word recognition in a referential context. In addition, this context offers a link to the study of multimodal information in word meaning learning.
  
keywords          : "Language understanding; audio-visual processing; word learning; speech perception; computational modeling."

header-includes:
   #- \usepackage{bibentry}
   - \usepackage[sortcites=false,sorting=none]{biblatex}
   
bibliography      : ["references.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf 

citation_package: biblatex

---

```{r load_packages, include = FALSE}
library(papaja)
library(readr)
library(tidyr)
library(ggplot2)
library(cowplot)
library(knitr)
library(boot)
library(dplyr)
library(langcog)
library(ggthemes)
library(broom)
library(kableExtra)
#library("knitcitations")
#cleanbib()
#options("citation_format" = "pandoc")
```

```{r}

#Data from the 3 experiments 
#############################
exp1 <- read_delim("../Data_and_analysis/data_exp1_anonym.txt", delim = " ") %>%
  filter(type == "Task") %>%
  mutate(experiment='Experiment1')

exp4 <- read_delim("../Data_and_analysis/data_rev_real1.txt", delim = " ") %>%
  filter(type == "Task") %>%
  mutate(experiment='Experiment1')

exp2 <- read_delim("../Data_and_analysis/data_exp2_anonym.txt", delim = " ") %>%
  filter(type == "Task") %>%
  mutate(experiment='Experiment2')

exp3 <- read_delim("../Data_and_analysis/data_exp3_anonym.txt", delim = " ") %>%
  filter(type == "Task") %>%
  mutate(experiment='Experiment3')
```



```{r}
#First Exlusion criteria:
#############################

#All data
N_all_1 <- exp1 %>%
  distinct(ID) %>%
  nrow()

N_all_2 <- exp2 %>%
  distinct(ID) %>%
  nrow()

N_all_3 <- exp3 %>%
  distinct(ID) %>%
  nrow()

N_all_4 <- exp4 %>%
  distinct(ID) %>%
  nrow()

#Participants who did not encounter a technical problem with the online experiment
 
#Exp1
exp1_noProb <- exp1 %>%
  filter(problem=="No") 

N_noProb_1 <- exp1_noProb %>%
  distinct(ID) %>%
  nrow()


#Exp2
exp2_noProb <- exp2 %>%
  filter(problem=="No") 

N_noProb_2 <- exp2_noProb %>%
  distinct(ID) %>%
  nrow()

#Exp3
exp3_noProb <- exp3 %>%
  filter(problem=="No") 

N_noProb_3 <- exp3_noProb %>%
  distinct(ID) %>%
  nrow()

#Exp 4
exp4_noProb <- exp4 %>%
  filter(problem=="No") 

N_noProb_4 <- exp4_noProb %>%
  distinct(ID) %>%
  nrow()

#Participants who who did not encounter a problem AND were above 50% accuracy on obvious trials

#Exp1
exp1_good <- exp1_noProb %>%
  filter(score > 0.5)

N_good_1 <- exp1_good %>%
  distinct(ID) %>%
  nrow()

#Exp2
exp2_good <- exp2_noProb %>%
  filter(score > 0.5)

N_good_2 <- exp2_good %>%
  distinct(ID) %>%
  nrow()

#Exp3
exp3_good <- exp3_noProb %>%
  filter(score > 0.5)

N_good_3 <- exp3_good %>%
  distinct(ID) %>%
  nrow()

#Exp 4 (The change in the selection threshold in 1 bis was re-registered: the longer experiment required stricter quality control)
exp4_good <- exp4_noProb %>%
  filter(score >= 0.75)

N_good_4 <- exp4_good %>%
  distinct(ID) %>%
  nrow()
```

# Introduction
Language uses symbols expressed in one modality --- the auditory modality, in the case of speech --- to communicate about the world, which we perceive through many different sensory modalities. Consider hearing someone yell "bee!" at a picnic, as a honey bee buzzes around the food. Identifying a word involves processing the auditory information as well as other perceptual signals (e.g., the visual image of the bee, the sound of its wings, the sensation of the bee flying by your arm). A word is successfully identified when information from these modalities provide convergent evidence. 

However, word identification takes place in a noisy world, and the cues received through each modality may not provide a definitive answer. On the auditory side, individual acoustic word tokens are almost always ambiguous with respect to the particular sequence of phonemes they represent, which is due to the inherent variability of how a phonetic category is realized acoustically [@hillenbrand1995]. And some tokens may be distorted additionally by mispronunciation or ambient noise. Perhaps the speaker was yelling "pea" and not "bee." Similarly, a sensory impression may not be enough to make a definitive identification of a visual category.\footnote{In the general case, language can of course be visual as well as auditory, and object identification can be done through many modalities. For simplicity, we focus on audio-visual matching here.} Perhaps the insect was a beetle or a fly instead.  How does the listener deal with such multimodal uncertainty to recognize the speaker's intended word? 

As a simplified case study of early word learning, the task of matching sounds to corresponding visual objects has been studied extensively in the developmental literature. For example, many studies focus on how children might succeed in this type of task despite referential ambiguity [@smith08; @pinker1989; @medina2011; @vouloumanos2008; @yurovsky2015; @suanda2014; @vlach2013]. However, even when they have learned the exact meaning of a word, observers (both children and adults) often still find it challenging to recognize which word the speaker has uttered, especially under noise [@mattys12; @peelle2018]. The purpose of the current study is thus to explore word recognition by adults under multimodal uncertainty, focusing on the special case where people have access to multimodal cues from the auditory speech and the visual referent. In the General Discussion, we return to the question of how these findings relate to questions about word learning. 

One rigorous way to approach this question is through conducting an *ideal observer* analysis. This research strategy provides a characterization of the task/goal and shows what the optimal performance should be under this characterization.^[It is, thus, a general instance of the rational approach to cognition [@anderson90], instantiating Marr's computational level of analysis [@marr1982].] When there is uncertainty in the input, the ideal observer performs an optimal probabilistic inference. For example, in order to recognize an ambiguous linguistic input, the model uses all available probabilistic knowledge in order to maximize the accuracy of this recognition. The ideal observer model can be seen as a theoretical upper limit on performance. It is not so much a realistic model of human performance, as much as a baseline against which human performance can be compared [@Geisler2003; @rahnev2018]. When there is a deviation from the ideal, it can reveal extra constraints on human cognition, such as limitations on the working memory or attentional resources. This approach has had a tremendous impact not only on speech-related research [@Norris08; @clayard08; @feldman2009; @kleinschmidt2015], but also on many other disciplines in the cognitive sciences [for reviews, see @chater06; @Knill04; @tenenbaum11]

Some prior ideal observer studies are closely related to the question we are addressing in the current work. For instance, @clayard08 simulated auditory uncertainty by manipulating the probability distribution of a cue (Voice Onset Time) that differentiated similar words (e.g., "beach" and "peach"). They found that humans were sensitive to these probabilistic cues and their judgments closely reflected the optimal predictions. And @feldman2009 studied the perceptual magnet effect, a phenomenon that involves reduced discriminability near prototypical sounds in the native language [@kuhl1991], showing that this effect can be explained as the consequence of optimally solving the problem of perception under uncertainty [see also @kronrod2016].  

Besides the acoustic cues explored in @clayard08 and @feldman2009, there is extensive evidence that information from the visual modality, such as the speaker's facial features,  also influences speech understanding [see @Campbell2008 for a review]. @bejjanki2011 offered a mathematical characterization of how probabilistic cues from speech and lip movements can be optimally combined. They showed that human performance during audio-visual phonemic labeling was consistent (at least at the qualitative level) with the predictions of an ideal observer. This previous research did not,  however, study speech understanding when visual information was obtained through the referential context rather than through observation of speaker's face. Although some experimental findings show that information about the identity of a referent can be integrated with linguistic information to resolve lexical and syntactic ambiguities in speech [e.g., @Eberhard1995; @Tanenhaus1995; @spivey2002], to our knowledge no study has offered an ideal observer analysis of this task. 

Combining information between words and visual referents might seem similar to audio-visual speech integration [e.g., @bejjanki2011], but there are at least two fundamental differences between these two cases, and both can influence the way the auditory and visual cues are combined. 

First, in the case of audio-visual speech, both modalities offer information about the same underlying speech category. They differ only in terms of their informational reliability. In a referential context, however, the auditory and visual modalities play different roles in the referential process -- the auditory input represents the *symbol* whereas the visual input represents the *meaning* (and these differences are in addition to possible differences in informational reliability). Further, speech is claimed to have a privileged status compared to other sensory stimuli [@vouloumanos2014; @waxman2009; @waxman1995; @edmiston2015; @lupyan2012], and this privilege is suggested to be specifically related to the ability to refer [@waxman2009].^[There is, however, a debate as to whether speech is privileged for children and adults for similar reasons. Whereas some researchers suggest that speech is privileged for both children and adults because of its ability to refer [e.g., @waxman2009], others suggest that speech might *not* have a referential status from the start. Rather, speech might be preferred by children only because of a low level auditory ``overshadowing'' [e.g., @sloutsky2003].] Thus, in a referential context, it is possible that listeners do not treat the auditory and visual modalities as equivalent sources of information. Instead, there could be a sub-optimal bias for the auditory modality beyond what is expected from informational reliability alone. 

Second, in the case of audio-visual speech, the auditory and visual stimuli are expected to be perceptually correlated. The expectation for this correlation is strong enough that when there is a mismatch between the auditory and visual input, they are still integrated into a unified (but illusory) percept [e.g., the McGurk Effect; @mcgurk1976]. In the case of referential language, however, the multimodal association is by nature *arbitrary* [@saussure1916; @greenberg1957].  For instance, there is no logical or perceptual connection between the sound "bee" and the corresponding insect. Moreover, variation in the way the sound "bee" is pronounced is generally not expected to correlate perceptually with variation in the shape (or any other visual property) in the category of bees. In sum, cue combination in the case of arbitrary audio-visual associations (word-referent) is likely to be less automatic, more effortful, and therefore less conducive to optimal integration than it is in the case of perceptually correlated associations (as in audio-visual speech perception). 

## The current study

We investigate how cues from the auditory and the visual modality are combined to recognize novel words in a referential context. In particular, we study how this combination is performed under various degrees of uncertainty in both the auditory and the visual modality. Imagine, for example, that someone is uncertain whether they heard "pea" or "bee." Does this uncertainty make them rely more on the referent (e.g., the object being pointed at)? Or, if they are not sure if they saw a bee or a fly, does this uncertainty make them rely more on the sound? More importantly, when input in both modalities is uncertain to varying degrees, do they weight each modality according to its relative reliability (the optimal strategy), or do they over-rely on a particular modality?

We begin by proposing an ideal observer model that performs the combination in an optimal fashion. We then compare the predictions of the optimal model to human responses. Humans can deviate from the ideal for several reasons. For instance, as mentioned above, a sub-optimality can be induced by the privileged status of a particular modality or by the arbitrariness of the referential association.  In order to study possible patterns of sub-optimality, we compare the optimal normative model to a descriptive model (which is fit to actual responses). Comparing parameter estimates between these two formulations allows us to quantify the degree of deviation from optimality.

We tested the ideal observer model's predictions in four behavioral experiments where we varied the source of uncertainty. In Experiment 1, audio-visual tokens were ambiguous with respect to their category membership (in addition to sensory noise). In Experiment 2, we intervened by adding environmental noise that degraded information from the auditory modality and in Experiment 3 we intervened by adding environmental noise that degraded information from the visual modality. Finally, Experiment 4 is a replication of Experiment 1 with a higher power design, allowing us to test cue combination at the individual level. 

<!--In all experiments, participants were quantitatively near-optimal, though overall response precision was slightly lower than expected. In Experiment 1 -- where neither of the modalities was perturbed with background noise -- participants weighted auditory and visual cues according to the relative reliability predicted by the optimal model. However, in Experiment 2 and 3, participants over-relied on one modality when the other modality was perturbed with additional noise. -->

# Paradigm and Models 

In this section we first briefly introduce the multimodal combination task. Then we explain how behavior in this paradigm can be characterized optimally with an ideal observer model.

## The Audio-Visual Word Recognition Task

We introduce an experimental paradigm adapted from a task used by @sloutsky2003. The original was used with both children and adults to probe audio-visual encoding [see @robinson2010 for review]. Here we use a slightly different version to test word recognition in a referential context.  We use two visual categories (cat and dog) and two auditory categories (/b/ and /d/ embedded in the minimal pair /aba/-/ada/). For each participant, an arbitrary pairing is set between the auditory and the visual categories, leading to two audio-visual word categories (e.g., dog-/aba/, cat-/ada/). In each trial, participants are presented with an audio-visual target (the prototype of the target category), immediately followed by an audio-visual test stimulus (Figure\ \@ref(fig:task)). The test stimulus may differ from the target in both the auditory and the visual components.  After these two presentations, participants press "same" or "different."


```{r task, fig.cap = "Overview of the task. In the audio-visual condition, participants are first presented with an audio-visual target (the prototype of the target category), immediately followed by an audio-visual test. The test may differ from the target in both the auditory and the visual components. After these two presentations, participants press `same' (i.e., the same category as the target) or `different' (not the same category). The auditory-only and visual-only conditions are similar to the audio-visual condition, except that only the sounds are heard, or only the pictures are shown, respectively.", fig.align = "center", out.width = "400px"}
knitr::include_graphics("pictures/task.png", dpi = 108)
```

In the testing phase of the original task [@sloutsky2003], participants were asked whether or not the two audio-visual presentations are *identical*. In the current study, we are interested, rather, in the categorization, i.e., determining whether or not two similar tokens are members of the same phonological/semantic category. Therefore, testing in our task is category-based: Participants are asked to press "same" if they think the second item (the test) belongs to the same category as the first (target) (e.g.,  dog-/aba/), even if there is a slight difference in the sound, in the referent, or in both. They are instructed to press "different" only if they think that the second stimulus was an instance of the other category (cat-/ada/). The task also includes trials where pictures are hidden (audio-only) or where sounds are muted (visual-only). These unimodal trials provide us with the participants' evaluation of the probabilistic information present in the auditory and visual categories. As we shall see, these unimodal distributions are used as inputs to the optimal cue combination model.


## Optimal Model

We construct an ideal observer model that combines probabilistic information from the auditory and visual modalities. In contrast to the  model used in most research on multisensory integration [e.g., @ernst02], which typically studies continuous stimuli (e.g., size, location), the probabilistic information in our case cannot be characterized with *sensory noise* only.  Since our task involves responses over categorical variables (phonemes and concepts), the optimal model should take into account not only the noise variability around an individual perceptual estimate but also its *categorical variability*, i.e., the uncertainty related to whether this perceptual estimate belongs to a given category [see also @Bankieris17; @bejjanki2011]. In what follows, we describe a probabilistic model that accounts for both types of variability. First, we describe the model in the simplified case of categorical variability only. Second, we augment this simplified model to account for sensory and environmental noise.

### Categorical variability

We assume that both the auditory categories (i.e., /aba/ and /ada/) and the visual categories (cat and dog) are distributed along a single acoustic and semantic dimension, respectively (Figure\ \@ref(fig:model)). Moreover, we assume that all categories are normally distributed. Formally speaking, if $A$ denotes an auditory category (/ada/ or /aba/), then the probability that a point $a$ along the acoustic dimension belongs to the category $A$ is
$$ p(a | A) \sim  N(\mu_A, \sigma^2_A) $$
where $\mu_A$ and $\sigma^2_A$ are respectively the mean and the variance of the auditory category.
Similarly, the probability that a point $v$ along the visual dimension belongs to the category $V$ is
$$ p(v | V) \sim  N(\mu_V, \sigma^2_V) $$
where $\mu_V$ and $\sigma^2_V$ are the mean and the variance of the visual category.
An audio-visual signal $w=(a,v)$ can be represented as a point in the audio-visual space. These audio-visual tokens define bivariate distributions in the bi-dimentional space. We call these bivariate distributions *Word categories*, noted $W$, and are distributed as follows: 
$$ p(w | W) \sim  N(M_W, \Sigma_W) $$
where $M_W=(\mu_A, \mu_V)$ and $\Sigma_W$ are the mean and the covariance matrix of the word category. The main assumption of the model is that the auditory and visual variables are independent (i.e., uncorrelated), so the covariance matrix is simply:
 \[
   \Sigma_W=
  \left[ {\begin{array}{cc}
   \sigma^2_A & 0 \\
   0 & \sigma^2_V \\
  \end{array} } \right]
\]

```{r model, out.width = '\\textwidth', fig.pos = "!h", fig.cap = "Illustration of the model using simulated data. A word category is defined as the joint bivariate distribution of an auditory category (horizontal, bottom panel) and a visual semantic category (vertical, left panel). Upon the presentation of a word token $w$, participants guess whether it is sampled from the word type $W_1$ or from the word type $W_2$. Decision threshold is where the guessing probability is 0.5."}
knitr::include_graphics("pictures/model.png", dpi = 108)
```


\noindent This assumption says that, given a word-object mapping, e.g., $W=$("cat"-CAT), variation in the way "cat" is pronounced does not correlate with changes in any visual property of the object CAT, which is a valid assumption in the context of our task.\footnote{Note that this assumptions is more adequate in the case of arbitrary associations such as ours, and less so in the case of redundant association such as audio-visual speech. In the latter, variation in the pronunciation is expected to correlate, at least to some extent, with lip movements.}

Now we turn to the crucial question of modeling how the optimal decision should proceed given the probabilistic (categorical) information in the auditory and the visual modalities, as characterized above. We have two word categories: dog-/aba/ ($W_1$) and cat-/ada/ ($W_2$).\footnote{This mapping is randomized in the experiments.} When making decisions, participants can be understood as choosing one of these two word categories (Figure\ \@ref(fig:model)). For an ideal observer, the probability of choosing category 2 when presented with an audio-visual instance $w=(a,v)$ is the posterior probability of this category:
$$
p(W_2 | w)=\frac{p(w|W_2)p(W_2)}{p(w|W_2)p(W_2)+p(w|W_1)p(W_1)}
$$
Using our assumption that the cues are uncorrelated, we have:
$$p(w | W) = p(a,v| W) = p(a| A)p(v| V)$$
Under this assumption, the posterior probability reduces to the following formula (see Appendix 1 for the details of the derivation):
\begin{equation}
 p(W_2 | w)=\frac{1}{1+(1+b)\exp(\beta_0+\beta_aa+\beta_vv)}
\end{equation}
where 
$$1+b=\frac{p(W_1)}{p(W_2)}$$
$$\beta_0=\frac{\mu^2_{A2}-\mu^2_{A1}}{2\sigma^2_{A}}+\frac{\mu^2_{V2}-\mu^2_{V1}}{2\sigma^2_{V}}$$

$$\beta_a=\frac{\mu_{A1}-\mu_{A2}}{\sigma^2_{A}}$$
$$\beta_v=\frac{\mu_{V1}-\mu_{V2}}{\sigma^2_{V}}.$$

The parameter $b$ represents the differential between the categories' prior probabilities. However, since the identity of word categories is randomized across participants, $b$ measures, rather, a response bias to "same" if $b > 0$, and a response bias to "different" if $b < 0$. We expect a general bias towards answering "different" because of the categorical nature of our same-different task: When two items are ambiguous but perceptually different, participants might have a slight preference for "different" over "same". As for the means, their values are fixed, and they correspond to the most typical tokens in our stimuli. Finally, observations from each modality ($a$ and $v$) are weighted in Equation 1 according to their reliability (that is, according to the *inverse* of their variance): $$\beta_a \propto \frac{1}{\sigma^2_{A}}$$ $$\beta_v \propto \frac{1}{\sigma^2_{V}}.$$

### Sensory variability

So far, we have only accounted for categorical variability, i.e.,  $\sigma^2_{A} = \sigma^2_{A_C}$. For instance, if the speaker generates a target production $a_t$ from an auditory category
$p(a_t | A) \sim N(\mu_{A}, \sigma^2_{A_C})$, the ideal model assumes that it has direct access to this production token (i.e., $a=a_t$), and that all uncertainty is about the category membership of this token. However, we might also want to account for internal noise in the brain and/or external noise in the environment.  For example, the observer might not have access to the exact produced target, but only to the target perturbed by noise. If we assume this noise to be normally distributed, that is,  $p(a | a_t) \sim N(a_t, \sigma^2_{A_N})$, then integrating over $a_t$ leads to this new expression of the probability distribution:
$$ p(a | A) \sim N(\mu_{A}, \sigma^2_{A_C}+\sigma^2_{A_N})$$
Similarly, in the case of sensory noise in the visual modality, we get:
$$ p(a | V) \sim N(\mu_{V}, \sigma^2_{V}+\sigma^2_{V_N})$$
Finally, using exactly the same derivation as above, we end up with the following multimodal weighting scheme in the optimal combination model (Equation 1) which takes into account both categorical and sensory variability:

 $$\beta_a \propto \frac{1}{\sigma^2_{A_C}+\sigma^2_{A_N}}$$ 
 $$\beta_v \propto \frac{1}{\sigma^2_{V_C} +\sigma^2_{V_N}}.$$
 
 
### Optimal cue combination 

Equation 1 provides the optimal model's predictions for how probabilities that characterize uncertainty in the auditory and the visual modalities can be combined to make categorical decisions. Parameter estimates of the probability distributions in each modality are derived by fitting unimodal posteriors to the participants' responses in the unimodal conditions, i.e., the condition where only the sounds are heard  or only the pictures are seen (Figure\ \@ref(fig:task)).\footnote{Further technical detail about model fitting in the unimodal conditions will be given in the method section of Experiment 1.} Using these derived parameters, the optimal model makes predictions about responses in the bimodal (i.e., audio-visual) condition where participants both hear the sounds and see the pictures. 

### Auditory and Visual baselines

The predictions of the optimal model will be compared to two baselines. The first baseline is a visual model which assumes that participants rely only on visual information, and an auditory model, which assumes that participants rely only on auditory information. More precisely, these baseline models assume that the participants' responses in the bimodal condition will not be different from their response in either the visual-only or the auditory-only condition. However, if the participants rely on both the auditory and the visual modalities to make decision in the bimodal condition, the optimal model would explain more variance in human responses than the visual or the auditory model do.

## Descriptive model and analysis of (sub-)optimality

The optimal model (as well as the auditory and visual baselines) are *normative* models. Their predictions are made about human data in the bimodal condition, but their  parameters (i.e., variances associated with the visual and auditory modalities) are derived from data in the unimodal conditions.
In addition to these normative models, we consider a *descriptive* model. It is formally identical to the normative optimal model (Equation 1), except that the parameters are fit to actual responses in the bimodal condition. If the referential task induces sub-optimality (due, for instance, to the arbitrary nature of the sound-object association), then the descriptive model should explain more variance than the optimal model does. 

Comparison of the optimal and the descriptive models allows us, not only to quantify how much people deviate from optimality, but also to understand precisely the nature of this deviation. Let $\sigma^2_{A}$ and $\sigma^2_{V}$ be the values of the variances used in the optimal model (derived from the unimodal conditions), and $\sigma^2_{Ab}$ and $\sigma^2_{Vb}$ be the values observed through the descriptive model in the bimodal condition. Deviation from optimality is measured in two ways. First, we measure the change in the values of the variance specific to each modality, that is, how $\sigma^2_{A}$ compares to $\sigma^2_{Ab}$, and how $\sigma^2_{V}$ compares to $\sigma^2_{Vb}$. Second, we measure changes in the proportion of the visual and auditory variances, i.e., we examine how $\frac{\sigma^2_{A}}{\sigma^2_{V}}$ compares to $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}}$. The first measure allows us to test if response precision changes for each modality when we move from the unimodal to the bimodal conditions. The second allows us to test the extent to which the weighting scheme follows the prediction of the optimal model.  The reason we used the proportion of the variances as a measure of cross-modal weighting is because this proportion corresponds to the slope\footnote{Or more precisely the absolute value of the slope.} of the decision threshold in the audio-visual space (Figure\ \@ref(fig:model)). The decision threshold is defined as the set of values in this audio-visual space along which the posterior is equal to 0.5. Formally speaking, the decision threshold has the following form:

$$v=-\frac{\sigma^2_V}{\sigma^2_A}a+v_0$$

If the absolute value of the slope derived from the descriptive model is greater than that of the optimal model, the corresponding shift in the decision threshold indicates that participants have a preference for the auditory modality in the bimodal case. Similarly, a smaller absolute value of the slope would lead to a preference for the visual modality. The limit cases are when there is exclusive reliance on the auditory cue (a vertical line), and where there is exclusive reliance on the visual (a horizontal line). 

There are three possible ways human responses can deviate from optimality. These scenarios are illustrated in Figure\ \@ref(fig:subOptim), and are as follows: 

1) Both variances may increase, but their proportion remains the same. That is, $\sigma^2_{Ab} \geqslant \sigma^2_{A}$ and $\sigma^2_{Vb} \geqslant \sigma^2_{V}$, but  $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}} \approx \frac{\sigma^2_{A}}{\sigma^2_{V}}$. In this case, sub-optimality would be due to increased randomness in human responses in the bimodal condition. However, this randomness would not affect the relative weighting of both modalities, i.e., participants would still weigh modalities according to the relative reliability predicted by the optimal model.

2) The auditory variance increases at a higher rate.  That is, $\sigma^2_{Ab} \gg \sigma^2_{A}$ and $\sigma^2_{Vb} \geqslant \sigma^2_{V}$, leading to $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}} > \frac{\sigma^2_{A}}{\sigma^2_{V}}$. In this case, sub-optimality would consist not only in participants being more random in the bimodal condition, but also in having a systematic preference for the visual modality, even after accounting for informational reliability. 

3) The visual variance increases at a higher rate. That is, $\sigma^2_{Vb} \gg \sigma^2_{V}$, and  $\sigma^2_{Ab} \geqslant \sigma^2_{A}$, leading to $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}} > \frac{\sigma^2_{A}}{\sigma^2_{V}}$. This case is the reverse of case 2, i.e., in addition to increased randomness in the bimodal condition, there is a systematic preference for the auditory modality, even after accounting for informational reliability. 

```{r subOptim, out.width = "\\textwidth", fig.pos = "!h", fig.cap = "Illustration using simulated data showing the example of a prediction made by the optimal model (top), and the three possible ways human participants can deviate from this prediction (bottom). These cases are the following: 1) The variance increases equally for both modalities, but the weighting scheme (characterized by the decision threshold) is optimal, 2) The auditory variance increases at a higher rate, leading to a preference for the auditory modality, and 3) The visual variance increases at a higher rate, leading to a preference for the visual modality."}

knitr::include_graphics("pictures/sub-optimal", dpi = 108)
```

We compared these models to human responses in three experiments. In Experiment 1, we studied the case where bimodal uncertainty was due to categorical variability and sensory noise. In Experiment 2 and 3 we added environmental noise to the auditory and the visual modalities, respectively. 

# Experiment 1

In this Experiment, we test the predictions of the model in the case where uncertainty is due to categorical variability (i.e., ambiguity in terms of category membership) and inherent sensory noise. We do not add any external noise to the background. Thus, we test the following (normative) cue weighting scheme:

$$\beta_a \propto \frac{1}{\sigma^2_{A}} = \frac{1}{\sigma^2_{A_C} + \sigma^2_{A_N}}$$
$$\beta_v \propto \frac{1}{\sigma^2_{V}} = \frac{1}{\sigma^2_{V_C} + \sigma^2_{V_N}}.$$  

## Methods

### Participants

We recruited a planned sample of `r N_all_1` participants from Amazon Mechanical Turk. Only participants with US IP addresses and a task approval rate above 85\% were allowed to participate. Participants were excluded if they reported having experienced a technical problem of any sort during the online experiment (N=`r N_all_1 - N_noProb_1`), or if they had less than 50\% accurate responses on the unambiguous training trials (N=`r N_noProb_1 - N_good_1`). The final sample consisted of N = `r N_good_1` participants. All participants provided informed consent before taking the experiment. \footnote{The sample size and exclusion criteria were specified in the pre-registration at https://osf.io/h7mzp/.}

### Stimuli
For auditory stimuli, we used the continuum introduced in @vroomen2004, a 9-point /aba/--/ada/ speech continuum created by varying the frequency of the second (F2) formant in equal steps. We selected 5 equally spaced points from the original continuum by keeping the endpoints (prototypes) 1 and 9, as well as points 3, 5, and 7 along the continuum. For visual stimuli, we used a cat/dog morph continuum introduced in @freedman2001. From the original 14 points, we selected 5 points as follows: we kept the item that seemed most ambiguous (point 8), the 2 preceding points (i.e., 7 and 6) and the 2 following points (i.e., 9 and 10). The 6 and 10 points along the morph were quite distinguishable, and we took them to be our prototypes. 

### Design and Procedure
We told participants that an alien was naming two objects: a dog, called "aba" in the alien language, and a cat, called "ada". In each trial, we presented the first object (the target) on the left side of the screen simultaneously with the corresponding sound. For each participant, the target was always the same (e.g., dog-/aba/). The second sound-object pair (the test) followed on the other side of the screen after 500ms and varied in its category membership. For both the target and the test, visual stimuli were present for the duration of the sound clip ($\sim$ 800ms). We instructed participants to press "S" for same if they thought the alien was naming another dog-/aba/, and "D" for different if they thought the alien was naming a cat-/ada/. We randomized the sound-object mapping (e.g., dog-/aba/, cat-/ada/) as well as the identity of the target (dog or cat) across participants.

The first part of the experiment trained participants using only the prototype pictures and the prototype sounds (12 trials, 4 each from the bimodal, audio-only, and visual-only conditions). After completing training, we instructed participants on the structure of the task and encouraged them to base their answers on both the sounds and the pictures (in the bimodal condition). There were a total of 25 possible combinations in the bimodal condition, and 5 in each of the unimodal conditions. Each participant saw each possible trial twice, for a total of 70 trials/participant. Trials were blocked by condition and blocks were presented in random order. The experiment lasted around 15 minutes.^[The experiment can be accessed and played from the github repository: https://github.com/afourtassi/WordRec/]

### Model fitting details
#### Unimodal conditions
Remember that data in these conditions allows us to derive the variances of both the auditory and the visual categories, and that these variances are used to make predictions about bimodal data (in the visual and auditory baselines as well as in the optimal model). These individual variances were derived as follows (we explain the derivation for the auditory-only case, but the same applies for the visual-only case). We use the same Bayesian reasoning as we did in the derivation of the bimodal model: When presented with an audio instance $a$, the probability of choosing the sound category 2 (that is, to answer "different") is the posterior probability of this category $p(A_2|a)$. If we assume that both sound categories have equal variances, the posterior probability reduces to:

$$p(A_2 | a)=\frac{1}{1+(1+b_A)\exp(\beta_{a0}+\beta_aa)}$$

with $\beta_a=\frac{\mu_{A_1}-\mu_{A_2}}{\sigma^2_{A}}$ and  $\beta_{a0}=\frac{\mu^2_{A_2}-\mu^2_{A_1}}{2\sigma^2_{A}}$. $b_A$ is the response bias in the auditory-only condition. For this model (as well as all other models in this study), we fixed the values of the means to be the end-points of the corresponding continuum, since these points are the most typical instances in our stimuli. Thus, we have $\mu_{A1}=0$ and $\mu_{A2}=4$ (and similarly $\mu_{V1}=0$, and $\mu_{V2}=4$). This leaves us with two free parameters: the bias $b_A$ and the variance $\sigma^2_{A}$. To determine the values of these parameters, we fit the unimodal posterior to human data in the unimodal case. 

#### Bimodal condition
In this condition, only the descriptive model is fit to the data, using the expression of the posterior (Equation 1). Since the values of the means are fixed, we have 3 free parameters: the variances for the visual and the auditory modalities, respectively, and $b$, the response bias.  The visual and auditory baselines as well as the optimal model are not fit to the bimodal data, but their predictions are tested against these bimodal data. All these normative models use the variances derived from the unimodal data and the bias term derived from the fit to bimodal data. 

Although the paradigm is within-subjects, we did not have enough statistical power to fit a different model for each individual participant (but see Experiment 4). Instead, models were constructed with data collapsed across all participants. The fit was done with a nonlinear least squares regression using the NLS package in R [@bates88]. We computed the values of the parameters using non-parametric bootstrap (with 10000 iterations).

```{r param, results="asis", echo=FALSE}

parameters <- feather::read_feather("../Data_and_analysis/saved_data/parameters.feather") 

parameters[,-1] <-round(parameters[,-1],2)

names(parameters) <- c("Experiment",  "b\\textsubscript{A}", "Var\\textsubscript{A}", "b\\textsubscript{V}", "Var\\textsubscript{V}", "b\\textsubscript{b}","Var\\textsubscript{Ab}","Var\\textsubscript{Vb}")

kable(parameters, format = "latex", escape = FALSE, booktabs = TRUE,
      linesep = "", format.args = list(big.mark = ","),
      caption = "Statistics for the dataset we used."#,
      #col.names = c("Experiment",  "b~A~", expression("*o['T']*"), "b~V~", "sigma~V~", "b~b~","sigma~Ab~","sigma~Vb~")
      ) %>%
  add_header_above(c("", "Auditory" = 2, "Visual" = 2, "Bimodal" = 3 )) %>%
  column_spec(1, bold = TRUE) %>%
  kable_styling(position = "center")

```

## Results and analysis

### Unimodal conditions

Average categorization judgments and best fits are shown in Figure \@ref(fig:unimodal). The categorization function of the auditory condition was slightly steeper than that of the visual condition, meaning that participants perceived the sound tokens slightly more categorically and with higher certainty than they did with the visual tokens.  The unimodal models' estimates are shown in Table \@ref(tab:param).

```{r unimodal, out.width = "\\textwidth", fig.pos = "!h", fig.cap = "Human responses in the unimodal conditions across the three experiments. Points represent the proportion of `different' to `same' responses in the auditory-only condition (left), and visual-only condition (right). Error bars are 95\\% confidence intervals. Solid lines represent best unimodal posterior fits."}

exp_uni <- feather::read_feather("../Data_and_analysis/saved_data/exp_uni.feather")
pred_uni <- feather::read_feather("../Data_and_analysis/saved_data/pred_uni.feather")

### plot unimodal data + fit functions 
ggplot(exp_uni, 
       aes(x = distance, y = mean)) + 
  #geom_point()+
  geom_pointrange(aes(ymin = lower, ymax = upper), 
                  position = position_dodge(width = .1), size = 0.4, fatten = 2) + 
  #geom_line(data=uniV,aes(x=xV, y=yV))+
  geom_line(data=pred_uni, aes(x=distance, y=prediction), col='black')+
  xlab("Distance") +ylab("Prob. different")+
  scale_y_continuous(limits = c(0, 1))+#theme(aspect.ratio = 0.7)+
  theme_few()+
  theme(aspect.ratio = 0.7) + facet_grid(experiment ~ Condition)
  #stat_function(fun = Logistic_v, colour="red"))

```


### Bimodal condition

Figure \@ref(fig:bimodal) compares the predictions of the normative and descriptive models against human responses. Remember that the normative models use the parameters estimated from the unimodal conditions (where people see input from only one modality) to predict behavior in the bimodal condition (where people see input from both modalities). The descriptive model has a similar structure than the optimal model, but is directly fit to human responses in the bimodal condition in order to allow us to assess deviation from optimality. 

We found, through comparing the correlation values, that the optimal model explained more variance than the visual and auditory models did. However, the optimal model was not perfect: It explained less variance than the descriptive model did, which indicates a deviation from optimality. To investigate this deviation, we compare the parameter values of the optimal model to the values obtained in the descriptive model (Table \@ref(tab:param)).\footnote{Note that the descriptive model explained almost all the variance in mean responses, which makes it a reasonable proxy for human real performance in the bimodal condition.}. We note an increase in *both* the auditory and visual variances. This increase in noise is compatible with the fact that human responses appear to be pulled towards chance (i.e., the value 0.5) when compared to the optimal model (see \@ref(fig:bimodal)). Below we investigate if this deviation from optimality can be related to the cue combination strategy.

### Cue combination

We analyzed if the cue combination was performed in an optimal way, or if there was a systematic preference for one modality when making decisions in the bimodal condition. As explained in \@ref(fig:subOptim), modality preference can be characterized formally as a deviation from the decision threshold predicted by the optimal model. The results in Figure \@ref(fig:bias) (top) show both the decision threshold derived from the descriptive model (in black) and the decision threshold predicted by the optimal model (in red). We found that the descriptive and optimal decision thresholds were almost identical. Indeed, non-parametric resampling of the data showed no evidence of a deviation from the optimal prediction (Figure\ \@ref(fig:bias), bottom). 

<!--\noindent The descriptive model explained almost all the variance of total variance.  However, unlike the normative models, the descriptive model was fit to the same bomodal data. Thus, there is a risk that this high correlation is due to overfitting. To examine this possibility, we cross-validated the model using half the responses to predict the other half (averaging across X random partitions). The predictive power of the model remained very high (XX). -->

```{r bimodal, out.width = "\\textwidth", fig.pos = "!h", fig.cap = "Human responses vs. Models' predictions in the bimodal condition across the three experiments. Each point represents data form a particular audio-visual matching (corresponding to an instance from the set of 5x5 possible matchings in the audio-visual space). Shape represents auditory distance from the target, and color represents visual distance from the target. Thus, each point is characterized by both  shape and color."}

pred_bimod <- feather::read_feather("../Data_and_analysis/saved_data/all_bimod.feather")

pred_bimod$model <- factor(pred_bimod$model, levels = c('Visual','Auditory', 'Optimal', 'Descriptive'))

ggplot(pred_bimod, 
       aes(x = pred, y = Joint, col = factor(concept_dist), 
           shape = factor(sound_dist))) + 
  geom_point(size =1)+
  scale_colour_solarized()+
 #geom_pointrange(aes(ymin = summary_ci_lower, ymax = summary_ci_upper), 
  #                position = position_dodge(width = .1), size=0.2) + 

  geom_abline(slope = 1, lty = 2) +
  #annotate("text",aes(label=paste("R^2 ==", cor, sep="")), x=0.1, y=0.9, size=2, col='black', fontface = "bold", parse=TRUE)+
  geom_text(aes(label=paste("r^2 ==", cor, sep="")), x=0.15, y=0.9, size=2, col='black',fontface = "bold", parse=TRUE)+
  xlab("Predictions") +ylab("Human data")+
  facet_grid(experiment ~ model)+
  theme_few()+
theme(aspect.ratio = 0.7, 
      axis.text=element_text(size=6),
      strip.text.y = element_text(size = 8))+
  guides(color=guide_legend(title="Visual Distance")) +
  guides(shape=guide_legend(title="Auditory Distance")) 

```


## Discussion

This experiment studied the way participants combine multimodal information to recognize novel words. We found that the optimal model explained more variance than the auditory or the visual models did, indicating that participants take into account both the auditory and visual cues when making a decision. That said, Figure \@ref(fig:bimodal) shows that the participants deviated slightly --- but systematically--- from the optimal prediction in that they were slightly pulled toward chance (i.e., the probability 0.5). This fact was captured by the increase in the value of the variance associated with each modality (as can be noted from Table \@ref(tab:param)). Note, however, that despite this increase in response randomness, our analysis of modality preference showed that the *relative* values of these variances were not different (Figure\ \@ref(fig:bias)), meaning that there was no evidence for a modality preference. 

To sum up, 1) the participants used both the auditory and visual information, 2) they responded slightly more randomly that what was predicted, but 3) this increased randomness was general and did not influence the cue combination strategy, i.e., the participants still weighted modalities according to their relative reliability as predicted by the optimal model. This situation corresponds to the first case of sub-optimality described in Figure\ \@ref(fig:subOptim). 

In Experiment 1, we tested word recognition when there was multimodal uncertainty in terms of category membership and perceptual noise. In real life, however, both sound and visual tokens can undergo distortions due to noisy factors in the environment (e.g., car noise in the background, blurry vision in a foggy weather). In Experiment 2 and 3, we explore this additional level of uncertainty. 

# Experiment 2

In this Experiment, we explored the effect of added envirnonmental noise $\sigma^2_{E}$ on performance. We tested a case where the background noise was added to the auditory modality. We were interested to know if participants would treat this new source of uncertainty as predicted by the optimal model, that is, according to the following weighting scheme:

$$\beta_a \propto \frac{1}{\sigma^2_{A}} = \frac{1}{\sigma^2_{A_C}+\sigma^2_{A_N} + \sigma^2_{A_E}}$$ 
$$\beta_v \propto \frac{1}{\sigma^2_{V}} = \frac{1}{\sigma^2_{V_C}+\sigma^2_{V_N}}.$$

The alternative hypothesis is that noise in one modality leads to a systematic preference for the non-noisy modality.  

## Methods

### Participants

A sample of `r N_all_2` participants was recruited online through Amazon Mechanical Turk. We used the same exclusion criteria as in Experiment 1. `r N_noProb_2 - N_good_2` participants were excluded because they had less than 50\% accurate responses on the unambiguous training trials. The final sample consisted of N = `r N_good_2` participants.

### Stimuli and Procedure

We used the same visual stimuli as in Experiment 1. We also used the same auditory stimuli, but we convolved each item with Brown noise of amplitude 1 using the free sound editor Audacity (2.1.2). The average signal-to-noise ratio was - 4.4 dB. The procedure was exactly the same as in the previous experiment, except that the test stimuli (but not the target) were presented with the new noisy auditory stimuli.

## Results

The analysis are similar to the analysis we did in Experiment 1.

### Unimodal condition
We fit a model for each modality.  Figure \@ref(fig:unimodal) shows human responses together with their best fits. The visual data is a replication of the visual data in Experiment 1. The auditory data, in contrast, were flatter, showing more uncertainty.

### Bimodal condition
We used the values derived from the unimodal condition to construct the visual, auditory and optimal models. In addition, we fit a descriptive model which allowed us to assess real human performance in this condition. Figure \@ref(fig:bimodal) shows that, similar to Experiment 1, the optimal model explained more variance than the auditory and visual models did (note, however, that the visual model explained more variance than the auditory model did). Also similar to Experiment 1, the values of the variances increased in the bimodal condition (Table \@ref(tab:param)). 

### Cue combination
Here we investigated whether the observed increase in the auditory and visual variances affected the relative weighting of the corresponding modalities. Figure\ \@ref(fig:bias) (top) shows that the participants' decision threshold deviated from optimality, and that this deviation was biased towards the visual modality (the non-noisy modality). Indeed non-parametric resampling of the data showed a decrease in the value of the slope in the descriptive model compared to the optimal model (Figure\ \@ref(fig:bias), bottom).

## Discussion
Experiment 2 tested audi-visual combination in the case where the auditory input was noisy. We found, similar to Experiment 1, that the optimal model explained more variance than the auditory or the visual models did. In other words, despite additional noise, participants still used information from the noisy modality to recognize words. We also found a similar discrepancy between the descriptive and optimal models as response randomness increased along both the auditory and the visual modalities. As for the relative weighting, and contrary to Experiment 1 where modalities were weighted optimally, we found in this experiment that the visual modality had a greater weight than what was expected from its relative reliability. This situation corresponds to the second case of sub-optimality described in Figure \@ref(fig:subOptim).

Whereas in Experiment 2 we tested the case of added background noise to the auditory modality, in Experiment 3 we test the case of added noise to the visual modality.

# Experiment 3

Similar to Experiment 2, we were interested to know if participants would treat additional uncertainty as predicted by the optimal model, that is, according to the following weighting scheme: 

$$\beta_a \propto \frac{1}{\sigma^2_{A}} = \frac{1}{\sigma^2_{A_C}+\sigma^2_{A_N}}$$ 
$$\beta_v \propto \frac{1}{\sigma^2_{V}} = \frac{1}{\sigma^2_{V_C}+\sigma^2_{V_N} + \sigma^2_{V_E}}.$$

The alternative hypothesis is that noise in the visual modality would lead to a preference for the auditory input, just like noise in the auditory modality lead to a preference for the visual input in Experiment 2.  

## Methods

### Participants

A planned sample of `r N_all_3` participants was recruited online through Amazon Mechanical Turk. We used the same exclusion criteria as in both previous experiments. N=`r N_all_3 - N_noProb_3` participants were excluded because they reported having a technical problem, and N=`r N_noProb_3 - N_good_3` participants were excluded because they had less than 50\% accurate responses on the unambiguous training trials. The final sample consisted of N = `r N_good_3` participants.

### Stimuli and Procedure
We used the same auditory stimuli as in Experiment 1. We also used the same visual stimuli, but we blurred the tokens using the free image editor GIMP (2.8.20). We used a Gaussian blur with a radius\footnote{A features that modulates the intensity of the blur.} of 10 pixels. The experimental procedure was exactly the same as in the previous Experiments.

## Results 


```{r echo=FALSE, warning = FALSE}

#Modality preference in an audio-vidual space

pref_data <- feather::read_feather("../Data_and_analysis/pref_data.feather")
  
pref_thres <- ggplot(pref_data, aes(x=Auditory, y=value, col = factor(model))) +
  geom_line(aes(linetype = factor(model) )) +
  facet_grid(.~Experiment)+
  scale_colour_manual(values = c("Optimal" = "red", "Auditory bias" = "blue", "Visual bias" = "green", "Fit" = "black"))+
  scale_linetype_manual(values = c("Optimal" = "solid", "Auditory bias" = "dashed", "Visual bias" = "dashed", "Fit" = "solid"))+
  xlab("Auditory") +ylab("Visual") +
  scale_x_continuous(limits = c(1.7, 2.3))+
  scale_y_continuous(limits = c(1.7, 2.3))+
  theme_few()+
  theme(aspect.ratio = 1)+
  theme(legend.title = element_blank())

```


```{r echo=FALSE, warning = FALSE}

dataSave1 <- feather::read_feather("../Data_and_analysis/dataExp1.feather")
dataSave2 <- feather::read_feather("../Data_and_analysis/dataExp2.feather")
dataSave3 <- feather::read_feather("../Data_and_analysis/dataExp3.feather")

bias_val_1=dataSave1$estimate[which(dataSave1$variable == "bias")]
bias_ci1_1=dataSave1$lower[which(dataSave1$variable == "bias")]
bias_ci2_1=dataSave1$upper[which(dataSave1$variable == "bias")]

bias_val_2=dataSave2$estimate[which(dataSave2$variable == "bias")]
bias_ci1_2=dataSave2$lower[which(dataSave2$variable == "bias")]
bias_ci2_2=dataSave2$upper[which(dataSave2$variable == "bias")]

bias_val_3=dataSave3$estimate[which(dataSave3$variable == "bias")]
bias_ci1_3=dataSave3$lower[which(dataSave3$variable == "bias")]
bias_ci2_3=dataSave3$upper[which(dataSave3$variable == "bias")]

#Modality bias
preference = c(bias_val_1, bias_val_2, bias_val_3)
experiment =c("Exp 1 \n", "Exp 2", "Exp 3")
ci_low=c(bias_ci1_1, bias_ci1_2, bias_ci1_3)
ci_up=c(bias_ci2_1, bias_ci2_2, bias_ci2_3)

pref = data.frame(preference, ci_low, ci_up)

bias <- ggplot(pref, 
       aes(x = experiment, y=preference)) +
  geom_hline(yintercept = 1, linetype='solid', color="red", size=1)+
  geom_hline(yintercept = 0.5, linetype=2, color="green")+
  geom_hline(yintercept = 2, linetype=2, color="blue")+
  geom_point(size=3)+
  geom_errorbar(aes(ymin = ci_low, ymax = ci_up), 
                  width = 0.1,
                  position = position_dodge(width = 0.1))+
  theme_few()+
  theme(aspect.ratio = 1, axis.text=element_text(size=10))+
  xlab("") +ylab("Relative weighting")+
  scale_y_log10(breaks=c(0.5,1,2),labels=c("Visual bias","Optimal","Auditory bias"))
  
  #coord_cartesian(ylim=c(0, 2.5))

```

```{r bias, echo=FALSE, warning = FALSE, out.width = "\\textwidth", fig.pos = "!h", fig.cap = "Modality preference is characterized as a deviation from the optimal decision threshold. A) The decision thresholds of both the optimal and the descriptive models (solid red and black lines, respectively). Deviation from optimality is compared to two hypothetical cases of modality preference. In these cases, deviation from  optimality is due to over-lying on the visual or the auditory input by a factor of 2 (green and blue dotted lines, respectively). B) An alternative way to represent the same data. Each point represents the value of the decision threshold's slope derived from the descriptive model relative to that of the optimal model (log-scaled). The lines represent the optimal case as well as the two hypothetical cases of modality preference. Error bars represent 95\\% confidence intervals over the distribution obtained through non-parametric resampling."}


legend <- get_legend(pref_thres)
plot_noLegend <- plot_grid(pref_thres + theme(legend.position="none"), NULL, bias, labels = c("A", "", "B"), ncol = 1, align = "v", rel_heights = c(1.1, 0.1, 1.3))
plot_grid(plot_noLegend, legend, rel_widths = c(2, .5))

```

### Unimodal conditions
Figure \@ref(fig:unimodal) shows responses in the unimodal conditions as well as the corresponding fits. The auditory data is a replication of the auditory data in Experiment 1. As for the visual data, we found that, in contrast to Experiment 1 and 2, responses were flatter, showing much more uncertainty.

### Bimodal condition
Figure \@ref(fig:bimodal) shows that almost all the variance was captured by the auditory model alone, the addition of visual information in the optimal model did not improve the prediction of human responses. Similar to Experiments 1 and 2, the values of the variances increased in the bimodal condition (Table \@ref(tab:param)). 

### Cue Combination
Figure \@ref(fig:bias) indicates that the decision threshold was biased towards the auditory modality (the non-noisy modality). Indeed non-parametric resampling of the data showed an increase in the value of the slope in the descriptive model compared to the optimal model (Figure\ \@ref(fig:bias)).

## Discussion

Experiment 3 tested audi-visual combination in the case where the visual input was noisy. Whereas in previous experiments the optimal model explained more variance than the auditory or the visual models did, here the auditory model alone explained almost all the variance. In other words, though participants were sensitive to variation in the noisy visual input when presented in isolation (as shown in Figure \@ref(fig:unimodal)), they tended to ignore this information when the visual input was presented simultaneously with the auditory input (i.e., in the bimodal condition). Instead, they relied almost exclusively on the non-noisy auditory modality.\footnote{The reason why we saw this (floor) effect when we added noise to the visual modality (Experiment 3), and not when we added noise to the auditory modality (Experiment 2), is the fact that our visual stimuli were originally perceived less categorically and with less certainty than the auditory stimuli (see Experiment 1 in \@ref(fig:unimodal)). This fact made it more likely for the visual categorization function to become flat and uninformative after a few drops in precision due to noise on the one had, and to the additional randomness induced by the bimodal presentation on the other hand.}

This finding corresponds to the third case of sub-optimality described in Figure\ \@ref(fig:subOptim). Indeed, precision dropped for both modalities in the bimodal condition compared to the unimodal condition. But the drop was much greater for the visual modality, resulting in a much lower weight assigned to it than what is expected from the optimal model. Therefore, just like participants over-relied on the visual modality when the auditory modality was noisy (Experiment 2), they also over-relied on the auditory modality when the visual modality was noisy (Experiment 3).

So far we have studied the problem of cue combination at the population level --- the models were fit to the data aggregated across all participants. However, it is important to investigate individual variability, especially for cases when we reported optimal cue combination (i.e., Experiment 1). In fact, optimality at the population level can be spurious if it is obtained only on average while most individuals have sub-otimal strategies (e.g., over-relying on the visual or the auditory modalities). In Experiment 4, below, we examine how the average cue combination relates to individual strategies.

# Experiment 4 

As we noted earlier, we did not have enough statistical power in Experiment 1 to fit a different model for each participant. Thus, we used a higher power design, allowing us to collect the number of datapoints necessary to model cue combination at the individual level.

```{r }

weight <- feather::read_feather("../Data_and_analysis/ind_weights.feather")

weight_low <- quantile(subset(weight, type=='simulated')$prop, 0.025)
weight_up <- quantile(subset(weight, type=='simulated')$prop, 0.975) 


subj_vis <- weight %>%
  filter(type=='real') %>%
  filter(prop < weight_low) %>%
  nrow()

subj_aud <- weight %>%
  filter(type=='real') %>%
  filter(prop > weight_up) %>%
  nrow()


weights_plots <- ggplot(data=weight, aes(x=prop, y=..ndensity..)) +
  geom_histogram(data=subset(weight, type=='real'), fill="red", alpha=0.2,, binwidth = 0.4)+
  geom_density(data=subset(weight, type=='simulated'), fill="black", alpha=0.05)+
  #geom_histogram(data=subset(bimod_all, data=='simulation'), fill="blue", alpha=0.2, binwidth = 0.2)+
  #geom_histogram(binwidth = 0.2) +
  geom_vline(xintercept = weight_low, linetype='dashed') +
  geom_vline(xintercept = 1, color='red') +
  geom_vline(xintercept = weight_up, linetype='dashed')+
  annotate("text", x = 12, y = 0.7, size =5,  label = "Auditory bias") +
  annotate("text", x = 0.15, y = 0.7, size =5,  label = "Visual bias") +
 scale_x_log10(breaks =c(0.1,1,10)) +
   xlab("Optimal weighting relative to observed weighting") +ylab("Density")+
  theme_few()+
  theme(aspect.ratio = 0.7) #+
  #coord_cartesian(xlim=c(0.01,100))


```

### Participants
We recruited a planned sample of $N=$ `r N_all_4` participants from Amazon Mechanical Turk. Only participants with US IP addresses and a task approval rate above 99\% were allowed to participate. Participants were excluded if they reported having experienced a technical problem of any sort during the online experiment ($N=$ `r N_all_4 - N_noProb_4`), or if they had less than 75\% accurate responses on the unambiguous training trials ($N=$ `r N_noProb_4 - N_good_4`). The final sample consisted of $N =$ `r N_good_4` participants. All participants provided informed consent before taking the experiment.\footnote{The sample size, exclusion criteria and the main analyses were pre-registered at https://osf.io/h7mzp/.}

### Stimuli
We used the same stimuli as in Experiment 1.

### Design and Procedure
The design and procedure were similar to Experiment 1. There were, however, two differences: 1) We increased the number of responses elicited per subject from 70 to 300, and 2) we randomized the order of the three blocks (i.e., visual-only, auditory-only, and audio-visual) *within* subject: Each participant saw the 3 blocks exactly 6 times, covering all possible ordering combinations. Unlike the between-subject randomization that we used in Experiment 1, this choices allowed us to avoid a possible confound linked to the order of exposure.

```{r weights, echo=FALSE, warning = FALSE, out.width = "\\textwidth", fig.pos = "!h", fig.cap = "The histogram shows the distribution of the participants' predicted (i.e., optimal) cue weighting relative the observed (i.e., descriptive) weighting (see Figure 3 for the details). The density plot shows the distribution of simulated data sampled from the population-level probabilistic model. The dashed lines represent 95\\% confidence interval on this simulated distribution. Optimal behavior is observed when the value of the relative cue weighting is 1 (red solid line). Participants whose values are outside the confidence interval of the simulated distribution over-rely on the visual modality (left side) or auditory modality (right side) beyond sampling-related randomness (i.e., with p < 0.05)."}

weights_plots

```

## Results

### Unimodal and Bimodal conditions
In order to replicate the analysis of Experiment 1, we started by fitting population-level models to the aggregated data. Indeed, we found that the results --- as shown in Figure \@ref(fig:unimodal), Figure \@ref(fig:bimodal), and Table \@ref(tab:param) --- mirror closely the patterns obtained in Experiment 1. 

<!--As a first analysis, we show the histogram comparing the normative models, only (Figure XX). The histogram shows interesting between-subject variability: The optimal model explained the most variance in the majority of cases, showing that participants in these cases combined information from the auditory and visual modality to perform the task. However, the visual and auditory models were sufficient to explain most variance in a large number of participants. These partcipants relied predominantly on only one modality to perform the task.
This is the crucial analysis which would allow us to test the extent to which participants were optimal at the individual level. We perfomed an optimality analysis following the procedure outlines above, i.e., through comparing the optimal and descriptive models. Firs, at the population level, the fitted mean decision threshold was indisguisable form the optimal threshold, thus replicating the results of Experiment 1 (Figure\ \@ref(fig:bias) shows that). 
-->
### Cue combination
We analyzed the cue combination strategies at the individual level. For each participant, we computed the optimal weighting, $\frac{\sigma^2_{A}}{\sigma^2_{V}}$, relative to the observed (i.e., descriptive) weighting, $\frac{\sigma^2_{Ab}}{\sigma^2_{Vb}}$. We show the resulting distribution in Figure \@ref(fig:weights). We note first that the distribution has a rather bimodal shape, centered around the optimal cue combination strategy. This finding rules out the hypothesis that optimality at the population level is a spurious finding, i.e., only obtained by aggregating over various sub-optimal strategies.

In addition, we asked whether the observed variance in the individual distribution was due to the randomness inherent to the process of sampling from a probabilistic model or whether it corresponded to a real between-subject variability induced by different cue weighting strategies. We simulated responses through sampling from the population-level models and we computed the resulting distribution of cue weighting for each simulated individual leading to the density function in Figure \@ref(fig:weights). We can observe that most empirical values fall within the 95% confidence interval of the simulated density, showing that this part of the variance can be due to mere sampling randomness. However, a few participants had values outside this interval, indicating that they systematically over-relied on the visual modality ($N=$ `r subj_vis`, $p < 0.05$) or the auditory modality ($N=$ `r subj_aud`, $p < 0.05$).

## Discussion

This experiment was an extension to Experiment 1. Through collecting larger-size data per subject, we were able to analyze the cue combination optimality, not only at the population level, but also at the individual level. The population-level analysis replicated the results of Experiment 1. The individual-level analysis showed that the distribution of cue combination scores had a unimodally-shaped distribution centered around the optimal combination, thus reflecting genuine cue combination at the individual level. That said, the variance of this distribution indicates that a few participants tended to over-rely on the auditory modality and others tended to over-rely on the visual modality beyond sampling-related randomness.

# General Discussion

In the current paper, we explored word recognition under uncertainty about both words and their referents. We conducted an ideal observer analysis of this task whereby a model provided predictions about how information from each modality should be combined in an optimal fashion. The predictions of the model were tested in a series of four experiments where instances of both the form and the meaning were ambiguous with respect to their category membership only (Experiment 1 and 4), when instances of the form were perturbed with additional background noise (Experiment 2), and when instances of the referent were perturbed with additional visual noise (Experiment 3). We discuss the findings of these studies first with respect to our ideal observer model and inferences about optimality and second with respect to their implications for word identification more generally. 

## Patterns of optimality and sub-optimality

In all of our experiments, and when compared to the predictions of the visual or the auditory models, participants generally relied on both modalities to make their decisions in the bimodal condition. Indeed, in Experiment 1 and 2, the optimal model accounted for more variance in mean responses than the auditory or the visual models did. In Experiment 3, participants appeared to rely on one modality, but this was likely a floor effect, due to the fact that noise made the visual input barely perceptible. Further, in Experiment 1 and 4, which did not involve background noise, participants not only relied on both modalities, but generally weighted these modalities according to the predictions of the optimal model, that is, according to their relative reliability.  At the individual level, Experiment 4 showed that most participant were near-optimal. Only a few subjects over-relied on the auditory or visual modalities beyond sampling errors.

Despite this overall near-optimal behavior, we documented two major cases of sub-optimality. First, in all experiments, the variance associated with each modality increased in the bimodal condition compared to the unimodal conditions: Participants responded slightly more randomly.  This increase in randomness could be due limitation on cognitive resources: Processing two separate --- and perceptually uncorrelated --- cues instead of one cue (as in the unimodal case) is likely to place extra demands on working memory, causing general performance to drop [see @mattys11].  

Previous research has found similar cases of suboptimal behavior. For instance, studies that have explored the identification of ambiguous, newly learned pairs of word-referent associations have reported what appears to be a decrease in speech perception acuity in both children [@stager1997] and adults [@pajak2016]. In agreement with the findings of our study, @hofer2017 characterized the apparent reduction in perceptual acuity as an increase in the noise variance of the auditory modality. Our findings, besides providing more evidence to this documented fact, suggest that the reduction in perceptual acuity may occur simultaneously in both the auditory *and* the visual modalities.

The second case of sub-optimality is related to how participants weighted the cues from the visual and the auditory modalities in a noisy context. In contrast to Experiment 1 and 4 where the cue combination was indistinguishable from the optimal predictions, results of Experiment 2 and 3 suggested that participants had a systematic preference for the non-noisy modality. This finding is reminiscent of the fact that humans tend to compensate for a degraded speech signal by relying more on other sources of information such as the accompanying visual cues, the semantic/syntactic context, or the top-down expectations. This kind of compensation has been observed with adults [@Tanenhaus1995; @mattys12], and recent evidence suggests that it starts in childhood [@yurovsky2017; @macdonald2018]. 

Generally speaking, previous experimental studies have not differentiated between an optimal compensatory strategy (i.e., relying more on the alternative source while using all information still available in the distorted signal), and a sub-optimal strategy (i.e., relying more on the alternative source while ignoring at least some of the information still available in the distorted signal), however. The formal approach followed in this paper allowed us to tease apart these two possibilities, and our analysis supports the sub-optimal compensatory strategy: The preference for the non-noisy modality is above and beyond what can be explained by the relative reliability alone, meaning that the participants tend to ignore at least part of the information still available in the noisy modality.

This second case of sub-optimal behavior may be related to the fact that language understanding under degraded conditions is cognitively more taxing than language understanding under normal conditions [@ronnberg2010; @mattys12; @peelle2018]. Perhaps these demands lead to sub-optimal behavior (i.e., over-reliance on the less noisy cue) as participants seek to minimize cognitive effort. One could also explain this phenomenon in terms of the metacognitive experience about the fluency with which information is processed. The perceived perceptual fluency (e.g., the ease with which a stimulus' physical identity can be identified) can affect a wide variety of human judgements [see @schwarz2004 for a review]. In particular, variables that improve fluency tend to increase liking/preference [@reber98]. In our case, the subjective experience of lower fluency in the noisy modality might cause people to underestimate information that can be extracted from this modality, especially when presented simultaneously with a higher fluency alternative.

## Word recognition in the wild 

An important question to ask is how the combination mechanism --- as revealed in our controlled study --- scales up to real life situations. Note that in order to test audio-visual cue combination under uncertainty, we had to use a case of double ambiguity, that is, a case where both the word forms ("ada"--"aba") and the referents (cat--dog) were similar and, thus, confusable. To what extent does such a case occur in real languages?  Cross-linguistic corpus analyses suggest that lexical encoding tends, surprisingly, towards double ambiguity in many languages [@dautriche17; @Monaghan2014; @Tamariz2008]. For instance, @dautriche17 analyzed 100 languages and found that words that are similar phonologically  tend to be similar semantically as well.  These studies suggest that the case of double uncertainty, though perhaps not pervasive, could be a real issue in language as it increase the probability of confusability for many words. That said, the inferences discussed here might play a more significant role in naturalistic language comprehension when ambiguity in both the form and/or the referent is induced by an *external* noisy context --- e.g., a very noisy party or a far away referent --- even when these forms and referents are not confusable in normal situations. 

Though we only studied adult performance in this paper, the problem of word recognition under uncertainty is likely more pressing for children. In fact, young children have greater difficulties learning the meanings of novel similar-sounding words (e.g., "bin" vs. "din"), even when these words are uttered very clearly [@stager1997; @Merriman91; @Creel2012; @Swingley2016; @white2008b]. Such similar-sounding words can be shown to be differentiated by infants in simplified experimental settings [e.g., @yoshida2009]. Nevertheless, @Swingley2007 suggested that the ability to make this differentiation is likely not mature in early childhood; children's representations are almost certainly noisier than the adults' representations and may also be encoded with lower confidence. Thus, children even more than adults might benefit from additional disambiguating cues during new word-referent encoding and recognition. 

A multi-modal cue combination strategy might help children not only recognize words, but also refine their underlying phonological and semantic representations in the process. Previous research in early word learning has -- whether implicitly or explicitly -- largely treated the process of refining the word form and of refining the word meaning as following a linear timeline. However, developmental data reveal that children do not wait to have complete acquisition of word forms before they start learning their meanings [@bergelson2012; @tincoff1999]. Rather, both form and meaning representations develop in a parallel fashion. A few studies have already suggested the possibility of an interaction between sound and meaning in early acquisition. For instance, @waxman1995 showed that labeling various objects with the same name helps infants form the underlying semantic category [but see @sloutsky2003]. And in the opposite direction, @yeung09 showed that pairing similar sounds with different objects can helps infants enhance their sensitivity to subtle phonological contrasts in their native language. The present study proposes a first step towards a formal framework where these sorts of sound-meaning interactions in development can be unified and further explored. 

## Limitations

One salient limitation of our current work is that we used a restricted and highly simplified stimulus set. For the auditory modality, we used speech categories that varied along a single acoustic dimension. While this dimension might be sufficient to recognize  words in our specific case, in general the speech signal is far more complex, varying along several acoustic/phonetic dimensions. Additionally, these dimensions may be highly variable due to various kinds of speaker and context differences. 

Concerning the visual dimension, simulating meaningful variability has been a notoriously difficult problem. Following previous studies [@freedman2001; @havy2016; @sloutsky2004], we used a visual continuum along a one-dimensional morph. This simplification was motivated by the need to construct a multimodal input where the auditory and visual components are parametrized in a symmetrical fashion, allowing us to compare graded effects of auditory and visual information on categorical judgment.  Though such a visual variability is clearly artificial (one does not encounters in real life an animal that is, e.g., 30 % dog and 70 % cat), we assume that the induced uncertainty form this visual stimuli has a similar effect on word recognition as the uncertainty induced by more naturalistic semantic variability.

It is an open question whether people use the same strategy in controlled laboratory conditions and more naturalistic settings where they have to deal with various levels of variability. An answer to this question is likely to involve a multifaceted research approach that goes beyond controlled experimentation. We believe that one fruitful approach is to test computational mechanisms with an input that more accurately represents the full extent of multimodal variability in the learning environment [@fourtassi2014b; @harwath2016; @roy2015; @dupoux2018].

Finally, though we used the terminology of word recognition, our work is only indirectly related to the literature about how a rich lexicon is accessed [e.g., @mcclelland1986]. We have used this term in a more specific way, describing access to a simplified lexicon made of two novel, ambiguous words. Such a simplified experimental context is not new and has been crucial to our understanding of early word learning and recognition [e.g., @stager1997].  

# Conclusions and future research directions

Our work used an ideal observer model to study word recognition under audio-visual uncertainty. This framework enabled us not only to test optimality, but also to examine systematically how and by how much people deviate from optimality in their combination strategies. Thus, our work is part of a growing effort to go beyond optimality tests --- which have limited explanatory power --- and use models that also allow us to identify and explain various patterns of sub-optimality in human behavior [@rahnev2018].

While we focused on the case of arbitrary associations in novel word recognition, it is possible to extend the framework to other cases such as that of \textit{iconicity}, i.e., when there is a resemblance between the sound of a word and its referent. Previous work has suggested that iconicity, among other things, helps with learning (and generalizing the meaning of) new words [see @dingemanse2015 for a review]. Using the research strategy in this paper, we can, for example, test whether iconicity has such an advantage because it mitigates the sub-optimal patterns observed with more arbitrary pairings. 

Finally, though the current framework only characterizes adult novel word recognition, it provides a first step towards a model where developmental questions can also be investigated. For instance, future work should explore whether children, like adults, use probabilistic cues from both the auditory and the visual input to recognize ambiguous words, the extent to which they combine these cues in an optimal fashion, and whether this cue combination help them learn words and refine their early phonological and semantic representations.

\vspace{1em} \fbox{\parbox[b][][c]{14cm}{\centering All data and code for these analyses are available at\ \url{https://github.com/afourtassi/WordRec}}} \vspace{1em}


# Acknowledgements
This work was supported by a post-doctoral grant from the Fyssen Foundation.


# Disclosure statement
None of the authors have any financial interest or a conflict of interest regarding this work and this submission.

# Appendix 1: derivation of the posterior (Equation 1)

For an ideal observer, the probability of choosing category 2 when presented with an audio-visual instance $w = (a, v)$ is the posterior probability of this category:

$$p(W_2 | w)=\frac{p(w|W_2)p(W_2)}{p(w|W_2)p(W_2)+p(w|W_1)p(W_1)}$$

Which reduces to:

$$p(W_2 | w)=\frac{1}{1+\frac{p(w|W_1)}{p(w|W_2)} \frac{p(W_1)}{p(W_2)}}$$
In order to further simplify the quantity $\frac{p(w|W_1)}{p(w|W_2)}$, we use our assumption that the cues are uncorrelated:
$$p(w | W) = p(a,v| W) = p(a| A)p(v| V)$$
Using the $\log$ transformation, we get:

$$ \ln(\frac{p(w |W_1)}{p(w|W_2)})=\ln(\frac{p(a|W_1)}{p(a|W_2)})+\ln(\frac{p(v|W_1)}{p(v|W_2)}) $$ 
Under the assumption that the categories are normally distributed and that, within each modality, the categories have equal variances, we get (after simplification):

$$\ln(\frac{p(a|W_1)}{p(a|W_2)})=\frac{\mu_{A1}-\mu_{A2}}{\sigma^2_{A}}\times a+ \frac{\mu^2_{A2}-\mu^2_{A1}}{2\sigma^2_{A}}$$

and similarly:

$$\ln(\frac{p(v|W_1)}{p(v|W_2)})=\frac{\mu_{V1}-\mu_{V2}}{\sigma^2_{V}}\times v+ \frac{\mu^2_{V2}-\mu^2_{V1}}{2\sigma^2_{V}}$$

When putting all these terms together, we obtain this final expression for the posterior:
$$p(W_2 | w)=\frac{1}{1+(1+b)\exp(\beta_0+\beta_aa+\beta_vv)}$$

where 

$$1+b=\frac{p(W_1)}{p(W_2)}$$
$$\beta_0=\frac{\mu^2_{A2}-\mu^2_{A1}}{2\sigma^2_{A}}+\frac{\mu^2_{V2}-\mu^2_{V1}}{2\sigma^2_{V}}$$

$$\beta_a=\frac{\mu_{A1}-\mu_{A2}}{\sigma^2_{A}}$$
$$\beta_v=\frac{\mu_{V1}-\mu_{V2}}{\sigma^2_{V}}.$$ 


# References
```{r create_r-references}
r_refs(file = "references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
